# import os
# import json
# import hashlib
# import time
# import re  
# import logging
# from tqdm import tqdm
# from datasketch import MinHash, MinHashLSH
# from config import (
#     INTERMEDIATE_PATH, BATCH_SIZE_PREPROCESS, MINHASH_NUM_PERM, LSH_THRESHOLD,
#     MIN_CHAR_LEN, MAX_CHAR_LEN, ACADEMIC_REQUIRE
# )
# from utils import stats, load_jsonl_files_with_sampling, is_colloquial, is_sensitive, has_academic_features

# def preprocess_and_md5_deduplicate():
#     """é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆé•¿åº¦ã€æ•æ„Ÿã€å£è¯­ã€å­¦æœ¯ç‰¹å¾ï¼‰"""
#     start_time = time.time()
#     logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆå«æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼‰")
    
#     md5_set = set()
#     output_file = os.path.join(INTERMEDIATE_PATH, "preprocessed_md5_dedup_with_sensitive_filter.jsonl")
#     batch_buffer = []
    
#     with open(output_file, "w", encoding="utf-8") as f_out:
#         for item in tqdm(load_jsonl_files_with_sampling(), desc="é¢„å¤„ç†+ç­›é€‰", total=stats["sampled_count"] if stats["sampled_count"] > 0 else None):
#             text = item["text"]
#             original_data = item["original_data"]
#             source_file = item["source_file"]
            
#             # 1. åŸºç¡€é•¿åº¦è¿‡æ»¤
#             if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
#                 stats["preprocess_filtered"] += 1
#                 continue
            
#             # 2. æ–‡æœ¬æ¸…æ´—ï¼ˆå»é™¤é›¶å®½ç©ºæ ¼å’Œå¤šä½™ç©ºæ ¼ï¼‰
#             text = re.sub(r"[\u200b\s]+", " ", text).strip()
#             if not text:
#                 stats["preprocess_filtered"] += 1
#                 continue
            
#             # 3. æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼ˆä¼˜å…ˆè¿‡æ»¤è¿è§„æ•°æ®ï¼‰
#             if is_sensitive(text):
#                 stats["sensitive_filtered"] += 1
#                 continue
            
#             # 4. å£è¯­åŒ–ç­›é€‰
#             if is_colloquial(text):
#                 stats["colloquial_filtered"] += 1
#                 continue
            
#             # 5. å­¦æœ¯ç‰¹å¾ç­›é€‰ï¼ˆå¯é€‰ï¼‰
#             if ACADEMIC_REQUIRE and not has_academic_features(text):
#                 stats["non_academic_filtered"] += 1
#                 continue
            
#             # 6. MD5ç²¾ç¡®å»é‡
#             md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
#             if md5_hash in md5_set:
#                 stats["md5_duplicated"] += 1
#                 continue
#             md5_set.add(md5_hash)
            
#             # æ‰¹é‡ç¼“å­˜å†™å…¥
#             batch_buffer.append({
#                 "text": text,
#                 "original_data": original_data,
#                 "source_file": source_file,
#                 "md5": md5_hash,
#                 "has_academic_features": has_academic_features(text)
#             })
            
#             if len(batch_buffer) >= BATCH_SIZE_PREPROCESS:
#                 for data in batch_buffer:
#                     f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
#                 batch_buffer = []
        
#         # å†™å…¥å‰©ä½™æ•°æ®
#         if batch_buffer:
#             for data in batch_buffer:
#                 f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
    
#     # ç»Ÿè®¡ç»“æœ
#     stats["stage_time"]["preprocess_md5"] = round(time.time() - start_time, 2)
#     remaining = stats["sampled_count"] - stats["preprocess_filtered"] - \
#                 stats["colloquial_filtered"] - stats["non_academic_filtered"] - \
#                 stats["md5_duplicated"] - stats["sensitive_filtered"]
    
#     logging.info(
#         f"âœ… é¢„å¤„ç†+ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['preprocess_md5']}ç§’ | "
#         f"æŠ½æ ·åï¼š{stats['sampled_count']} | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']} | "
#         f"æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼š{stats['sensitive_filtered']} | å£è¯­åŒ–ï¼š{stats['colloquial_filtered']} | "
#         f"æ— å­¦æœ¯ç‰¹å¾ï¼š{stats['non_academic_filtered']} | å®Œå…¨é‡å¤ï¼ˆMD5ï¼‰ï¼š{stats['md5_duplicated']} | å‰©ä½™ï¼š{remaining}"
#     )
#     return output_file

# def create_minhash_signature(text):
#     """ç”Ÿæˆæ–‡æœ¬çš„MinHashç­¾åï¼ˆ2-gramæ•æ‰ä¸­æ–‡è¯­ä¹‰ï¼‰"""
#     minhash = MinHash(num_perm=MINHASH_NUM_PERM)
#     if len(text) < 2:
#         grams = [text] if text else []
#     else:
#         grams = [text[i:i+2] for i in range(len(text)-1)]
#     for gram in grams:
#         token_hash = hashlib.sha256(gram.encode('utf-8')).hexdigest()
#         minhash.update(token_hash.encode('utf-8'))
#     return minhash

# def minhash_lsh_deduplicate(input_file):
#     """Minhash LSHè¯­ä¹‰å»é‡ï¼ˆå»é™¤ç›¸ä¼¼æ–‡æœ¬ï¼‰"""
#     start_time = time.time()
#     logging.info(f"ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡ï¼ˆé˜ˆå€¼{ LSH_THRESHOLD }ï¼‰")
    
#     # è¯»å–é¢„å¤„ç†åçš„æ•°æ®
#     texts = []
#     data_list = []
#     with open(input_file, "r", encoding="utf-8") as f:
#         for line in tqdm(f, desc="è¯»å–é¢„å¤„ç†æ•°æ®"):
#             data = json.loads(line)
#             texts.append(data["text"])
#             data_list.append(data)
    
#     if not texts:
#         logging.warning("âš ï¸ MD5å»é‡åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
#         return input_file
    
#     # åˆå§‹åŒ–LSHå¹¶æ‰§è¡Œå»é‡
#     lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
#     keep_indices = []
#     duplicate_count = 0
    
#     for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
#         minhash = create_minhash_signature(texts[i])
#         similar_docs = lsh.query(minhash)
#         if not similar_docs:
#             lsh.insert(str(i), minhash)
#             keep_indices.append(i)
#         else:
#             duplicate_count += 1
    
#     stats["minhash_duplicated"] = duplicate_count
    
#     # ä¿å­˜å»é‡åçš„æ•°æ®
#     output_file = os.path.join(INTERMEDIATE_PATH, "minhash_dedup_with_sensitive_filter.jsonl")
#     with open(output_file, "w", encoding="utf-8") as f_out:
#         for idx in keep_indices:
#             f_out.write(json.dumps(data_list[idx], ensure_ascii=False) + "\n")
    
#     # ç»Ÿè®¡ç»“æœ
#     stats["stage_time"]["minhash_lsh"] = round(time.time() - start_time, 2)
#     remaining = len(keep_indices)
#     logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['minhash_lsh']}ç§’ | "
#                  f"è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']} | å‰©ä½™ï¼š{remaining}")
#     return output_file

import hashlib
from tqdm import tqdm
from datasketch import MinHash, MinHashLSH
import time

from config import MINHASH_NUM_PERM, LSH_THRESHOLD
from utils import load_jsonl_files_with_sampling

def md5_deduplication(data_generator, stats):
    """MD5ç²¾ç¡®å»é‡ï¼ˆæ¥æ”¶æ•°æ®ç”Ÿæˆå™¨ï¼Œè¿”å›å»é‡åçš„æ•°æ®åˆ—è¡¨ï¼‰"""
    import logging
    start_time = time.time()
    logging.info("ğŸš€ å¼€å§‹MD5ç²¾ç¡®å»é‡")
    
    md5_set = set()
    deduplicated_data = []
    
    for item in tqdm(data_generator, desc="MD5å»é‡", total=stats["sampled_count"] if stats["sampled_count"] > 0 else None):
        text = item["text"]
        md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
        
        if md5_hash in md5_set:
            stats["md5_duplicated"] += 1
            continue
        
        md5_set.add(md5_hash)
        item["md5"] = md5_hash
        deduplicated_data.append(item)
    
    logging.info(f"âœ… MD5å»é‡å®Œæˆ | å®Œå…¨é‡å¤ï¼š{stats['md5_duplicated']}æ¡ | å‰©ä½™ï¼š{len(deduplicated_data)}æ¡")
    return deduplicated_data

def create_minhash_signature(text, num_perm=MINHASH_NUM_PERM):
    """åˆ›å»ºMinHashç­¾åï¼ˆ2-gramæ•æ‰ä¸­æ–‡è¯­ä¹‰ï¼‰"""
    minhash = MinHash(num_perm=num_perm)
    if len(text) < 2:
        grams = [text] if text else []
    else:
        grams = [text[i:i+2] for i in range(len(text)-1)]
    for gram in grams:
        token_hash = hashlib.sha256(gram.encode('utf-8')).hexdigest()
        minhash.update(token_hash.encode('utf-8'))
    return minhash

def minhash_lsh_deduplication(data_list, stats):
    """MinHash LSHè¯­ä¹‰å»é‡ï¼ˆæ¥æ”¶æ•°æ®åˆ—è¡¨ï¼Œè¿”å›å»é‡åçš„æ•°æ®åˆ—è¡¨ï¼‰"""
    import logging
    start_time = time.time()
    logging.info(f"ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡ï¼ˆé˜ˆå€¼{ LSH_THRESHOLD }ï¼‰")
    
    if not data_list:
        logging.warning("âš ï¸ æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
        return data_list
    
    texts = [item["text"] for item in data_list]
    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
    keep_indices = []
    duplicate_count = 0
    
    for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
        minhash = create_minhash_signature(texts[i])
        similar_docs = lsh.query(minhash)
        if not similar_docs:
            lsh.insert(str(i), minhash)
            keep_indices.append(i)
        else:
            duplicate_count += 1
    
    stats["minhash_duplicated"] = duplicate_count
    deduplicated_data = [data_list[idx] for idx in keep_indices]
    
    logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']}æ¡ | å‰©ä½™ï¼š{len(deduplicated_data)}æ¡")
    return deduplicated_data