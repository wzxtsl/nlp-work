# import os
# import json
# import time
# import numpy as np
# import torch
# import logging
# from tqdm import tqdm
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from config import (
#     OUTPUT_PATH, MODEL_ID, DEVICE, MAX_SEQ_LENGTH, BATCH_SIZE_PERPLEXITY,MIN_CHAR_LEN,
#     MODERN_PERPLEXITY_PERCENTILE, CLASSIC_PERPLEXITY_PERCENTILE, PERPLEXITY_MAX_LIMIT
# )
# from utils import stats, is_classic_chinese

# def load_perplexity_model():
#     """åŠ è½½å›°æƒ‘åº¦è®¡ç®—æ¨¡å‹ï¼ˆGPT2-Chineseï¼‰"""
#     logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦ï¼ˆç”¨äºè¯„ä¼°æ–‡æœ¬è´¨é‡ï¼‰")
#     start_time = time.time()
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
#     if tokenizer.pad_token is None:
#         tokenizer.pad_token = tokenizer.eos_token  # è¡¥å…¨pad_token
#     model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
#     model.eval()  # æ¨ç†æ¨¡å¼
#     logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
#     return tokenizer, model

# def calculate_perplexity_batch_optimized(texts, tokenizer, model):
#     """æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¼˜åŒ–ç»´åº¦åŒ¹é…å’Œå†…å­˜å ç”¨ï¼‰"""
#     try:
#         # æ–‡æœ¬ç¼–ç 
#         inputs = tokenizer(
#             texts, 
#             padding=True, 
#             truncation=True, 
#             max_length=MAX_SEQ_LENGTH, 
#             return_tensors="pt"
#         ).to(DEVICE)
        
#         input_ids = inputs["input_ids"]
#         attention_mask = inputs["attention_mask"]
#         batch_size = input_ids.shape[0]
        
#         with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)
#             logits = outputs.logits
            
#             perplexities = []
#             for i in range(batch_size):
#                 # è¿‡æ»¤pad_token
#                 valid_mask = (input_ids[i] != tokenizer.pad_token_id)
#                 valid_token_ids = input_ids[i][valid_mask]
                
#                 if len(valid_token_ids) <= 1:
#                     perplexities.append(float('inf'))
#                     continue
                
#                 # ä¿®æ­£ç»´åº¦åŒ¹é…ï¼ˆshift logitså’Œlabelsï¼‰
#                 shift_logits = logits[i, :-1, :].contiguous()
#                 shift_labels = valid_token_ids[1:].contiguous()
#                 valid_len = len(shift_labels)
#                 shift_logits = shift_logits[:valid_len, :]  # æˆªæ–­åˆ°æœ‰æ•ˆé•¿åº¦
                
#                 # è®¡ç®—äº¤å‰ç†µæŸå¤±
#                 loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
#                 loss = loss_fct(
#                     shift_logits.view(-1, shift_logits.size(-1)), 
#                     shift_labels.view(-1)
#                 )
                
#                 perplexity = torch.exp(loss).item()
#                 perplexities.append(perplexity)
            
#             return perplexities
            
#     except RuntimeError as e:
#         # GPUå†…å­˜ä¸è¶³æ—¶è‡ªåŠ¨å‡å°æ‰¹é‡
#         if "out of memory" in str(e):
#             logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
#             half_size = len(texts) // 2
#             if half_size >= 1:
#                 perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
#                 perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
#                 return perplexities1 + perplexities2
#             else:
#                 return [calculate_perplexity_single(text, tokenizer, model) for text in texts]
#         else:
#             raise e
#     except Exception as e:
#         logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
#         return [calculate_perplexity_single(text, tokenizer, model) for text in texts]

# def calculate_perplexity_single(text, tokenizer, model):
#     """å•æ–‡æœ¬å›°æƒ‘åº¦è®¡ç®—ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
#     try:
#         inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
#         with torch.no_grad():
#             outputs = model(**inputs)
#             logits = outputs.logits
#             shift_logits = logits[..., :-1, :].contiguous()
#             shift_labels = inputs["input_ids"][..., 1:].contiguous()
#             loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
#             avg_loss = loss.mean()
#             return torch.exp(avg_loss).item()
#     except:
#         return float('inf')

# def analyze_perplexity_distribution_layered(input_file, sample_size=1000):
#     """åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆåˆ†åˆ«è®¡ç®—å¤æ–‡/ç°ä»£æ–‡åˆ†ä½æ•°ï¼‰"""
#     logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç°ä»£æ–‡=ä½å›°æƒ‘åº¦ä¼˜è´¨ï¼Œå¤æ–‡=é«˜å›°æƒ‘åº¦çœŸå®ï¼‰")
#     tokenizer, model = load_perplexity_model()
    
#     # è¯»å–æ ·æœ¬æ•°æ®
#     texts = []
#     with open(input_file, "r", encoding="utf-8") as f:
#         lines = f.readlines()
#         if len(lines) > sample_size:
#             import random
#             lines = random.sample(lines, sample_size)  # éšæœºæŠ½æ ·
#         for line in lines:
#             try:
#                 data = json.loads(line)
#                 text = data.get("text", "").strip()
#                 if text and len(text) >= MIN_CHAR_LEN:
#                     texts.append(text)
#             except:
#                 continue
    
#     if not texts:
#         logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
#         return 100.0, 30.0  # é»˜è®¤é˜ˆå€¼
    
#     logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
#     # åˆ†ç±»å¤æ–‡/ç°ä»£æ–‡
#     classic_texts = []
#     modern_texts = []
#     for text in texts:
#         if is_classic_chinese(text):
#             classic_texts.append(text)
#         else:
#             modern_texts.append(text)
    
#     logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
#     # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
#     modern_perplexities = []
#     for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
#         batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
#         batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
#         modern_perplexities.extend(batch_perplexities)
    
#     classic_perplexities = []
#     for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
#         batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
#         batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
#         classic_perplexities.extend(batch_perplexities)
    
#     # è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆè¶…å‡ºä¸Šé™çš„è§†ä¸ºæ— æ•ˆï¼‰
#     modern_valid = np.array([p for p in modern_perplexities if p < PERPLEXITY_MAX_LIMIT])
#     classic_valid = np.array([p for p in classic_perplexities if p < PERPLEXITY_MAX_LIMIT])
    
#     # å¤„ç†ç©ºæ•°æ®
#     if len(modern_valid) == 0:
#         logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#         modern_valid = np.array([100.0])
#     if len(classic_valid) == 0:
#         logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#         classic_valid = np.array([30.0])
    
#     # è®¡ç®—ç›®æ ‡åˆ†ä½æ•°
#     modern_threshold = np.percentile(modern_valid, MODERN_PERPLEXITY_PERCENTILE)
#     classic_threshold = np.percentile(classic_valid, CLASSIC_PERPLEXITY_PERCENTILE)
    
#     # è¾“å‡ºåˆ†å¸ƒæ—¥å¿—
#     logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆä½å›°æƒ‘åº¦=è´¨é‡é«˜ã€æ¨¡å‹æ˜“ç†è§£ï¼‰:")
#     percentiles = [0, 10, 25, 50, 75, 90, 95, 100]
#     for p in percentiles:
#         logging.info(f"    {p}% åˆ†ä½æ•°: {np.percentile(modern_valid, p):.2f}")
#     logging.info(f"ğŸ¯ ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f} ({MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)")
    
#     logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆé«˜å›°æƒ‘åº¦=æ›´çœŸå®ã€éç°ä»£æ”¹å†™ï¼‰:")
#     for p in percentiles:
#         logging.info(f"    {p}% åˆ†ä½æ•°: {np.percentile(classic_valid, p):.2f}")
#     logging.info(f"ğŸ¯ å¤æ–‡é˜ˆå€¼: â‰¥{classic_threshold:.2f} ({CLASSIC_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)")
    
#     # ç»˜åˆ¶åˆ†å¸ƒå›¾ï¼ˆå¯é€‰ï¼‰
#     try:
#         import matplotlib.pyplot as plt
#         plt.figure(figsize=(12, 8))
#         plt.hist(modern_valid, bins=50, alpha=0.7, label='Modern Chinese (Low Perplexity = High Quality)', color='skyblue', edgecolor='black')
#         plt.hist(classic_valid, bins=50, alpha=0.7, label='Classic Chinese (High Perplexity = Authentic)', color='salmon', edgecolor='black')
#         plt.axvline(modern_threshold, color='blue', linestyle='--', linewidth=2, label=f'Modern â‰¤ {modern_threshold:.2f} ({MODERN_PERPLEXITY_PERCENTILE}%tile)')
#         plt.axvline(classic_threshold, color='red', linestyle='--', linewidth=2, label=f'Classic â‰¥ {classic_threshold:.2f} ({CLASSIC_PERPLEXITY_PERCENTILE}%tile)')
#         plt.xlabel('Perplexity', fontsize=12)
#         plt.ylabel('Frequency', fontsize=12)
#         plt.title('Layered Perplexity Distribution (CLMMU/CEVAL Metric Priority)', fontsize=14, fontweight='bold')
#         plt.legend(fontsize=10)
#         plt.grid(True, alpha=0.3)
#         plt.tight_layout()
        
#         output_path = os.path.join(OUTPUT_PATH, 'layered_perplexity_distribution_final.png')
#         plt.savefig(output_path, dpi=300, bbox_inches='tight')
#         plt.close()
#         logging.info(f"ğŸ“Š åˆ†å±‚åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
#     except Exception as e:
#         logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
    
#     return modern_threshold, classic_threshold

# def determine_layered_thresholds(input_file):
#     """ç¡®å®šå¤æ–‡/ç°ä»£æ–‡çš„å›°æƒ‘åº¦ç­›é€‰é˜ˆå€¼"""
#     logging.info("ğŸ¯ å¼€å§‹ç¡®å®šåˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼")
#     try:
#         modern_threshold, classic_threshold = analyze_perplexity_distribution_layered(input_file, sample_size=500)
#         logging.info(f"ğŸ¤– ç°ä»£æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¤{modern_threshold:.2f} ({MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)")
#         logging.info(f"ğŸ“œ å¤æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¥{classic_threshold:.2f} ({CLASSIC_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)")
#         return modern_threshold, classic_threshold
#     except Exception as e:
#         logging.error(f"âŒ ç¡®å®šåˆ†å±‚é˜ˆå€¼å¤±è´¥: {str(e)}")
#         logging.info(f"ğŸ”„ ä½¿ç”¨é»˜è®¤é˜ˆå€¼: ç°ä»£æ–‡=80 ({MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°), å¤æ–‡=30 ({CLASSIC_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)")
#         return 80.0, 30.0

# def layered_perplexity_filter(input_file, modern_threshold, classic_threshold):
#     """åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆç°ä»£æ–‡ä½å›°æƒ‘åº¦ + å¤æ–‡é«˜å›°æƒ‘åº¦ï¼‰"""
#     start_time = time.time()
#     logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰")
#     logging.info(f"   - ç°ä»£æ–‡ï¼šâ‰¤{modern_threshold:.2f} ({MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)ï¼Œä¿ç•™ä½å›°æƒ‘åº¦ä¼˜è´¨æ•°æ®")
#     logging.info(f"   - å¤æ–‡ï¼šâ‰¥{classic_threshold:.2f} ({CLASSIC_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°)ï¼Œä¿ç•™é«˜å›°æƒ‘åº¦çœŸå®å¤æ–‡")
    
#     tokenizer, model = load_perplexity_model()
    
#     # è¾“å‡ºæ–‡ä»¶è·¯å¾„
#     kept_file = os.path.join(OUTPUT_PATH, "clmmu_kept_data_final.jsonl")
#     filtered_file = os.path.join(OUTPUT_PATH, "clmmu_filtered_data_final.jsonl")
#     classic_file = os.path.join(OUTPUT_PATH, "clmmu_classic_chinese_data_final.jsonl")
    
#     batch_texts = []
#     batch_data = []
    
#     # ç»Ÿè®¡æ€»è¾“å…¥æ•°
#     with open(input_file, "r", encoding="utf-8") as f_in:
#         total_input = sum(1 for _ in f_in)
    
#     # ç­›é€‰æµç¨‹
#     with open(input_file, "r", encoding="utf-8") as f_in, \
#          open(kept_file, "w", encoding="utf-8") as f_kept, \
#          open(filtered_file, "w", encoding="utf-8") as f_filtered, \
#          open(classic_file, "w", encoding="utf-8") as f_classic:
        
#         f_in.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
#         for line in tqdm(f_in, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰", total=total_input):
#             try:
#                 data = json.loads(line)
#                 batch_texts.append(data["text"])
#                 batch_data.append(data)
#             except:
#                 continue
            
#             # æ‰¹é‡å¤„ç†
#             if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
#                 perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
#                 for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
#                     data_item["perplexity"] = round(perp, 2)
#                     data_item["is_classic"] = is_classic_chinese(text)
                    
#                     # ç­›é€‰é€»è¾‘
#                     if data_item["is_classic"]:
#                         # å¤æ–‡ï¼šä¿ç•™â‰¥é˜ˆå€¼ä¸”æ— å¼‚å¸¸å€¼
#                         if perp >= classic_threshold and perp < PERPLEXITY_MAX_LIMIT:
#                             final_data = data_item["original_data"].copy()
#                             final_data.update({
#                                 "cleaned_text": text,
#                                 "md5": data_item["md5"],
#                                 "perplexity": data_item["perplexity"],
#                                 "source_file": data_item["source_file"],
#                                 "text_type": "classic_chinese",
#                                 "has_academic_features": data_item.get("has_academic_features", False)
#                             })
#                             f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             stats["final_kept"] += 1
#                             stats["classic_chinese_kept"] += 1
#                         else:
#                             filtered_data = data_item.copy()
#                             filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¥{classic_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
#                             f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                             stats["perplexity_filtered"] += 1
#                     else:
#                         # ç°ä»£æ–‡ï¼šä¿ç•™â‰¤é˜ˆå€¼ä¸”æ— å¼‚å¸¸å€¼
#                         if perp <= modern_threshold and perp < PERPLEXITY_MAX_LIMIT / 3:  # ç°ä»£æ–‡é˜ˆå€¼æ›´ä¸¥æ ¼
#                             final_data = data_item["original_data"].copy()
#                             final_data.update({
#                                 "cleaned_text": text,
#                                 "md5": data_item["md5"],
#                                 "perplexity": data_item["perplexity"],
#                                 "source_file": data_item["source_file"],
#                                 "text_type": "modern_chinese",
#                                 "has_academic_features": data_item.get("has_academic_features", False)
#                             })
#                             f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             stats["final_kept"] += 1
#                             stats["modern_chinese_kept"] += 1
#                         else:
#                             filtered_data = data_item.copy()
#                             filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¤{modern_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
#                             f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                             stats["perplexity_filtered"] += 1
                
#                 # é‡ç½®æ‰¹é‡ç¼“å­˜
#                 batch_texts = []
#                 batch_data = []
        
#         # å¤„ç†å‰©ä½™æ•°æ®
#         if batch_texts:
#             perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
#             for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
#                 # é‡å¤ä¸Šè¿°ç­›é€‰é€»è¾‘ï¼ˆç•¥ï¼Œä¸æ‰¹é‡å¤„ç†ä¸€è‡´ï¼‰
#                 data_item["perplexity"] = round(perp, 2)
#                 data_item["is_classic"] = is_classic_chinese(text)
#                 if data_item["is_classic"]:
#                     if perp >= classic_threshold and perp < PERPLEXITY_MAX_LIMIT:
#                         final_data = data_item["original_data"].copy()
#                         final_data.update({
#                             "cleaned_text": text, "md5": data_item["md5"], "perplexity": data_item["perplexity"],
#                             "source_file": data_item["source_file"], "text_type": "classic_chinese",
#                             "has_academic_features": data_item.get("has_academic_features", False)
#                         })
#                         f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         stats["final_kept"] += 1
#                         stats["classic_chinese_kept"] += 1
#                     else:
#                         filtered_data = data_item.copy()
#                         filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¥{classic_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
#                         f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                         stats["perplexity_filtered"] += 1
#                 else:
#                     if perp <= modern_threshold and perp < PERPLEXITY_MAX_LIMIT / 3:
#                         final_data = data_item["original_data"].copy()
#                         final_data.update({
#                             "cleaned_text": text, "md5": data_item["md5"], "perplexity": data_item["perplexity"],
#                             "source_file": data_item["source_file"], "text_type": "modern_chinese",
#                             "has_academic_features": data_item.get("has_academic_features", False)
#                         })
#                         f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         stats["final_kept"] += 1
#                         stats["modern_chinese_kept"] += 1
#                     else:
#                         filtered_data = data_item.copy()
#                         filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¤{modern_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
#                         f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                         stats["perplexity_filtered"] += 1
    
#     # ç»Ÿè®¡ç»“æœ
#     stats["stage_time"]["perplexity"] = round(time.time() - start_time, 2)
#     logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’")
#     logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
#     logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
#     return kept_file, filtered_file, classic_file

import re
import numpy as np
from tqdm import tqdm
import time

from config import (
    COLLOQUIAL_WORDS, SENSITIVE_KEYWORDS, ACADEMIC_PATTERNS, ACADEMIC_REQUIRE,
    MIN_CHAR_LEN, MAX_CHAR_LEN, PERCENTILES, MODERN_PERPLEXITY_PERCENTILE,
    CLASSIC_PERPLEXITY_PERCENTILE, MAX_MODERN_PERPLEXITY, MAX_CLASSIC_PERPLEXITY,
    CLASSIC_CHINESE_WORDS, MODERN_CHINESE_WORDS, CLASSIC_CHINESE_DENSITY_THRESHOLD,
    OUTPUT_PATH, BATCH_SIZE_PERPLEXITY
)
from utils import (
    load_perplexity_model, calculate_perplexity_batch_optimized,
    plot_perplexity_distribution
)

def is_colloquial(text):
    """æ£€æµ‹å£è¯­åŒ–æ–‡æœ¬ï¼ˆæ‰©å……å…³é”®è¯å’Œå¥å¼åŒ¹é…ï¼‰"""
    # å…³é”®è¯åŒ¹é…
    for word in COLLOQUIAL_WORDS:
        if word in text:
            return True
    # è¿ç»­æ ‡ç‚¹åŒ¹é…ï¼ˆ3ä¸ªä»¥ä¸Šï¼‰
    if re.search(r"[ï¼ï¼Ÿã€‚,ï¼Œï¼›;ï¼š:]{3,}", text):
        return True
    # å£è¯­åŒ–å¥å¼åŒ¹é…
    colloquial_patterns = [
        r"[æˆ‘ä½ ä»–å¥¹å®ƒ]ï¼ˆä»¬ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"[è¿™é‚£å“ª]ï¼ˆä¸ªäº›ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"^[å“ˆå“ˆå˜¿å˜¿å˜»å˜»å‘µå‘µ]+"
    ]
    if any(re.search(pattern, text) for pattern in colloquial_patterns):
        return True
    return False

def is_sensitive(text):
    """æ£€æµ‹æ•æ„Ÿè¯é¢˜æ–‡æœ¬"""
    # æ•æ„Ÿå…³é”®è¯åŒ¹é…
    for category, words in SENSITIVE_KEYWORDS.items():
        for word in words:
            if word in text:
                return True
    # æ•æ„Ÿå¥å¼åŒ¹é…
    sensitive_patterns = [
        r"å‡ºå”®.*(è‰²æƒ…|AV|ä¸‰çº§ç‰‡)",
        r"(å«–å¨¼|å–æ·«|æ€§äº¤æ˜“).*(ä»·æ ¼|è”ç³»æ–¹å¼|åœ°ç‚¹)",
        r"(æ€äºº|æŠ¢åŠ«|ç»‘æ¶).*(æ–¹æ³•|æ•™ç¨‹|å·¥å…·)",
        r"(æ¯’å“|å¤§éº»|å†°æ¯’).*(è´­ä¹°|å‡ºå”®|è¿è¾“)",
        r"(å°ç‹¬|æ¸¯ç‹¬|ç–†ç‹¬).*(æ”¯æŒ|å®£ä¼ |åˆ†è£‚)"
    ]
    if any(re.search(pattern, text, re.IGNORECASE) for pattern in sensitive_patterns):
        return True
    return False

def has_academic_features(text):
    """æ£€æµ‹å­¦æœ¯ç‰¹å¾"""
    return any(re.search(pattern, text) for pattern in ACADEMIC_PATTERNS)

def preprocess_and_filter(data_generator, stats):
    """é¢„å¤„ç†+åŸºç¡€ç­›é€‰ï¼ˆé•¿åº¦ã€æ•æ„Ÿè¯ã€å£è¯­åŒ–ã€å­¦æœ¯ç‰¹å¾ï¼‰"""
    import logging
    start_time = time.time()
    logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + åŸºç¡€ç­›é€‰ï¼ˆå«æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼‰")
    
    filtered_data = []
    
    for item in tqdm(data_generator, desc="é¢„å¤„ç†+ç­›é€‰", total=stats["sampled_count"] if stats["sampled_count"] > 0 else None):
        text = item["text"]
        original_data = item["original_data"]
        source_file = item["source_file"]
        
        # 1. åŸºç¡€é•¿åº¦è¿‡æ»¤
        if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
            stats["preprocess_filtered"] += 1
            continue
        
        # 2. æ–‡æœ¬æ¸…æ´—
        text = re.sub(r"[\u200b\s]+", " ", text).strip()
        if not text:
            stats["preprocess_filtered"] += 1
            continue
        
        # 3. æ•æ„Ÿè¯é¢˜è¿‡æ»¤
        if is_sensitive(text):
            stats["sensitive_filtered"] += 1
            continue
        
        # 4. å£è¯­åŒ–ç­›é€‰
        if is_colloquial(text):
            stats["colloquial_filtered"] += 1
            continue
        
        # 5. å­¦æœ¯ç‰¹å¾ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        if ACADEMIC_REQUIRE and not has_academic_features(text):
            stats["non_academic_filtered"] += 1
            continue
        
        # è®°å½•ç‰¹å¾
        filtered_data.append({
            "text": text,
            "original_data": original_data,
            "source_file": source_file,
            "has_academic_features": has_academic_features(text)
        })
    
    remaining = len(filtered_data)
    logging.info(
        f"âœ… é¢„å¤„ç†+ç­›é€‰å®Œæˆ | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']}æ¡ | "
        f"æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼š{stats['sensitive_filtered']}æ¡ | å£è¯­åŒ–ï¼š{stats['colloquial_filtered']}æ¡ | "
        f"æ— å­¦æœ¯ç‰¹å¾ï¼š{stats['non_academic_filtered']}æ¡ | å‰©ä½™ï¼š{remaining}æ¡"
    )
    return filtered_data

def is_classic_chinese(text):
    """æ£€æµ‹å¤æ–‡ï¼ˆåŸºäºå…³é”®è¯å¯†åº¦å’Œå¥å¼ï¼‰"""
    # å«ç°ä»£è¯ç›´æ¥åˆ¤å®šä¸ºç°ä»£æ–‡
    for word in MODERN_CHINESE_WORDS:
        if word in text:
            return False
    
    total_chars = len(text)
    if total_chars == 0:
        return False
    
    # è®¡ç®—å¤æ–‡ç‰¹å¾è¯å¯†åº¦
    classic_char_count = 0
    for word in CLASSIC_CHINESE_WORDS:
        classic_char_count += text.count(word)
    density = classic_char_count / total_chars
    
    # å¤æ–‡å¥å¼åŒ¹é…
    classic_patterns = [
        r'^[\u4e00-\u9fff]{1,5}æ›°', r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^å‘œå‘¼', r'^å—Ÿå¤«',
        r'^ç›–é—»', r'^çªƒä»¥ä¸º', r'^è‡£é—»', r'^åœ£ç‹', r'^è´¤å›', r'^å¿ è‡£', r'^ä¹‰å£«'
    ]
    pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
    # åˆ¤å®šé€»è¾‘ï¼šå¯†åº¦è¾¾æ ‡ æˆ– å¥å¼åŒ¹é…
    return (density > CLASSIC_CHINESE_DENSITY_THRESHOLD) or pattern_match

def analyze_perplexity_distribution(minhash_data, sample_size=1000):
    """åˆ†æå›°æƒ‘åº¦åˆ†å¸ƒï¼Œè¿”å›åˆ†å±‚é˜ˆå€¼"""
    import logging
    logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç°ä»£æ–‡=ä½å›°æƒ‘åº¦ä¼˜è´¨ï¼Œå¤æ–‡=é«˜å›°æƒ‘åº¦çœŸå®ï¼‰")
    
    # æå–æœ‰æ•ˆæ–‡æœ¬
    texts = [item["text"] for item in minhash_data if len(item["text"]) >= MIN_CHAR_LEN]
    if len(texts) > sample_size:
        import random
        texts = random.sample(texts, sample_size)
    
    if not texts:
        logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
        return 80.0, 30.0  # é»˜è®¤é˜ˆå€¼
    
    logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
    # åˆ†ç±»å¤æ–‡/ç°ä»£æ–‡
    classic_texts = []
    modern_texts = []
    for text in texts:
        if is_classic_chinese(text):
            classic_texts.append(text)
        else:
            modern_texts.append(text)
    
    logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_perplexity_model()
    
    # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
    modern_perplexities = []
    for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
        batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        modern_perplexities.extend(batch_perplexities)
    
    classic_perplexities = []
    for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
        batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        classic_perplexities.extend(batch_perplexities)
    
    # å¤„ç†æœ‰æ•ˆæ•°æ®ï¼ˆè¿‡æ»¤å¼‚å¸¸å€¼ï¼‰
    modern_perplexities = np.array(modern_perplexities)
    classic_perplexities = np.array(classic_perplexities)
    
    modern_valid = modern_perplexities[modern_perplexities < 15000]
    classic_valid = classic_perplexities[classic_perplexities < 15000]
    
    if len(modern_valid) == 0:
        logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        modern_valid = np.array([100.0])
    
    if len(classic_valid) == 0:
        logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        classic_valid = np.array([30.0])
    
    # è®¡ç®—åˆ†ä½æ•°
    modern_percentiles = np.percentile(modern_valid, PERCENTILES)
    classic_percentiles = np.percentile(classic_valid, PERCENTILES)
    
    # è·å–ç›®æ ‡é˜ˆå€¼
    modern_threshold = modern_percentiles[MODERN_PERPLEXITY_PERCENTILE]
    classic_threshold = classic_percentiles[CLASSIC_PERPLEXITY_PERCENTILE]
    
    # æ—¥å¿—è¾“å‡º
    logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆä½å›°æƒ‘åº¦=è´¨é‡é«˜ã€æ¨¡å‹æ˜“ç†è§£ï¼‰:")
    for p, val in zip(PERCENTILES, modern_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f} ({PERCENTILES[MODERN_PERPLEXITY_PERCENTILE]}%åˆ†ä½æ•°)")
    
    logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆé«˜å›°æƒ‘åº¦=æ›´çœŸå®ã€éç°ä»£æ”¹å†™ï¼‰:")
    for p, val in zip(PERCENTILES, classic_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ å¤æ–‡é˜ˆå€¼: â‰¥{classic_threshold:.2f} ({PERCENTILES[CLASSIC_PERPLEXITY_PERCENTILE]}%åˆ†ä½æ•°)")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    plot_path = f"{OUTPUT_PATH}/layered_perplexity_distribution_final.png"
    if plot_perplexity_distribution(modern_valid, classic_valid, modern_threshold, classic_threshold,
                                   PERCENTILES, MODERN_PERPLEXITY_PERCENTILE, CLASSIC_PERPLEXITY_PERCENTILE, plot_path):
        logging.info(f"ğŸ“Š åˆ†å±‚åˆ†å¸ƒå›¾å·²ä¿å­˜: {plot_path}")
    
    return modern_threshold, classic_threshold

def layered_perplexity_filter(minhash_data, modern_threshold, classic_threshold, stats):
    """åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆåªç”Ÿæˆé«˜è´¨é‡æ•°æ®æ–‡ä»¶ï¼‰"""
    import logging
    import json
    start_time = time.time()
    
    # æ—¥å¿—è¯´æ˜
    modern_p = PERCENTILES[MODERN_PERPLEXITY_PERCENTILE]
    classic_p = PERCENTILES[CLASSIC_PERPLEXITY_PERCENTILE]
    logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰")
    logging.info(f"   - ç°ä»£æ–‡ï¼šâ‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™ä½å›°æƒ‘åº¦ä¼˜è´¨æ•°æ®")
    logging.info(f"   - å¤æ–‡ï¼šâ‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™é«˜å›°æƒ‘åº¦çœŸå®å¤æ–‡")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_perplexity_model()
    
    # è¾“å‡ºæ–‡ä»¶ï¼ˆåªä¿ç•™é«˜è´¨é‡æ•°æ®ï¼‰
    kept_file = f"{OUTPUT_PATH}/clmmu_kept_data_final.jsonl"
    
    batch_texts = []
    batch_data = []
    total_input = len(minhash_data)
    
    with open(kept_file, "w", encoding="utf-8") as f_kept:
        for item in tqdm(minhash_data, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰", total=total_input):
            try:
                batch_texts.append(item["text"])
                batch_data.append(item)
            except:
                continue
            
            if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
                perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
                for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                    data_item["perplexity"] = round(perp, 2)
                    data_item["is_classic"] = is_classic_chinese(text)
                    
                    # ç­›é€‰é€»è¾‘
                    if data_item["is_classic"]:
                        # å¤æ–‡ï¼šä¿ç•™â‰¥é˜ˆå€¼ä¸”æ— å¼‚å¸¸å€¼
                        if perp >= classic_threshold and perp < MAX_CLASSIC_PERPLEXITY:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "classic_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["classic_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                    else:
                        # ç°ä»£æ–‡ï¼šä¿ç•™â‰¤é˜ˆå€¼ä¸”æ— å¼‚å¸¸å€¼
                        if perp <= modern_threshold and perp < MAX_MODERN_PERPLEXITY:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "modern_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["modern_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                
                batch_texts = []
                batch_data = []
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if batch_texts:
            perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
            for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                data_item["perplexity"] = round(perp, 2)
                data_item["is_classic"] = is_classic_chinese(text)
                
                if data_item["is_classic"]:
                    if perp >= classic_threshold and perp < MAX_CLASSIC_PERPLEXITY:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "classic_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["classic_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
                else:
                    if perp <= modern_threshold and perp < MAX_MODERN_PERPLEXITY:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "modern_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["modern_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
    
    logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
    logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
    logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
    return kept_file