# import os
# import json
# import logging
# import mmap
# import numpy as np
# import psutil
# import threading
# import re
# from tqdm import tqdm
# from config import (
#     INPUT_DIR, SAMPLING_ENABLE, SAMPLE_RATIO, MAX_SAMPLE_COUNT,
#     COLLOQUIAL_WORDS, SENSITIVE_KEYWORDS, ACADEMIC_PATTERNS,
#     CLASSIC_CHINESE_WORDS, MODERN_CHINESE_WORDS, CLASSIC_DENSITY_THRESHOLD
# )

# # å…¨å±€ç»Ÿè®¡å˜é‡ï¼ˆè·¨æ–‡ä»¶å…±äº«ï¼‰
# stats = {
#     "total_input": 0,
#     "sampled_count": 0,
#     "preprocess_filtered": 0,
#     "colloquial_filtered": 0,
#     "non_academic_filtered": 0,
#     "md5_duplicated": 0,
#     "minhash_duplicated": 0,
#     "sensitive_filtered": 0,
#     "perplexity_filtered": 0,
#     "final_kept": 0,
#     "classic_chinese_kept": 0,
#     "modern_chinese_kept": 0,
#     "stage_time": {}
# }

# # ç›‘æ§çº¿ç¨‹å…¨å±€å˜é‡
# monitor_running = True
# gpu_util = 0
# cpu_mem = 0

# def get_gpu_utilization():
#     """è·å–GPUåˆ©ç”¨ç‡ï¼ˆä»…æ”¯æŒNVIDIAæ˜¾å¡ï¼‰"""
#     try:
#         result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
#         return int(result.strip().split("\n")[0]) if result else 0
#     except:
#         return 0

# def get_cpu_memory():
#     """è·å–å½“å‰è¿›ç¨‹CPUå†…å­˜å ç”¨ï¼ˆMBï¼‰"""
#     process = psutil.Process(os.getpid())
#     return round(process.memory_info().rss / (1024 * 1024), 2)

# def monitor_thread():
#     """ç›‘æ§çº¿ç¨‹ï¼šæ¯30ç§’è¾“å‡ºGPU/å†…å­˜çŠ¶æ€"""
#     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
#     while monitor_running:
#         global gpu_util, cpu_mem
#         gpu_util = get_gpu_utilization()
#         cpu_mem = get_cpu_memory()
        
#         # è®¡ç®—å·²å¤„ç†è¿›åº¦
#         total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - \
#                           stats["colloquial_filtered"] - stats["non_academic_filtered"] - \
#                           stats["md5_duplicated"] - stats["minhash_duplicated"] - stats["sensitive_filtered"]
#         progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
        
#         logging.info(
#             f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
#             f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
#         )
#         threading.Event().wait(30)  # æ›´ç¨³å®šçš„ä¼‘çœ 
#     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

# def load_jsonl_files_with_sampling():
#     """åŠ è½½JSONLæ–‡ä»¶ï¼ˆæ”¯æŒæŠ½æ ·æ¨¡å¼ï¼‰"""
#     jsonl_files = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith(".jsonl")]
#     if not jsonl_files:
#         raise ValueError(f"âŒ è¾“å…¥ç›®å½• {INPUT_DIR} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
#     logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
#     sampled_count = 0
#     for file_idx, file in enumerate(jsonl_files):
#         file_name = os.path.basename(file)
#         logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
#         with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
#             for line_bytes in iter(mm.readline, b""):
#                 line = line_bytes.decode("utf-8").strip()
#                 if not line:
#                     continue
#                 try:
#                     data = json.loads(line)
#                     text = data.get("text", "").strip()
#                     stats["total_input"] += 1
                    
#                     # æŠ½æ ·é€»è¾‘
#                     if SAMPLING_ENABLE:
#                         if sampled_count >= MAX_SAMPLE_COUNT:
#                             logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
#                             return
#                         if np.random.random() > SAMPLE_RATIO:
#                             continue
#                         sampled_count += 1
#                         stats["sampled_count"] = sampled_count
                    
#                     yield {"text": text, "original_data": data, "source_file": file_name}
#                 except Exception as e:
#                     stats["preprocess_filtered"] += 1
#                     continue
    
#     logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
#     logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

# def is_colloquial(text):
#     """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºå£è¯­åŒ–ï¼ˆå…³é”®è¯+å¥å¼åŒ¹é…ï¼‰"""
#     # 1. å£è¯­å…³é”®è¯åŒ¹é…
#     for word in COLLOQUIAL_WORDS:
#         if word in text:
#             return True
#     # 2. è¿ç»­æ ‡ç‚¹åŒ¹é…ï¼ˆ3ä¸ªä»¥ä¸Šï¼‰
#     if re.search(r"[ï¼ï¼Ÿã€‚,ï¼Œï¼›;ï¼š:]{3,}", text):
#         return True
#     # 3. å£è¯­åŒ–å¥å¼åŒ¹é…
#     colloquial_patterns = [
#         r"[æˆ‘ä½ ä»–å¥¹å®ƒ]ï¼ˆä»¬ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
#         r"[è¿™é‚£å“ª]ï¼ˆä¸ªäº›ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
#         r"^[å“ˆå“ˆå˜¿å˜¿å˜»å˜»å‘µå‘µ]+"
#     ]
#     if any(re.search(pattern, text) for pattern in colloquial_patterns):
#         return True
#     return False

# def is_sensitive(text):
#     """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«æ•æ„Ÿè¯é¢˜"""
#     # 1. æ•æ„Ÿå…³é”®è¯åŒ¹é…
#     for category, words in SENSITIVE_KEYWORDS.items():
#         for word in words:
#             if word in text:
#                 return True
#     # 2. æ•æ„Ÿå¥å¼åŒ¹é…
#     sensitive_patterns = [
#         r"å‡ºå”®.*(è‰²æƒ…|AV|ä¸‰çº§ç‰‡)",
#         r"(å«–å¨¼|å–æ·«|æ€§äº¤æ˜“).*(ä»·æ ¼|è”ç³»æ–¹å¼|åœ°ç‚¹)",
#         r"(æ€äºº|æŠ¢åŠ«|ç»‘æ¶).*(æ–¹æ³•|æ•™ç¨‹|å·¥å…·)",
#         r"(æ¯’å“|å¤§éº»|å†°æ¯’).*(è´­ä¹°|å‡ºå”®|è¿è¾“)",
#         r"(å°ç‹¬|æ¸¯ç‹¬|ç–†ç‹¬).*(æ”¯æŒ|å®£ä¼ |åˆ†è£‚)"
#     ]
#     if any(re.search(pattern, text, re.IGNORECASE) for pattern in sensitive_patterns):
#         return True
#     return False

# def has_academic_features(text):
#     """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«å­¦æœ¯ç‰¹å¾"""
#     return any(re.search(pattern, text) for pattern in ACADEMIC_PATTERNS)

# def is_classic_chinese(text):
#     """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºå¤æ–‡ï¼ˆå…³é”®è¯å¯†åº¦+å¥å¼åŒ¹é…ï¼‰"""
#     # 1. å«ç°ä»£è¯ç›´æ¥æ’é™¤
#     for word in MODERN_CHINESE_WORDS:
#         if word in text:
#             return False
    
#     total_chars = len(text)
#     if total_chars == 0:
#         return False
    
#     # 2. è®¡ç®—å¤æ–‡ç‰¹å¾è¯å¯†åº¦
#     classic_char_count = 0
#     for word in CLASSIC_CHINESE_WORDS:
#         classic_char_count += text.count(word)
#     density = classic_char_count / total_chars
    
#     # 3. å¤æ–‡å¥å¼åŒ¹é…
#     classic_patterns = [
#         r'^[\u4e00-\u9fff]{1,5}æ›°', r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^å‘œå‘¼', r'^å—Ÿå¤«',
#         r'^ç›–é—»', r'^çªƒä»¥ä¸º', r'^è‡£é—»', r'^åœ£ç‹', r'^è´¤å›', r'^å¿ è‡£', r'^ä¹‰å£«'
#     ]
#     pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
#     # 4. åˆ¤å®šé€»è¾‘ï¼šå¯†åº¦è¾¾æ ‡ æˆ– å¥å¼åŒ¹é…
#     return (density > CLASSIC_DENSITY_THRESHOLD) or pattern_match

import os
import json
import mmap
import time
import psutil
import threading
import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

from config import (
    MODEL_ID, DEVICE, MAX_SEQ_LENGTH, SAMPLING_ENABLE,
    SAMPLE_RATIO, MAX_SAMPLE_COUNT, MIN_CHAR_LEN
)

# å…¨å±€ç›‘æ§å˜é‡
monitor_running = True
gpu_util = 0
cpu_mem = 0

def get_gpu_utilization():
    """è·å–GPUåˆ©ç”¨ç‡"""
    try:
        result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
        return int(result.strip().split("\n")[0]) if result else 0
    except:
        return 0

def get_cpu_memory():
    """è·å–CPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return round(process.memory_info().rss / (1024 * 1024), 2)

def monitor_thread(stats):
    """ç›‘æ§çº¿ç¨‹ï¼šæ¯30ç§’è¾“å‡ºGPU/å†…å­˜çŠ¶æ€"""
    global monitor_running, gpu_util, cpu_mem
    import logging
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
    while monitor_running:
        gpu_util = get_gpu_utilization()
        cpu_mem = get_cpu_memory()
        total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - \
                          stats["colloquial_filtered"] - stats["non_academic_filtered"] - \
                          stats["md5_duplicated"] - stats["minhash_duplicated"] - stats["sensitive_filtered"]
        progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
        logging.info(
            f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
            f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
        )
        time.sleep(30)
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

def load_jsonl_files_with_sampling(input_dir, stats):
    """åŠ è½½JSONLæ–‡ä»¶å¹¶æ”¯æŒæŠ½æ ·"""
    import logging
    jsonl_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".jsonl")]
    if not jsonl_files:
        raise ValueError(f"âŒ è¾“å…¥ç›®å½• {input_dir} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
    logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
    sampled_count = 0
    for file_idx, file in enumerate(jsonl_files):
        file_name = os.path.basename(file)
        logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
        with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            for line_bytes in iter(mm.readline, b""):
                line = line_bytes.decode("utf-8").strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    text = data.get("text", "").strip()
                    stats["total_input"] += 1
                    
                    if SAMPLING_ENABLE:
                        if sampled_count >= MAX_SAMPLE_COUNT:
                            logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
                            return
                        if np.random.random() > SAMPLE_RATIO:
                            continue
                        sampled_count += 1
                        stats["sampled_count"] = sampled_count
                    
                    yield {"text": text, "original_data": data, "source_file": file_name}
                except:
                    stats["preprocess_filtered"] += 1
                    continue
    logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

def load_perplexity_model():
    """åŠ è½½å›°æƒ‘åº¦è®¡ç®—æ¨¡å‹"""
    import logging
    logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦ï¼ˆç”¨äºè¯„ä¼°æ–‡æœ¬è´¨é‡ï¼‰")
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
    model.eval()
    logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
    return tokenizer, model

def calculate_perplexity_batch_optimized(texts, tokenizer, model):
    """æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¼˜åŒ–ç»´åº¦åŒ¹é…ï¼‰"""
    try:
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=MAX_SEQ_LENGTH, 
            return_tensors="pt"
        ).to(DEVICE)
        
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        batch_size = input_ids.shape[0]
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            perplexities = []
            for i in range(batch_size):
                valid_mask = (input_ids[i] != tokenizer.pad_token_id)
                valid_token_ids = input_ids[i][valid_mask]
                
                if len(valid_token_ids) <= 1:
                    perplexities.append(float('inf'))
                    continue
                
                # ä¿®æ­£ç»´åº¦åŒ¹é…é€»è¾‘
                shift_logits = logits[i, :-1, :].contiguous()
                shift_labels = valid_token_ids[1:].contiguous()
                valid_len = len(shift_labels)
                shift_logits = shift_logits[:valid_len, :]
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)), 
                    shift_labels.view(-1)
                )
                
                perplexity = torch.exp(loss).item()
                perplexities.append(perplexity)
            
            return perplexities
            
    except RuntimeError as e:
        import logging
        if "out of memory" in str(e):
            logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
            half_size = len(texts) // 2
            if half_size >= 1:
                perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
                perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
                return perplexities1 + perplexities2
            else:
                return [calculate_perplexity(text, tokenizer, model) for text in texts]
        else:
            raise e
    except Exception as e:
        import logging
        logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
        return [calculate_perplexity(text, tokenizer, model) for text in texts]

def calculate_perplexity(text, tokenizer, model):
    """å•æ–‡æœ¬å›°æƒ‘åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["input_ids"][..., 1:].contiguous()
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            avg_loss = loss.mean()
            return torch.exp(avg_loss).item()
    except:
        return float('inf')

def plot_perplexity_distribution(modern_valid, classic_valid, modern_threshold, classic_threshold, percentiles, modern_idx, classic_idx, output_path):
    """ç»˜åˆ¶å›°æƒ‘åº¦åˆ†å¸ƒå›¾"""
    try:
        plt.figure(figsize=(12, 8))
        plt.hist(modern_valid, bins=50, alpha=0.7, label='Modern Chinese (Low Perplexity = High Quality)', color='skyblue', edgecolor='black')
        plt.hist(classic_valid, bins=50, alpha=0.7, label='Classic Chinese (High Perplexity = Authentic)', color='salmon', edgecolor='black')
        plt.axvline(modern_threshold, color='blue', linestyle='--', linewidth=2, label=f'Modern â‰¤ {modern_threshold:.2f} ({percentiles[modern_idx]}%tile)')
        plt.axvline(classic_threshold, color='red', linestyle='--', linewidth=2, label=f'Classic â‰¥ {classic_threshold:.2f} ({percentiles[classic_idx]}%tile)')
        plt.xlabel('Perplexity', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.title('Layered Perplexity Distribution (CLMMU/CEVAL Metric Priority)', fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        return True
    except Exception as e:
        import logging
        logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
        return False