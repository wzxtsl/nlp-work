# # import os
# # import json
# # import torch
# # import logging
# # import mmap
# # import time
# # import hashlib
# # import psutil
# # import threading
# # import re
# # import numpy as np
# # from transformers import AutoTokenizer, AutoModelForCausalLM
# # from tqdm import tqdm
# # from datasketch import MinHash, MinHashLSH

# # # å¿½ç•¥æ— å…³è­¦å‘Š
# # import warnings
# # warnings.filterwarnings("ignore", category=UserWarning)

# # # ========== æ ¸å¿ƒé…ç½® ==========
# # INPUT_DIR = "data"  # è¾“å…¥JSONLæ–‡ä»¶ç›®å½•
# # INTERMEDIATE_PATH = "data/intermediate"  # ä¸­é—´æ–‡ä»¶ç›®å½•
# # OUTPUT_PATH = "data/output"  # è¾“å‡ºæ–‡ä»¶ç›®å½•
# # os.makedirs(INTERMEDIATE_PATH, exist_ok=True)
# # os.makedirs(OUTPUT_PATH, exist_ok=True)

# # # æŠ½æ ·é…ç½®
# # SAMPLING_ENABLE = True
# # SAMPLE_RATIO = 0.01
# # MAX_SAMPLE_COUNT = 10000

# # # æ‰¹é‡é…ç½®
# # BATCH_SIZE_PREPROCESS = 1024
# # BATCH_SIZE_PERPLEXITY = 16
# # BATCH_SIZE_MINHASH = 5000

# # # æ•°æ®è¿‡æ»¤åŸºç¡€é…ç½®
# # MIN_CHAR_LEN = 10
# # MAX_CHAR_LEN = 10000
# # PERPLEXITY_THRESHOLD = None
# # MAX_SEQ_LENGTH = 512

# # # å»é‡é…ç½®
# # MINHASH_NUM_PERM = 128
# # LSH_THRESHOLD = 0.8

# # # æ¨¡å‹é…ç½®ï¼ˆGPT2 ä¸­æ–‡è‡ªå›å½’ï¼‰
# # MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
# # DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# # # å…¨å±€ç»Ÿè®¡å˜é‡
# # stats = {
# #     "total_input": 0,
# #     "sampled_count": 0,
# #     "preprocess_filtered": 0,
# #     "md5_duplicated": 0,
# #     "minhash_duplicated": 0,
# #     "perplexity_filtered": 0,
# #     "final_kept": 0,
# #     "stage_time": {}
# # }

# # # ç›‘æ§çº¿ç¨‹å˜é‡
# # monitor_running = True
# # gpu_util = 0
# # cpu_mem = 0

# # # ========== å·¥å…·å‡½æ•° ==========
# # def get_gpu_utilization():
# #     try:
# #         result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
# #         return int(result.strip().split("\n")[0]) if result else 0
# #     except:
# #         return 0

# # def get_cpu_memory():
# #     process = psutil.Process(os.getpid())
# #     return round(process.memory_info().rss / (1024 * 1024), 2)

# # def monitor_thread():
# #     global monitor_running, gpu_util, cpu_mem
# #     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
# #     while monitor_running:
# #         gpu_util = get_gpu_utilization()
# #         cpu_mem = get_cpu_memory()
# #         total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - stats["md5_duplicated"] - stats["minhash_duplicated"]
# #         progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
# #         logging.info(
# #             f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
# #             f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
# #         )
# #         time.sleep(30)
# #     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

# # def load_jsonl_files_with_sampling(input_dir):
# #     jsonl_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".jsonl")]
# #     if not jsonl_files:
# #         raise ValueError(f"âŒ è¾“å…¥ç›®å½• {input_dir} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
# #     logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
# #     sampled_count = 0
# #     for file_idx, file in enumerate(jsonl_files):
# #         file_name = os.path.basename(file)
# #         logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
# #         with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
# #             for line_bytes in iter(mm.readline, b""):
# #                 line = line_bytes.decode("utf-8").strip()
# #                 if not line:
# #                     continue
# #                 try:
# #                     data = json.loads(line)
# #                     text = data.get("text", "").strip()
# #                     stats["total_input"] += 1
                    
# #                     if SAMPLING_ENABLE:
# #                         if sampled_count >= MAX_SAMPLE_COUNT:
# #                             logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
# #                             return
# #                         if np.random.random() > SAMPLE_RATIO:
# #                             continue
# #                         sampled_count += 1
# #                         stats["sampled_count"] = sampled_count
                    
# #                     yield {"text": text, "original_data": data, "source_file": file_name}
# #                 except:
# #                     stats["preprocess_filtered"] += 1
# #                     continue
# #     logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
# #     logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

# # # ========== 1. é¢„å¤„ç† + MD5å»é‡ ==========
# # def preprocess_and_md5_deduplicate():
# #     start_time = time.time()
# #     logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡")
    
# #     md5_set = set()
# #     output_file = os.path.join(INTERMEDIATE_PATH, "preprocessed_md5_dedup.jsonl")
# #     batch_buffer = []
    
# #     with open(output_file, "w", encoding="utf-8") as f_out:
# #         for item in tqdm(load_jsonl_files_with_sampling(INPUT_DIR), desc="é¢„å¤„ç†+MD5å»é‡", total=stats["sampled_count"] if SAMPLING_ENABLE else None):
# #             text = item["text"]
# #             original_data = item["original_data"]
# #             source_file = item["source_file"]
            
# #             if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
# #                 stats["preprocess_filtered"] += 1
# #                 continue
# #             text = re.sub(r"[\u200b\s]+", " ", text).strip()
            
# #             md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
# #             if md5_hash in md5_set:
# #                 stats["md5_duplicated"] += 1
# #                 continue
# #             md5_set.add(md5_hash)
            
# #             batch_buffer.append({"text": text, "original_data": original_data, "source_file": source_file, "md5": md5_hash})
# #             if len(batch_buffer) >= BATCH_SIZE_PREPROCESS:
# #                 for data in batch_buffer:
# #                     f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
# #                 batch_buffer = []
        
# #         if batch_buffer:
# #             for data in batch_buffer:
# #                 f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
    
# #     stats["stage_time"]["preprocess_md5"] = round(time.time() - start_time, 2)
# #     remaining = stats["sampled_count"] - stats["preprocess_filtered"] - stats["md5_duplicated"]
# #     logging.info(
# #         f"âœ… é¢„å¤„ç†+MD5å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['preprocess_md5']}ç§’ | "
# #         f"æŠ½æ ·åï¼š{stats['sampled_count']} | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']} | "
# #         f"å®Œå…¨é‡å¤ï¼ˆMD5ï¼‰ï¼š{stats['md5_duplicated']} | å‰©ä½™ï¼š{remaining}"
# #     )
# #     return output_file

# # # ========== 2. Minhash LSHè¯­ä¹‰å»é‡ ==========
# # def create_minhash_signature(text, num_perm=MINHASH_NUM_PERM):
# #     minhash = MinHash(num_perm=num_perm)
# #     for token in list(text):
# #         token_hash = hashlib.sha256(token.encode('utf-8')).hexdigest()
# #         minhash.update(token_hash.encode('utf-8'))
# #     return minhash

# # def minhash_lsh_deduplicate(input_file):
# #     start_time = time.time()
# #     logging.info("ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡")
    
# #     texts = []
# #     data_list = []
# #     with open(input_file, "r", encoding="utf-8") as f:
# #         for line in tqdm(f, desc="è¯»å–é¢„å¤„ç†æ•°æ®"):
# #             data = json.loads(line)
# #             texts.append(data["text"])
# #             data_list.append(data)
    
# #     if not texts:
# #         raise ValueError("âŒ MD5å»é‡åæ— æœ‰æ•ˆæ•°æ®")
    
# #     lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
# #     keep_indices = []
# #     duplicate_count = 0
    
# #     for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
# #         minhash = create_minhash_signature(texts[i])
# #         similar_docs = lsh.query(minhash)
# #         if not similar_docs:
# #             lsh.insert(str(i), minhash)
# #             keep_indices.append(i)
# #         else:
# #             should_keep = True
# #             for doc_id in similar_docs:
# #                 if int(doc_id) < i:
# #                     should_keep = False
# #                     break
# #             if should_keep:
# #                 lsh.insert(str(i), minhash)
# #                 keep_indices.append(i)
# #             else:
# #                 duplicate_count += 1
    
# #     stats["minhash_duplicated"] = duplicate_count
# #     output_file = os.path.join(INTERMEDIATE_PATH, "minhash_dedup.jsonl")
# #     with open(output_file, "w", encoding="utf-8") as f_out:
# #         for idx in keep_indices:
# #             f_out.write(json.dumps(data_list[idx], ensure_ascii=False) + "\n")
    
# #     stats["stage_time"]["minhash_lsh"] = round(time.time() - start_time, 2)
# #     remaining = len(keep_indices)
# #     logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['minhash_lsh']}ç§’ | "
# #                  f"è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']} | å‰©ä½™ï¼š{remaining}")
# #     return output_file

# # # ========== 3. å›°æƒ‘åº¦åˆ†æ ==========
# # def load_perplexity_model():
# #     logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦")
# #     start_time = time.time()
# #     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
# #     if tokenizer.pad_token is None:
# #         tokenizer.pad_token = tokenizer.eos_token
# #     model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
# #     model.eval()
# #     logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
# #     return tokenizer, model

# # def calculate_perplexity(text, tokenizer, model):
# #     try:
# #         inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
# #         with torch.no_grad():
# #             outputs = model(**inputs)
# #             logits = outputs.logits
# #             shift_logits = logits[..., :-1, :].contiguous()
# #             shift_labels = inputs["input_ids"][..., 1:].contiguous()
# #             loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
# #             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
# #             avg_loss = loss.mean()
# #             return torch.exp(avg_loss).item()
# #     except:
# #         return float('inf')

# # def calculate_perplexity_batch(texts, tokenizer, model):
# #     return [calculate_perplexity(text, tokenizer, model) for text in texts]

# # def setup_chinese_font():
# #     """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³matplotlibä¸­æ–‡æ˜¾ç¤ºé—®é¢˜"""
# #     try:
# #         import matplotlib.pyplot as plt
# #         import matplotlib.font_manager as fm
        
# #         # å°è¯•å¤šç§ä¸­æ–‡å­—ä½“
# #         chinese_fonts = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial Unicode MS', 'STHeiti']
        
# #         for font_name in chinese_fonts:
# #             if font_name in [f.name for f in fm.fontManager.ttflist]:
# #                 plt.rcParams['font.family'] = font_name
# #                 break
# #         else:
# #             # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“å¹¶è­¦å‘Š
# #             logging.warning("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹å—")
        
# #         # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
# #         plt.rcParams['axes.unicode_minus'] = False
        
# #         return True
# #     except Exception as e:
# #         logging.warning(f"âš ï¸ è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {str(e)}")
# #         return False
    
# # def analyze_perplexity_distribution(input_file, sample_size=1000):
# #     logging.info("ğŸ” å¼€å§‹å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æ")
# #     tokenizer, model = load_perplexity_model()
    
# #     # è¯»å–æ•°æ®
# #     texts = []
# #     with open(input_file, "r", encoding="utf-8") as f:
# #         lines = f.readlines()
# #         if len(lines) > sample_size:
# #             import random
# #             lines = random.sample(lines, sample_size)
# #         for line in lines:
# #             try:
# #                 data = json.loads(line)
# #                 text = data.get("text", "").strip()
# #                 if text and len(text) >= MIN_CHAR_LEN:  # ç¡®ä¿æ–‡æœ¬æœ‰æ•ˆ
# #                     texts.append(text)
# #             except:
# #                 continue
    
# #     if not texts:
# #         logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
# #         return np.array([50.0] * 9)  # è¿”å›é»˜è®¤å€¼
    
# #     logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
# #     # è®¡ç®—å›°æƒ‘åº¦
# #     perplexities = []
# #     for i in tqdm(range(0, len(texts), BATCH_SIZE_PERPLEXITY), desc="åˆ†æå›°æƒ‘åº¦åˆ†å¸ƒ"):
# #         batch = texts[i:i+BATCH_SIZE_PERPLEXITY]
# #         batch_perplexities = calculate_perplexity_batch(batch, tokenizer, model)
# #         perplexities.extend(batch_perplexities)
    
# #     # å¤„ç†ç»“æœ
# #     perplexities = np.array(perplexities)
# #     valid_perplexities = perplexities[perplexities < 10000]  # è¿‡æ»¤å¼‚å¸¸å€¼
    
# #     if len(valid_perplexities) == 0:
# #         logging.error("âŒ æ— æ³•è®¡ç®—å›°æƒ‘åº¦åˆ†å¸ƒï¼šæ‰€æœ‰æ ·æœ¬å›°æƒ‘åº¦ä¸º inf æˆ– NaN")
# #         return np.array([50.0] * 9)
    
# #     # è®¡ç®—ç™¾åˆ†ä½æ•°
# #     percentiles = [0, 10, 25, 50, 75, 90, 95, 99, 100]
# #     percentile_values = np.percentile(valid_perplexities, percentiles)
    
# #     logging.info("ğŸ“ˆ å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æç»“æœï¼š")
# #     for p, val in zip(percentiles, percentile_values):
# #         logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    
# #     # ç»˜åˆ¶åˆ†å¸ƒå›¾ï¼ˆä¿®å¤å­—ä½“é—®é¢˜ï¼‰
# #     try:
# #         import matplotlib.pyplot as plt
        
# #         # è®¾ç½®ä¸­æ–‡å­—ä½“
# #         setup_chinese_font()
        
# #         plt.figure(figsize=(12, 8))
        
# #         # ç»˜åˆ¶ç›´æ–¹å›¾
# #         n, bins, patches = plt.hist(valid_perplexities, bins=50, alpha=0.7, 
# #                                    edgecolor='black', color='skyblue')
        
# #         # æ·»åŠ ä¸­ä½æ•°å’Œ90%åˆ†ä½çº¿
# #         median_line = plt.axvline(percentile_values[3], color='red', linestyle='--', 
# #                                  linewidth=2, label=f'Median: {percentile_values[3]:.2f}')
# #         p90_line = plt.axvline(percentile_values[5], color='orange', linestyle='--', 
# #                               linewidth=2, label=f'90%: {percentile_values[5]:.2f}')
        
# #         # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜ï¼‰
# #         plt.xlabel('Perplexity', fontsize=12)
# #         plt.ylabel('Frequency', fontsize=12)
# #         plt.title('Perplexity Distribution Histogram', fontsize=14, fontweight='bold')
        
# #         # æ·»åŠ å›¾ä¾‹
# #         plt.legend(fontsize=10)
        
# #         # æ·»åŠ ç½‘æ ¼
# #         plt.grid(True, alpha=0.3)
        
# #         # è°ƒæ•´å¸ƒå±€
# #         plt.tight_layout()
        
# #         # ä¿å­˜å›¾ç‰‡
# #         output_path = os.path.join(OUTPUT_PATH, 'perplexity_distribution.png')
# #         plt.savefig(output_path, dpi=300, bbox_inches='tight')
# #         plt.close()
        
# #         logging.info(f"ğŸ“Š åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
        
# #         # é¢å¤–ä¿å­˜ä¸€ä¸ªæ–‡æœ¬ç‰ˆæœ¬çš„åˆ†æç»“æœ
# #         analysis_file = os.path.join(OUTPUT_PATH, 'perplexity_analysis.txt')
# #         with open(analysis_file, 'w', encoding='utf-8') as f:
# #             f.write("å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æç»“æœ\n")
# #             f.write("=" * 50 + "\n")
# #             for p, val in zip(percentiles, percentile_values):
# #                 f.write(f"{p}% åˆ†ä½æ•°: {val:.2f}\n")
# #             f.write(f"\næ ·æœ¬æ•°é‡: {len(valid_perplexities)}\n")
# #             f.write(f"æ¨èé˜ˆå€¼ (90%åˆ†ä½æ•°): {percentile_values[5]:.2f}\n")
        
# #         logging.info(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
        
# #     except Exception as e:
# #         logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
    
# #     return percentile_values

# # # ========== 4. ç¡®å®šé˜ˆå€¼ & ç­›é€‰ ==========
# # def determine_perplexity_threshold(input_file):
# #     percentiles = analyze_perplexity_distribution(input_file, sample_size=500)
# #     recommended_threshold = percentiles[5]
# #     logging.info(f"ğŸ¤– è‡ªåŠ¨æ¨èé˜ˆå€¼: {recommended_threshold:.2f}")
# #     return int(recommended_threshold)

# # def perplexity_filter(input_file, threshold):
# #     start_time = time.time()
# #     logging.info(f"ğŸš€ å¼€å§‹å›°æƒ‘åº¦ç­›é€‰ï¼ˆé˜ˆå€¼â‰¤{threshold}ï¼‰")
    
# #     tokenizer, model = load_perplexity_model()
    
# #     kept_file = os.path.join(OUTPUT_PATH, "kept_data.jsonl")
# #     filtered_file = os.path.join(OUTPUT_PATH, "filtered_data.jsonl")
    
# #     batch_texts = []
# #     batch_data = []
    
# #     with open(input_file, "r", encoding="utf-8") as f_in, \
# #          open(kept_file, "w", encoding="utf-8") as f_kept, \
# #          open(filtered_file, "w", encoding="utf-8") as f_filtered:
        
# #         for line in tqdm(f_in, desc="è®¡ç®—å›°æƒ‘åº¦å¹¶ç­›é€‰"):
# #             data = json.loads(line)
# #             batch_texts.append(data["text"])
# #             batch_data.append(data)
            
# #             if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
# #                 perplexities = calculate_perplexity_batch(batch_texts, tokenizer, model)
# #                 for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
# #                     data_item["perplexity"] = round(perp,2)
# #                     if perp <= threshold:
# #                         final_data = data_item["original_data"].copy()
# #                         final_data["cleaned_text"] = text
# #                         final_data["md5"] = data_item["md5"]
# #                         final_data["perplexity"] = data_item["perplexity"]
# #                         final_data["source_file"] = data_item["source_file"]
# #                         f_kept.write(json.dumps(final_data, ensure_ascii=False)+"\n")
# #                         stats["final_kept"] += 1
# #                     else:
# #                         filtered_data = data_item.copy()
# #                         filtered_data["filter_reason"] = f"å›°æƒ‘åº¦è¶…å‡ºé˜ˆå€¼({perp:.2f}>{threshold})"
# #                         f_filtered.write(json.dumps(filtered_data, ensure_ascii=False)+"\n")
# #                         stats["perplexity_filtered"] += 1
# #                 batch_texts=[]
# #                 batch_data=[]
        
# #         if batch_texts:
# #             perplexities = calculate_perplexity_batch(batch_texts, tokenizer, model)
# #             for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
# #                 data_item["perplexity"] = round(perp,2)
# #                 if perp <= threshold:
# #                     final_data = data_item["original_data"].copy()
# #                     final_data["cleaned_text"] = text
# #                     final_data["md5"] = data_item["md5"]
# #                     final_data["perplexity"] = data_item["perplexity"]
# #                     final_data["source_file"] = data_item["source_file"]
# #                     f_kept.write(json.dumps(final_data, ensure_ascii=False)+"\n")
# #                     stats["final_kept"] += 1
# #                 else:
# #                     filtered_data = data_item.copy()
# #                     filtered_data["filter_reason"] = f"å›°æƒ‘åº¦è¶…å‡ºé˜ˆå€¼({perp:.2f}>{threshold})"
# #                     f_filtered.write(json.dumps(filtered_data, ensure_ascii=False)+"\n")
# #                     stats["perplexity_filtered"] += 1
    
# #     stats["stage_time"]["perplexity"] = round(time.time() - start_time,2)
# #     logging.info(f"âœ… å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’ | ä½è´¨é‡è¿‡æ»¤ï¼š{stats['perplexity_filtered']} | æœ€ç»ˆä¿ç•™ï¼š{stats['final_kept']}")
# #     return kept_file, filtered_file

# # # ========== ä¸»å‡½æ•° ==========
# # def main():
# #     global monitor_running
# #     start_time = time.time()
    
# #     logging.basicConfig(
# #         level=logging.INFO,
# #         format="%(asctime)s - %(levelname)s - %(message)s",
# #         handlers=[
# #             logging.FileHandler(os.path.join(OUTPUT_PATH, "filter_log.log"), encoding="utf-8"),
# #             logging.StreamHandler()
# #         ]
# #     )
    
# #     # å¯åŠ¨ç›‘æ§çº¿ç¨‹
# #     monitor = threading.Thread(target=monitor_thread, daemon=True)
# #     monitor.start()
    
# #     # 1. é¢„å¤„ç†+MD5å»é‡
# #     preprocessed_file = preprocess_and_md5_deduplicate()
    
# #     # 2. Minhash LSHå»é‡
# #     minhash_file = minhash_lsh_deduplicate(preprocessed_file)
    
# #     # 3. ç¡®å®šå›°æƒ‘åº¦é˜ˆå€¼
# #     threshold = determine_perplexity_threshold(minhash_file)
    
# #     # 4. å›°æƒ‘åº¦ç­›é€‰
# #     kept_file, filtered_file = perplexity_filter(minhash_file, threshold)
    
# #     # åœæ­¢ç›‘æ§
# #     monitor_running = False
# #     monitor.join()
    
# #     total_time = round(time.time() - start_time,2)
# #     logging.info(f"ğŸ‰ å…¨æµç¨‹å®Œæˆ | æ€»è€—æ—¶ï¼š{total_time}ç§’")
# #     logging.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼š{stats}")

# # if __name__ == "__main__":
# #     main()


# import os
# import json
# import torch
# import logging
# import mmap
# import time
# import hashlib
# import psutil
# import threading
# import re
# import numpy as np
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from tqdm import tqdm
# from datasketch import MinHash, MinHashLSH

# # å¿½ç•¥æ— å…³è­¦å‘Š
# import warnings
# warnings.filterwarnings("ignore", category=UserWarning)

# # ========== æ ¸å¿ƒé…ç½® ==========
# INPUT_DIR = "data"
# INTERMEDIATE_PATH = "data/intermediate"
# OUTPUT_PATH = "data/output"
# os.makedirs(INTERMEDIATE_PATH, exist_ok=True)
# os.makedirs(OUTPUT_PATH, exist_ok=True)

# # æŠ½æ ·é…ç½®
# SAMPLING_ENABLE = True
# SAMPLE_RATIO = 0.01
# MAX_SAMPLE_COUNT = 1000

# # æ‰¹é‡é…ç½®
# BATCH_SIZE_PREPROCESS = 1024
# BATCH_SIZE_PERPLEXITY = 32  # å¢åŠ æ‰¹é‡æé«˜GPUåˆ©ç”¨ç‡
# BATCH_SIZE_MINHASH = 5000

# # æ•°æ®è¿‡æ»¤åŸºç¡€é…ç½®
# MIN_CHAR_LEN = 10
# MAX_CHAR_LEN = 10000
# MAX_SEQ_LENGTH = 512

# # åˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼é…ç½®
# CLASSIC_CHINESE_THRESHOLD = 40.55  # å¤æ–‡é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼ä¿ç•™
# MODERN_CHINESE_THRESHOLD = None    # ç°ä»£æ–‡é˜ˆå€¼ï¼Œè‡ªåŠ¨ç¡®å®š

# # å»é‡é…ç½®
# MINHASH_NUM_PERM = 128
# LSH_THRESHOLD = 0.8

# # æ¨¡å‹é…ç½®
# MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
# DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# # å…¨å±€ç»Ÿè®¡å˜é‡
# stats = {
#     "total_input": 0,
#     "sampled_count": 0,
#     "preprocess_filtered": 0,
#     "md5_duplicated": 0,
#     "minhash_duplicated": 0,
#     "perplexity_filtered": 0,
#     "final_kept": 0,
#     "classic_chinese_kept": 0,  # æ–°å¢ï¼šå¤æ–‡ä¿ç•™æ•°é‡
#     "modern_chinese_kept": 0,   # æ–°å¢ï¼šç°ä»£æ–‡ä¿ç•™æ•°é‡
#     "stage_time": {}
# }

# # ç›‘æ§çº¿ç¨‹å˜é‡
# monitor_running = True
# gpu_util = 0
# cpu_mem = 0

# # ========== å·¥å…·å‡½æ•° ==========
# def get_gpu_utilization():
#     try:
#         result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
#         return int(result.strip().split("\n")[0]) if result else 0
#     except:
#         return 0

# def get_cpu_memory():
#     process = psutil.Process(os.getpid())
#     return round(process.memory_info().rss / (1024 * 1024), 2)

# def monitor_thread():
#     global monitor_running, gpu_util, cpu_mem
#     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
#     while monitor_running:
#         gpu_util = get_gpu_utilization()
#         cpu_mem = get_cpu_memory()
#         total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - stats["md5_duplicated"] - stats["minhash_duplicated"]
#         progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
#         logging.info(
#             f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
#             f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
#         )
#         time.sleep(30)
#     logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

# def load_jsonl_files_with_sampling(input_dir):
#     jsonl_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".jsonl")]
#     if not jsonl_files:
#         raise ValueError(f"âŒ è¾“å…¥ç›®å½• {input_dir} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
#     logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
#     sampled_count = 0
#     for file_idx, file in enumerate(jsonl_files):
#         file_name = os.path.basename(file)
#         logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
#         with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
#             for line_bytes in iter(mm.readline, b""):
#                 line = line_bytes.decode("utf-8").strip()
#                 if not line:
#                     continue
#                 try:
#                     data = json.loads(line)
#                     text = data.get("text", "").strip()
#                     stats["total_input"] += 1
                    
#                     if SAMPLING_ENABLE:
#                         if sampled_count >= MAX_SAMPLE_COUNT:
#                             logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
#                             return
#                         if np.random.random() > SAMPLE_RATIO:
#                             continue
#                         sampled_count += 1
#                         stats["sampled_count"] = sampled_count
                    
#                     yield {"text": text, "original_data": data, "source_file": file_name}
#                 except:
#                     stats["preprocess_filtered"] += 1
#                     continue
#     logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
#     logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

# # ========== 1. é¢„å¤„ç† + MD5å»é‡ ==========
# def preprocess_and_md5_deduplicate():
#     start_time = time.time()
#     logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡")
    
#     md5_set = set()
#     output_file = os.path.join(INTERMEDIATE_PATH, "preprocessed_md5_dedup.jsonl")
#     batch_buffer = []
    
#     with open(output_file, "w", encoding="utf-8") as f_out:
#         for item in tqdm(load_jsonl_files_with_sampling(INPUT_DIR), desc="é¢„å¤„ç†+MD5å»é‡", total=stats["sampled_count"] if SAMPLING_ENABLE else None):
#             text = item["text"]
#             original_data = item["original_data"]
#             source_file = item["source_file"]
            
#             if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
#                 stats["preprocess_filtered"] += 1
#                 continue
#             text = re.sub(r"[\u200b\s]+", " ", text).strip()
            
#             md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
#             if md5_hash in md5_set:
#                 stats["md5_duplicated"] += 1
#                 continue
#             md5_set.add(md5_hash)
            
#             batch_buffer.append({"text": text, "original_data": original_data, "source_file": source_file, "md5": md5_hash})
#             if len(batch_buffer) >= BATCH_SIZE_PREPROCESS:
#                 for data in batch_buffer:
#                     f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
#                 batch_buffer = []
        
#         if batch_buffer:
#             for data in batch_buffer:
#                 f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
    
#     stats["stage_time"]["preprocess_md5"] = round(time.time() - start_time, 2)
#     remaining = stats["sampled_count"] - stats["preprocess_filtered"] - stats["md5_duplicated"]
#     logging.info(
#         f"âœ… é¢„å¤„ç†+MD5å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['preprocess_md5']}ç§’ | "
#         f"æŠ½æ ·åï¼š{stats['sampled_count']} | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']} | "
#         f"å®Œå…¨é‡å¤ï¼ˆMD5ï¼‰ï¼š{stats['md5_duplicated']} | å‰©ä½™ï¼š{remaining}"
#     )
#     return output_file

# # ========== 2. Minhash LSHè¯­ä¹‰å»é‡ ==========
# def create_minhash_signature(text, num_perm=MINHASH_NUM_PERM):
#     minhash = MinHash(num_perm=num_perm)
#     for token in list(text):
#         token_hash = hashlib.sha256(token.encode('utf-8')).hexdigest()
#         minhash.update(token_hash.encode('utf-8'))
#     return minhash

# def minhash_lsh_deduplicate(input_file):
#     start_time = time.time()
#     logging.info("ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡")
    
#     texts = []
#     data_list = []
#     with open(input_file, "r", encoding="utf-8") as f:
#         for line in tqdm(f, desc="è¯»å–é¢„å¤„ç†æ•°æ®"):
#             data = json.loads(line)
#             texts.append(data["text"])
#             data_list.append(data)
    
#     if not texts:
#         logging.warning("âš ï¸ MD5å»é‡åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
#         return input_file
    
#     lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
#     keep_indices = []
#     duplicate_count = 0
    
#     for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
#         minhash = create_minhash_signature(texts[i])
#         similar_docs = lsh.query(minhash)
#         if not similar_docs:
#             lsh.insert(str(i), minhash)
#             keep_indices.append(i)
#         else:
#             should_keep = True
#             for doc_id in similar_docs:
#                 if int(doc_id) < i:
#                     should_keep = False
#                     break
#             if should_keep:
#                 lsh.insert(str(i), minhash)
#                 keep_indices.append(i)
#             else:
#                 duplicate_count += 1
    
#     stats["minhash_duplicated"] = duplicate_count
#     output_file = os.path.join(INTERMEDIATE_PATH, "minhash_dedup.jsonl")
#     with open(output_file, "w", encoding="utf-8") as f_out:
#         for idx in keep_indices:
#             f_out.write(json.dumps(data_list[idx], ensure_ascii=False) + "\n")
    
#     stats["stage_time"]["minhash_lsh"] = round(time.time() - start_time, 2)
#     remaining = len(keep_indices)
#     logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['minhash_lsh']}ç§’ | "
#                  f"è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']} | å‰©ä½™ï¼š{remaining}")
#     return output_file

# # ========== 3. å¤æ–‡æ£€æµ‹å‡½æ•° ==========
# def is_classic_chinese(text):
#     """
#     æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºå¤æ–‡
#     åŸºäºå¤æ–‡ç‰¹å¾è¯å’Œå­—ç¬¦æ¯”ä¾‹åˆ¤æ–­
#     """
#     # å¤æ–‡å¸¸è§ç‰¹å¾è¯
#     classic_words = [
#         'ä¹‹', 'ä¹', 'è€…', 'ä¹Ÿ', 'æ›°', 'å¾', 'æ±', 'å°”', 'ä¹ƒ', 'å…®', 'å­”å­','è€å­','åº„å­','é“å®¶','å¢¨å®¶','æ³•å®¶'
#         'çŸ£', 'å“‰', 'è€¶', 'æ¬¤', 'å¤«', 'ç›–', 'æ•…', 'ç„¶', 'åˆ™', 'è€Œ','ç§¦','æ±‰','æ˜ä»£','å”','æ˜æœ','å®‹æœ','å®‹ä»£'
#         'ä»¥', 'äº', 'ä¸º', 'å…¶', 'æ‰€', 'è¯¸', 'ç„‰', 'è€³', 'å·²', 'äº‘','ä¸‰å›½','è¥¿æ±‰','ä¸œæ±‰','å—åŒ—æœ','åŒ—å®‹','å—å®‹','å’¸ä¸°','éƒ‘å’Œ'
#     ]
    
#     # å¤æ–‡å¸¸è§å¥å¼å¼€å¤´
#     classic_patterns = [
#         r'^[\u4e00-\u9fff]{1,5}æ›°',  # "æŸæŸæ›°" æ ¼å¼
#         r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^æ—¢è€Œ'
#     ]
    
#     # è®¡ç®—å¤æ–‡ç‰¹å¾è¯å¯†åº¦
#     total_chars = len(text)
#     if total_chars == 0:
#         return False
    
#     classic_char_count = 0
#     for word in classic_words:
#         classic_char_count += text.count(word)
    
#     # ç‰¹å¾è¯å¯†åº¦é˜ˆå€¼
#     density_threshold = 0.03  # 3%çš„å­—ç¬¦æ˜¯å¤æ–‡ç‰¹å¾è¯
    
#     # æ£€æŸ¥å¤æ–‡å¥å¼
#     pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
#     # åˆ¤æ–­æ¡ä»¶ï¼šç‰¹å¾è¯å¯†åº¦é«˜æˆ–åŒ¹é…å¤æ–‡å¥å¼
#     is_classic = (classic_char_count / total_chars > density_threshold) or pattern_match
    
#     return is_classic

# # ========== 4. å›°æƒ‘åº¦åˆ†æï¼ˆåˆ†å±‚ç‰ˆæœ¬ï¼‰ ==========
# def load_perplexity_model():
#     logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦")
#     start_time = time.time()
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
#     if tokenizer.pad_token is None:
#         tokenizer.pad_token = tokenizer.eos_token
#     model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
#     model.eval()
#     logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
#     return tokenizer, model

# def calculate_perplexity_batch_optimized(texts, tokenizer, model):
#     """ä¼˜åŒ–ç‰ˆçš„æ‰¹é‡å›°æƒ‘åº¦è®¡ç®—ï¼Œæé«˜GPUåˆ©ç”¨ç‡"""
#     try:
#         # æ‰¹é‡ç¼–ç æ‰€æœ‰æ–‡æœ¬
#         inputs = tokenizer(
#             texts, 
#             padding=True, 
#             truncation=True, 
#             max_length=MAX_SEQ_LENGTH, 
#             return_tensors="pt"
#         ).to(DEVICE)
        
#         input_ids = inputs["input_ids"]
#         attention_mask = inputs["attention_mask"]
#         batch_size = input_ids.shape[0]
        
#         with torch.no_grad():
#             # å•æ¬¡å‰å‘ä¼ æ’­è®¡ç®—æ•´ä¸ªæ‰¹æ¬¡
#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)
#             logits = outputs.logits
            
#             perplexities = []
#             for i in range(batch_size):
#                 # æå–æœ‰æ•ˆtokenï¼ˆæ’é™¤paddingï¼‰
#                 valid_mask = (input_ids[i] != tokenizer.pad_token_id)
#                 valid_token_ids = input_ids[i][valid_mask]
                
#                 if len(valid_token_ids) <= 1:  # éœ€è¦è‡³å°‘2ä¸ªtokenè®¡ç®—å›°æƒ‘åº¦
#                     perplexities.append(float('inf'))
#                     continue
                
#                 # è®¡ç®—æŸå¤±
#                 shift_logits = logits[i, :-1, :].contiguous()
#                 shift_labels = valid_token_ids[1:].contiguous()
                
#                 loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
#                 loss = loss_fct(
#                     shift_logits.view(-1, shift_logits.size(-1)), 
#                     shift_labels.view(-1)
#                 )
                
#                 perplexity = torch.exp(loss).item()
#                 perplexities.append(perplexity)
            
#             return perplexities
            
#     except RuntimeError as e:
#         if "out of memory" in str(e):
#             logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
#             # é€’å½’å‡åŠæ‰¹é‡å¤§å°
#             half_size = len(texts) // 2
#             if half_size >= 1:
#                 perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
#                 perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
#                 return perplexities1 + perplexities2
#             else:
#                 return [calculate_perplexity(text, tokenizer, model) for text in texts]
#         else:
#             raise e
#     except Exception as e:
#         logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
#         return [calculate_perplexity(text, tokenizer, model) for text in texts]

# def calculate_perplexity(text, tokenizer, model):
#     """å•æ–‡æœ¬å›°æƒ‘åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
#     try:
#         inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
#         with torch.no_grad():
#             outputs = model(**inputs)
#             logits = outputs.logits
#             shift_logits = logits[..., :-1, :].contiguous()
#             shift_labels = inputs["input_ids"][..., 1:].contiguous()
#             loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
#             avg_loss = loss.mean()
#             return torch.exp(avg_loss).item()
#     except:
#         return float('inf')

# def analyze_perplexity_distribution_layered(input_file, sample_size=1000):
#     """åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æ"""
#     logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æ")
#     tokenizer, model = load_perplexity_model()
    
#     # è¯»å–æ•°æ®
#     texts = []
#     with open(input_file, "r", encoding="utf-8") as f:
#         lines = f.readlines()
#         if len(lines) > sample_size:
#             import random
#             lines = random.sample(lines, sample_size)
#         for line in lines:
#             try:
#                 data = json.loads(line)
#                 text = data.get("text", "").strip()
#                 if text and len(text) >= MIN_CHAR_LEN:
#                     texts.append(text)
#             except:
#                 continue
    
#     if not texts:
#         logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
#         return np.array([50.0] * 9), np.array([50.0] * 9)
    
#     logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
#     # è®¡ç®—å›°æƒ‘åº¦
#     perplexities = []
#     classic_texts = []  # å¤æ–‡æ–‡æœ¬
#     modern_texts = []   # ç°ä»£æ–‡æ–‡æœ¬
    
#     # å…ˆåˆ†ç±»å†æ‰¹é‡è®¡ç®—
#     for i in tqdm(range(0, len(texts), BATCH_SIZE_PERPLEXITY), desc="æ–‡æœ¬åˆ†ç±»"):
#         batch = texts[i:i+BATCH_SIZE_PERPLEXITY]
#         for text in batch:
#             if is_classic_chinese(text):
#                 classic_texts.append(text)
#             else:
#                 modern_texts.append(text)
    
#     logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
#     # æ‰¹é‡è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦
#     modern_perplexities = []
#     for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
#         batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
#         batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
#         modern_perplexities.extend(batch_perplexities)
    
#     # æ‰¹é‡è®¡ç®—å¤æ–‡å›°æƒ‘åº¦
#     classic_perplexities = []
#     for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
#         batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
#         batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
#         classic_perplexities.extend(batch_perplexities)
    
#     # å¤„ç†ç»“æœ
#     modern_perplexities = np.array(modern_perplexities)
#     classic_perplexities = np.array(classic_perplexities)
    
#     modern_valid = modern_perplexities[modern_perplexities < 10000]
#     classic_valid = classic_perplexities[classic_perplexities < 10000]
    
#     if len(modern_valid) == 0:
#         logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#         modern_valid = np.array([50.0])
    
#     if len(classic_valid) == 0:
#         logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#         classic_valid = np.array([50.0])
    
#     # è®¡ç®—ç™¾åˆ†ä½æ•°
#     percentiles = [0, 10, 25, 50, 75, 90, 95, 99, 100]
#     modern_percentiles = np.percentile(modern_valid, percentiles)
#     classic_percentiles = np.percentile(classic_valid, percentiles)
    
#     logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒ:")
#     for p, val in zip(percentiles, modern_percentiles):
#         logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    
#     logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒ:")
#     for p, val in zip(percentiles, classic_percentiles):
#         logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    
#     # ç»˜åˆ¶åˆ†å±‚åˆ†å¸ƒå›¾
#     try:
#         import matplotlib.pyplot as plt
        
#         plt.figure(figsize=(12, 8))
        
#         # ç»˜åˆ¶åŒç›´æ–¹å›¾
#         plt.hist(modern_valid, bins=50, alpha=0.7, label='Modern Chinese', color='skyblue', edgecolor='black')
#         plt.hist(classic_valid, bins=50, alpha=0.7, label='Classic Chinese', color='salmon', edgecolor='black')
        
#         # æ·»åŠ é˜ˆå€¼çº¿
#         plt.axvline(CLASSIC_CHINESE_THRESHOLD, color='red', linestyle='--', 
#                    linewidth=2, label=f'Classic Threshold: {CLASSIC_CHINESE_THRESHOLD}')
        
#         if len(modern_valid) > 0:
#             modern_threshold = modern_percentiles[5]  # 90%åˆ†ä½æ•°
#             plt.axvline(modern_threshold, color='blue', linestyle='--', 
#                        linewidth=2, label=f'Modern Threshold: {modern_threshold:.2f}')
        
#         plt.xlabel('Perplexity', fontsize=12)
#         plt.ylabel('Frequency', fontsize=12)
#         plt.title('Layered Perplexity Distribution', fontsize=14, fontweight='bold')
#         plt.legend(fontsize=10)
#         plt.grid(True, alpha=0.3)
#         plt.tight_layout()
        
#         output_path = os.path.join(OUTPUT_PATH, 'layered_perplexity_distribution.png')
#         plt.savefig(output_path, dpi=300, bbox_inches='tight')
#         plt.close()
        
#         logging.info(f"ğŸ“Š åˆ†å±‚åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
        
#     except Exception as e:
#         logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
    
#     return modern_percentiles, classic_percentiles

# def determine_layered_thresholds(input_file):
#     """ç¡®å®šåˆ†å±‚é˜ˆå€¼"""
#     logging.info("ğŸ¯ å¼€å§‹ç¡®å®šåˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼")
    
#     try:
#         modern_percentiles, classic_percentiles = analyze_perplexity_distribution_layered(input_file, sample_size=500)
        
#         # ç°ä»£æ–‡ä½¿ç”¨90%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
#         modern_threshold = modern_percentiles[5]  # 90%åˆ†ä½æ•°
#         logging.info(f"ğŸ¤– ç°ä»£æ–‡æ¨èé˜ˆå€¼: {modern_threshold:.2f} (90%åˆ†ä½æ•°)")
        
#         # å¤æ–‡ä½¿ç”¨å›ºå®šé˜ˆå€¼40.55ï¼Œä½†ä¹Ÿä¼šæ˜¾ç¤ºåˆ†å¸ƒæƒ…å†µ
#         classic_threshold = CLASSIC_CHINESE_THRESHOLD
#         logging.info(f"ğŸ“œ å¤æ–‡å›ºå®šé˜ˆå€¼: {classic_threshold}")
        
#         # æä¾›ç»Ÿè®¡ä¿¡æ¯
#         if len(classic_percentiles) > 5:
#             classic_p90 = classic_percentiles[5]
#             logging.info(f"ğŸ“Š å¤æ–‡90%åˆ†ä½æ•°: {classic_p90:.2f}")
#             if classic_p90 > classic_threshold:
#                 logging.info("ğŸ’¡ å¤æ–‡é˜ˆå€¼è®¾ç½®åˆç†ï¼Œå°†ä¿ç•™å¤§éƒ¨åˆ†å¤æ–‡")
#             else:
#                 logging.info("âš ï¸ å¤æ–‡é˜ˆå€¼å¯èƒ½è¿‡é«˜ï¼Œéƒ¨åˆ†é«˜è´¨é‡å¤æ–‡å¯èƒ½è¢«è¿‡æ»¤")
        
#         return modern_threshold, classic_threshold
        
#     except Exception as e:
#         logging.error(f"âŒ ç¡®å®šåˆ†å±‚é˜ˆå€¼å¤±è´¥: {str(e)}")
#         logging.info("ğŸ”„ ä½¿ç”¨é»˜è®¤é˜ˆå€¼: ç°ä»£æ–‡=50, å¤æ–‡=40.55")
#         return 50, CLASSIC_CHINESE_THRESHOLD

# # ========== 5. åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ ==========
# def layered_perplexity_filter(input_file, modern_threshold, classic_threshold):
#     """åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰"""
#     start_time = time.time()
#     logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰")
#     logging.info(f"   - ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f}")
#     logging.info(f"   - å¤æ–‡é˜ˆå€¼: >{classic_threshold} (ä¿ç•™é«˜å›°æƒ‘åº¦å¤æ–‡)")
    
#     tokenizer, model = load_perplexity_model()
    
#     kept_file = os.path.join(OUTPUT_PATH, "kept_data_layered.jsonl")
#     filtered_file = os.path.join(OUTPUT_PATH, "filtered_data_layered.jsonl")
#     classic_file = os.path.join(OUTPUT_PATH, "classic_chinese_data.jsonl")  # ä¸“é—¨ä¿å­˜å¤æ–‡
    
#     batch_texts = []
#     batch_data = []
    
#     with open(input_file, "r", encoding="utf-8") as f_in, \
#          open(kept_file, "w", encoding="utf-8") as f_kept, \
#          open(filtered_file, "w", encoding="utf-8") as f_filtered, \
#          open(classic_file, "w", encoding="utf-8") as f_classic:
        
#         for line in tqdm(f_in, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰"):
#             try:
#                 data = json.loads(line)
#                 batch_texts.append(data["text"])
#                 batch_data.append(data)
#             except:
#                 continue
            
#             if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
#                 # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
#                 perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
#                 for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
#                     data_item["perplexity"] = round(perp, 2)
#                     data_item["is_classic"] = is_classic_chinese(text)
                    
#                     # åˆ†å±‚ç­›é€‰é€»è¾‘
#                     if data_item["is_classic"]:
#                         # å¤æ–‡ï¼šå›°æƒ‘åº¦ > classic_threshold æ—¶ä¿ç•™
#                         if perp > classic_threshold:
#                             final_data = data_item["original_data"].copy()
#                             final_data["cleaned_text"] = text
#                             final_data["md5"] = data_item["md5"]
#                             final_data["perplexity"] = data_item["perplexity"]
#                             final_data["source_file"] = data_item["source_file"]
#                             final_data["text_type"] = "classic_chinese"
#                             f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             stats["final_kept"] += 1
#                             stats["classic_chinese_kept"] += 1
#                         else:
#                             # å¤æ–‡ä½†å›°æƒ‘åº¦ä½ï¼Œå¯èƒ½æ˜¯è´¨é‡ä¸é«˜çš„å¤æ–‡
#                             filtered_data = data_item.copy()
#                             filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦è¿‡ä½({perp:.2f}â‰¤{classic_threshold})"
#                             f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                             stats["perplexity_filtered"] += 1
#                     else:
#                         # ç°ä»£æ–‡ï¼šå›°æƒ‘åº¦ â‰¤ modern_threshold æ—¶ä¿ç•™
#                         if perp <= modern_threshold:
#                             final_data = data_item["original_data"].copy()
#                             final_data["cleaned_text"] = text
#                             final_data["md5"] = data_item["md5"]
#                             final_data["perplexity"] = data_item["perplexity"]
#                             final_data["source_file"] = data_item["source_file"]
#                             final_data["text_type"] = "modern_chinese"
#                             f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                             stats["final_kept"] += 1
#                             stats["modern_chinese_kept"] += 1
#                         else:
#                             filtered_data = data_item.copy()
#                             filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦è¿‡é«˜({perp:.2f}>{modern_threshold})"
#                             f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                             stats["perplexity_filtered"] += 1
                
#                 batch_texts = []
#                 batch_data = []
        
#         # å¤„ç†å‰©ä½™æ•°æ®
#         if batch_texts:
#             perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
#             for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
#                 data_item["perplexity"] = round(perp, 2)
#                 data_item["is_classic"] = is_classic_chinese(text)
                
#                 if data_item["is_classic"]:
#                     if perp > classic_threshold:
#                         final_data = data_item["original_data"].copy()
#                         final_data["cleaned_text"] = text
#                         final_data["md5"] = data_item["md5"]
#                         final_data["perplexity"] = data_item["perplexity"]
#                         final_data["source_file"] = data_item["source_file"]
#                         final_data["text_type"] = "classic_chinese"
#                         f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         stats["final_kept"] += 1
#                         stats["classic_chinese_kept"] += 1
#                     else:
#                         filtered_data = data_item.copy()
#                         filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦è¿‡ä½({perp:.2f}â‰¤{classic_threshold})"
#                         f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                         stats["perplexity_filtered"] += 1
#                 else:
#                     if perp <= modern_threshold:
#                         final_data = data_item["original_data"].copy()
#                         final_data["cleaned_text"] = text
#                         final_data["md5"] = data_item["md5"]
#                         final_data["perplexity"] = data_item["perplexity"]
#                         final_data["source_file"] = data_item["source_file"]
#                         final_data["text_type"] = "modern_chinese"
#                         f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
#                         stats["final_kept"] += 1
#                         stats["modern_chinese_kept"] += 1
#                     else:
#                         filtered_data = data_item.copy()
#                         filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦è¿‡é«˜({perp:.2f}>{modern_threshold})"
#                         f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
#                         stats["perplexity_filtered"] += 1
    
#     stats["stage_time"]["perplexity"] = round(time.time() - start_time, 2)
#     logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’")
#     logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
#     logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
#     return kept_file, filtered_file, classic_file

# # ========== ä¸»å‡½æ•° ==========
# def main():
#     global monitor_running
#     start_time = time.time()
    
#     logging.basicConfig(
#         level=logging.INFO,
#         format="%(asctime)s - %(levelname)s - %(message)s",
#         handlers=[
#             logging.FileHandler(os.path.join(OUTPUT_PATH, "filter_log_layered.log"), encoding="utf-8"),
#             logging.StreamHandler()
#         ]
#     )
    
#     logging.info("ğŸ‰ å¯åŠ¨åˆ†å±‚æ•°æ®ç­›é€‰æµç¨‹")
#     logging.info(f"ğŸ“‹ åˆ†å±‚é…ç½®:")
#     logging.info(f"   - ç°ä»£æ–‡: è‡ªåŠ¨ç¡®å®šé˜ˆå€¼ï¼Œä¿ç•™ä½å›°æƒ‘åº¦æ–‡æœ¬")
#     logging.info(f"   - å¤æ–‡: å›ºå®šé˜ˆå€¼ {CLASSIC_CHINESE_THRESHOLD}ï¼Œä¿ç•™é«˜å›°æƒ‘åº¦æ–‡æœ¬")
    
#     # å¯åŠ¨ç›‘æ§çº¿ç¨‹
#     monitor = threading.Thread(target=monitor_thread, daemon=True)
#     monitor.start()
    
#     try:
#         # 1. é¢„å¤„ç†+MD5å»é‡
#         preprocessed_file = preprocess_and_md5_deduplicate()
        
#         # 2. Minhash LSHå»é‡
#         minhash_file = minhash_lsh_deduplicate(preprocessed_file)
        
#         # 3. ç¡®å®šåˆ†å±‚é˜ˆå€¼
#         modern_threshold, classic_threshold = determine_layered_thresholds(minhash_file)
        
#         # 4. åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰
#         kept_file, filtered_file, classic_file = layered_perplexity_filter(minhash_file, modern_threshold, classic_threshold)
        
#         # åœæ­¢ç›‘æ§
#         monitor_running = False
#         monitor.join()
        
#         # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
#         total_time = round(time.time() - start_time, 2)
#         logging.info("\n" + "="*60)
#         logging.info("ğŸ“Š åˆ†å±‚ç­›é€‰æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
#         logging.info("="*60)
#         logging.info(f"æ€»è¾“å…¥æ•°æ®: {stats['total_input']}æ¡")
#         logging.info(f"æŠ½æ ·åæ•°æ®: {stats['sampled_count']}æ¡")
#         logging.info(f"ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']}æ¡")
#         logging.info(f"å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}æ¡")
#         logging.info(f"æœ€ç»ˆæ€»è®¡ä¿ç•™: {stats['final_kept']}æ¡")
#         logging.info(f"ä¿ç•™æ¯”ä¾‹: {stats['final_kept']/stats['sampled_count']*100:.1f}%")
#         logging.info(f"ç°ä»£æ–‡é˜ˆå€¼: {modern_threshold:.2f}")
#         logging.info(f"å¤æ–‡é˜ˆå€¼: >{classic_threshold}")
#         logging.info(f"æ€»è€—æ—¶: {total_time}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
#         logging.info("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
#         logging.info(f"âœ… å…¨éƒ¨ä¿ç•™æ•°æ®: {kept_file}")
#         logging.info(f"ğŸ“œ å¤æ–‡ä¸“é—¨æ–‡ä»¶: {classic_file}")
#         logging.info(f"âŒ è¿‡æ»¤æ•°æ®: {filtered_file}")
#         logging.info("="*60)
        
#     except Exception as e:
#         logging.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
#     finally:
#         monitor_running = False
#         monitor.join(timeout=5)
#         logging.info("ğŸ”š åˆ†å±‚ç­›é€‰æµç¨‹ç»“æŸ")

# if __name__ == "__main__":
#     main()

import os
import json
import torch
import logging
import mmap
import time
import hashlib
import psutil
import threading
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from datasketch import MinHash, MinHashLSH

# å¿½ç•¥æ— å…³è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========== æ ¸å¿ƒé…ç½®ï¼ˆæ»¡è¶³éœ€æ±‚+50%ä¿ç•™ç‡ï¼‰ ==========
INPUT_DIR = "data"
INTERMEDIATE_PATH = "data/intermediate"
OUTPUT_PATH = "data/output"
os.makedirs(INTERMEDIATE_PATH, exist_ok=True)
os.makedirs(OUTPUT_PATH, exist_ok=True)

# æŠ½æ ·é…ç½®
SAMPLING_ENABLE = True
SAMPLE_RATIO = 0.01
MAX_SAMPLE_COUNT = 10000

# æ‰¹é‡é…ç½®
BATCH_SIZE_PREPROCESS = 1024
BATCH_SIZE_PERPLEXITY = 32
BATCH_SIZE_MINHASH = 5000

# æ•°æ®è¿‡æ»¤åŸºç¡€é…ç½®
MIN_CHAR_LEN = 8
MAX_CHAR_LEN = 12000
MAX_SEQ_LENGTH = 512

# å»é‡é…ç½®ï¼ˆé€‚åº¦æ”¶ç´§ï¼Œå¼¥è¡¥åˆ é™¤çš„ç­›é€‰ç¯èŠ‚ï¼‰
MINHASH_NUM_PERM = 128
LSH_THRESHOLD = 0.76  # ä»0.75â†’0.76ï¼Œå‡å°‘å†—ä½™

# æ¨¡å‹é…ç½®
MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ========== æ ¸å¿ƒç­›é€‰é…ç½®ï¼ˆæ»¡è¶³éœ€æ±‚è°ƒæ•´ï¼‰ ==========
# 1. å£è¯­å…³é”®è¯æ‰©å……ï¼ˆæ–°å¢40+ï¼Œå¼ºåŒ–è¿‡æ»¤ï¼‰
COLLOQUIAL_WORDS = [
    # åŸæœ‰å…³é”®è¯
    "å§æ§½", "ç‰›é€¼", "å“ˆå“ˆå“ˆ", "å˜»å˜»", "å˜¿å˜¿", "è€é“", "æ‡‚å§", "ç»™åŠ›", "666", "yyds",
    "ç»ç»å­", "å®¶äººä»¬", "è°æ‡‚å•Š", "æ•‘å‘½", "å“­æ­»", "ç¬‘ä¸æ´»", "æ “Q", "æ‹¿æ", "ç ´é˜²", "èººå¹³",
    # æ–°å¢å£è¯­/ç½‘ç»œæµè¡Œè¯­
    "ç»äº†", "æ— è¯­", "æœäº†", "å‡‘æ´»", "å’‹æ•´", "å” å—‘", "ä¾ƒå¤§å±±", "æ‰¯çŠŠå­", "çé€¼é€¼", "é€¼é€¼èµ–èµ–",
    "ç£¨ç£¨å”§å”§", "å½å½æ­ªæ­ª", "ç¢ç¢å¿µ", "åæ§½", "æ€¼äºº", "æ ç²¾", "å†…å·", "èººå¹³", "æ‘†çƒ‚", "æ‘¸é±¼",
    "åˆ’æ°´", "æ‰“å·¥äºº", "å¹²é¥­äºº", "å°¾æ¬¾äºº", "å·¥å…·äºº", "å†¤ç§", "æ˜¾çœ¼åŒ…", "ç¤¾æ", "ç¤¾ç‰›", "ç¤¾æ­»",
    "emo", "ä½›ç³»", "å·ç‹", "æ‘†çƒ‚å¼", "æ‘¸é±¼å¼", "åˆ’æ°´å¼", "èººå¹³å¼", "æ•·è¡å¼", "ç³Šå¼„å­¦", "PUA",
    "CPU", "KTV", "yyds", "awsl", "ç»ç»å­", "YYDS", "ç»ç»å­", "æ “Q", "æ‹¿æäº†", "ç ´é˜²äº†"
]

# 2. æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼ˆæ–°å¢ï¼è¦†ç›–è‰²æƒ…ã€æš´åŠ›ã€æ¯’å“ã€æ”¿æ²»æ•æ„Ÿç­‰ï¼‰
SENSITIVE_KEYWORDS = {
    "è‰²æƒ…ç›¸å…³": [
        "è‰²æƒ…", "é»„è‰²", "è£¸èŠ", "æ€§äº¤æ˜“", "å«–å¨¼", "å–æ·«", "æ·«è¡", "è‰²æƒ…è§†é¢‘", "è‰²æƒ…å›¾ç‰‡", "AV",
        "ä¸‰çº§ç‰‡", "æ˜¥å®«", "è‰³ç…§", "éœ²éª¨", "æ€§è¡Œä¸º", "æ€§å™¨å®˜", "æ‰‹æ·«", "å«–å¨¼", "åŒ…å…»", "å°ä¸‰",
        "äºŒå¥¶", "æƒ…å¤«", "æƒ…å¦‡", "ä¸æ­£å½“å…³ç³»", "ä¸€å¤œæƒ…", "çº¦ç‚®", "æ€§æœåŠ¡", "è‰²æƒ…ç›´æ’­", "è‰²æƒ…å°è¯´"
    ],
    "æš´åŠ›ç›¸å…³": [
        "æ€äºº", "æŠ¢åŠ«", "å¼ºå¥¸", "ç»‘æ¶", "æ–—æ®´", "æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ€äººæ”¾ç«", "çˆ†ç‚¸", "æŠ•æ¯’",
        "å‡¶å™¨", "æªæ”¯", "å¼¹è¯", "ç®¡åˆ¶åˆ€å…·", "æš´åŠ›", "è¡€è…¥", "ææ€–", "è™æ€", "è™å¾…", "æ–½æš´",
        "æ®´æ‰“", "ç¾¤æ®´", "äº’æ®´", "å¯»è¡…æ»‹äº‹", "èšä¼—æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ•…æ„æ€äºº", "æŠ¢åŠ«è´¢ç‰©"
    ],
    "æ¯’å“ç›¸å…³": [
        "æ¯’å“", "å¤§éº»", "æµ·æ´›å› ", "å†°æ¯’", "å¯å¡å› ", "æ‘‡å¤´ä¸¸", "Kç²‰", "é¸¦ç‰‡", "å—å•¡", "æœå†·ä¸",
        "å¸æ¯’", "è´©æ¯’", "åˆ¶æ¯’", "å¸æ¯’è€…", "æ¯’è´©", "æ¯’å“äº¤æ˜“", "æ¯’å“è¿è¾“", "æ¯’å“èµ°ç§"
    ],
    "æ”¿æ²»æ•æ„Ÿ": [
        "æ•æ„Ÿæ”¿æ²»äººç‰©", "æ”¿æ²»æ•æ„Ÿäº‹ä»¶", "é¢ è¦†", "åˆ†è£‚", "å›å›½", "æš´åŠ¨", "éªšä¹±", "éæ³•é›†ä¼š",
        "ååŠ¨", "åæ”¿åºœ", "åç¤¾ä¼š", "æç«¯ä¸»ä¹‰", "ææ€–ä¸»ä¹‰", "é‚ªæ•™", "æ³•è½®åŠŸ", "å°ç‹¬", "æ¸¯ç‹¬", "ç–†ç‹¬"
    ],
    "å…¶ä»–æ•æ„Ÿ": [
        "èµŒåš", "è¯ˆéª—", "ä¼ é”€", "éæ³•é›†èµ„", "æ´—é’±", "å·ç¨æ¼ç¨", "è´ªæ±¡è…è´¥", "è¡Œè´¿å—è´¿",
        "å‡å¸", "éæ³•äº¤æ˜“", "é»‘å®¢", "å…¥ä¾µ", "ç—…æ¯’", "ç›—å·", "è¯ˆéª—çŸ­ä¿¡", "è¯ˆéª—ç”µè¯"
    ]
}

# 3. å­¦æœ¯ç‰¹å¾ï¼ˆä¿ç•™ï¼Œæå‡æ•°æ®è´¨é‡ï¼‰
ACADEMIC_PATTERNS = [
    r"[A-Za-z0-9]=.*[A-Za-z0-9]",  # å…¬å¼
    r"å®šä¹‰[:ï¼š]", r"å®šç†", r"å…¬ç†", r"å‘½é¢˜", r"æ¨è®º", r"åŸç†", r"æ–¹æ³•", r"å®éªŒ", r"åˆ†æ", r"ç»“è®º",
]
ACADEMIC_REQUIRE = False

# å…¨å±€ç»Ÿè®¡å˜é‡ï¼ˆåˆ é™¤å­¦ç§‘è¿‡æ»¤ç›¸å…³ç»Ÿè®¡é¡¹ï¼‰
stats = {
    "total_input": 0,
    "sampled_count": 0,
    "preprocess_filtered": 0,
    "colloquial_filtered": 0,
    "non_academic_filtered": 0,
    "md5_duplicated": 0,
    "minhash_duplicated": 0,
    "sensitive_filtered": 0,  # æ–°å¢æ•æ„Ÿè¯é¢˜ç»Ÿè®¡
    "perplexity_filtered": 0,
    "final_kept": 0,
    "classic_chinese_kept": 0,
    "modern_chinese_kept": 0,
    "stage_time": {}
}

# ç›‘æ§çº¿ç¨‹å˜é‡
monitor_running = True
gpu_util = 0
cpu_mem = 0

# ========== å·¥å…·å‡½æ•° ==========
def get_gpu_utilization():
    try:
        result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
        return int(result.strip().split("\n")[0]) if result else 0
    except:
        return 0

def get_cpu_memory():
    process = psutil.Process(os.getpid())
    return round(process.memory_info().rss / (1024 * 1024), 2)

def monitor_thread():
    global monitor_running, gpu_util, cpu_mem
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
    while monitor_running:
        gpu_util = get_gpu_utilization()
        cpu_mem = get_cpu_memory()
        total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - \
                          stats["colloquial_filtered"] - stats["non_academic_filtered"] - \
                          stats["md5_duplicated"] - stats["minhash_duplicated"] - stats["sensitive_filtered"]
        progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
        logging.info(
            f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
            f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
        )
        time.sleep(30)
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

def load_jsonl_files_with_sampling(input_dir):
    jsonl_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".jsonl")]
    if not jsonl_files:
        raise ValueError(f"âŒ è¾“å…¥ç›®å½• {input_dir} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
    logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
    sampled_count = 0
    for file_idx, file in enumerate(jsonl_files):
        file_name = os.path.basename(file)
        logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
        with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            for line_bytes in iter(mm.readline, b""):
                line = line_bytes.decode("utf-8").strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    text = data.get("text", "").strip()
                    stats["total_input"] += 1
                    
                    if SAMPLING_ENABLE:
                        if sampled_count >= MAX_SAMPLE_COUNT:
                            logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
                            return
                        if np.random.random() > SAMPLE_RATIO:
                            continue
                        sampled_count += 1
                        stats["sampled_count"] = sampled_count
                    
                    yield {"text": text, "original_data": data, "source_file": file_name}
                except:
                    stats["preprocess_filtered"] += 1
                    continue
    logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

# ========== æ ¸å¿ƒç­›é€‰å·¥å…·å‡½æ•°ï¼ˆæŒ‰éœ€æ±‚è°ƒæ•´ï¼‰ ==========
def is_colloquial(text):
    """æ‰©å……å£è¯­æ£€æµ‹ï¼Œè¿‡æ»¤æ›´å¤šæ— å­¦æœ¯ä»·å€¼æ–‡æœ¬"""
    # å…³é”®è¯åŒ¹é…
    for word in COLLOQUIAL_WORDS:
        if word in text:
            return True
    # è¿ç»­æ ‡ç‚¹åŒ¹é…ï¼ˆ3ä¸ªä»¥ä¸Šï¼‰
    if re.search(r"[ï¼ï¼Ÿã€‚,ï¼Œï¼›;ï¼š:]{3,}", text):
        return True
    # å£è¯­åŒ–å¥å¼åŒ¹é…
    colloquial_patterns = [
        r"[æˆ‘ä½ ä»–å¥¹å®ƒ]ï¼ˆä»¬ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"[è¿™é‚£å“ª]ï¼ˆä¸ªäº›ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"^[å“ˆå“ˆå˜¿å˜¿å˜»å˜»å‘µå‘µ]+"
    ]
    if any(re.search(pattern, text) for pattern in colloquial_patterns):
        return True
    return False

def is_sensitive(text):
    """æ–°å¢æ•æ„Ÿè¯é¢˜æ£€æµ‹ï¼Œè¿‡æ»¤è¿è§„æ•°æ®"""
    # æ•æ„Ÿå…³é”®è¯åŒ¹é…
    for category, words in SENSITIVE_KEYWORDS.items():
        for word in words:
            if word in text:
                return True
    # æ•æ„Ÿå¥å¼åŒ¹é…
    sensitive_patterns = [
        r"å‡ºå”®.*(è‰²æƒ…|AV|ä¸‰çº§ç‰‡)",
        r"(å«–å¨¼|å–æ·«|æ€§äº¤æ˜“).*(ä»·æ ¼|è”ç³»æ–¹å¼|åœ°ç‚¹)",
        r"(æ€äºº|æŠ¢åŠ«|ç»‘æ¶).*(æ–¹æ³•|æ•™ç¨‹|å·¥å…·)",
        r"(æ¯’å“|å¤§éº»|å†°æ¯’).*(è´­ä¹°|å‡ºå”®|è¿è¾“)",
        r"(å°ç‹¬|æ¸¯ç‹¬|ç–†ç‹¬).*(æ”¯æŒ|å®£ä¼ |åˆ†è£‚)"
    ]
    if any(re.search(pattern, text, re.IGNORECASE) for pattern in sensitive_patterns):
        return True
    return False

def has_academic_features(text):
    """æ£€æµ‹å­¦æœ¯ç‰¹å¾ï¼ˆCLMMU/CEVALåå¥½ï¼‰"""
    return any(re.search(pattern, text) for pattern in ACADEMIC_PATTERNS)

# ========== 1. é¢„å¤„ç† + MD5å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆæ–°å¢æ•æ„Ÿè¿‡æ»¤ï¼‰ ==========
def preprocess_and_md5_deduplicate():
    start_time = time.time()
    logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆå«æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼‰")
    
    md5_set = set()
    output_file = os.path.join(INTERMEDIATE_PATH, "preprocessed_md5_dedup_with_sensitive_filter.jsonl")
    batch_buffer = []
    
    with open(output_file, "w", encoding="utf-8") as f_out:
        for item in tqdm(load_jsonl_files_with_sampling(INPUT_DIR), desc="é¢„å¤„ç†+ç­›é€‰", total=stats["sampled_count"] if SAMPLING_ENABLE else None):
            text = item["text"]
            original_data = item["original_data"]
            source_file = item["source_file"]
            
            # 1. åŸºç¡€é•¿åº¦è¿‡æ»¤
            if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
                stats["preprocess_filtered"] += 1
                continue
            
            # 2. æ–‡æœ¬æ¸…æ´—
            text = re.sub(r"[\u200b\s]+", " ", text).strip()
            if not text:
                stats["preprocess_filtered"] += 1
                continue
            
            # 3. æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼ˆæ–°å¢ï¼ä¼˜å…ˆè¿‡æ»¤è¿è§„æ•°æ®ï¼‰
            if is_sensitive(text):
                stats["sensitive_filtered"] += 1
                continue
            
            # 4. å£è¯­åŒ–ç­›é€‰ï¼ˆæ‰©å……å…³é”®è¯ï¼‰
            if is_colloquial(text):
                stats["colloquial_filtered"] += 1
                continue
            
            # 5. å­¦æœ¯ç‰¹å¾ç­›é€‰ï¼ˆå¯é€‰ï¼‰
            if ACADEMIC_REQUIRE and not has_academic_features(text):
                stats["non_academic_filtered"] += 1
                continue
            
            # 6. MD5ç²¾ç¡®å»é‡
            md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
            if md5_hash in md5_set:
                stats["md5_duplicated"] += 1
                continue
            md5_set.add(md5_hash)
            
            # è®°å½•ç‰¹å¾
            batch_buffer.append({
                "text": text,
                "original_data": original_data,
                "source_file": source_file,
                "md5": md5_hash,
                "has_academic_features": has_academic_features(text)
            })
            
            # æ‰¹é‡å†™å…¥
            if len(batch_buffer) >= BATCH_SIZE_PREPROCESS:
                for data in batch_buffer:
                    f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
                batch_buffer = []
        
        if batch_buffer:
            for data in batch_buffer:
                f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    stats["stage_time"]["preprocess_md5"] = round(time.time() - start_time, 2)
    remaining = stats["sampled_count"] - stats["preprocess_filtered"] - \
                stats["colloquial_filtered"] - stats["non_academic_filtered"] - stats["md5_duplicated"] - stats["sensitive_filtered"]
    logging.info(
        f"âœ… é¢„å¤„ç†+ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['preprocess_md5']}ç§’ | "
        f"æŠ½æ ·åï¼š{stats['sampled_count']} | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']} | "
        f"æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼š{stats['sensitive_filtered']} | å£è¯­åŒ–ï¼š{stats['colloquial_filtered']} | "
        f"æ— å­¦æœ¯ç‰¹å¾ï¼š{stats['non_academic_filtered']} | å®Œå…¨é‡å¤ï¼ˆMD5ï¼‰ï¼š{stats['md5_duplicated']} | å‰©ä½™ï¼š{remaining}"
    )
    return output_file

# ========== 2. Minhash LSHè¯­ä¹‰å»é‡ ==========
def create_minhash_signature(text, num_perm=MINHASH_NUM_PERM):
    """2-gram Tokenæ•æ‰ä¸­æ–‡è¯­ä¹‰"""
    minhash = MinHash(num_perm=num_perm)
    if len(text) < 2:
        grams = [text] if text else []
    else:
        grams = [text[i:i+2] for i in range(len(text)-1)]
    for gram in grams:
        token_hash = hashlib.sha256(gram.encode('utf-8')).hexdigest()
        minhash.update(token_hash.encode('utf-8'))
    return minhash

def minhash_lsh_deduplicate(input_file):
    start_time = time.time()
    logging.info(f"ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡ï¼ˆé˜ˆå€¼{ LSH_THRESHOLD }ï¼‰")
    
    texts = []
    data_list = []
    with open(input_file, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="è¯»å–é¢„å¤„ç†æ•°æ®"):
            data = json.loads(line)
            texts.append(data["text"])
            data_list.append(data)
    
    if not texts:
        logging.warning("âš ï¸ MD5å»é‡åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
        return input_file
    
    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
    keep_indices = []
    duplicate_count = 0
    
    for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
        minhash = create_minhash_signature(texts[i])
        similar_docs = lsh.query(minhash)
        if not similar_docs:
            lsh.insert(str(i), minhash)
            keep_indices.append(i)
        else:
            duplicate_count += 1
    
    stats["minhash_duplicated"] = duplicate_count
    
    output_file = os.path.join(INTERMEDIATE_PATH, "minhash_dedup_with_sensitive_filter.jsonl")
    with open(output_file, "w", encoding="utf-8") as f_out:
        for idx in keep_indices:
            f_out.write(json.dumps(data_list[idx], ensure_ascii=False) + "\n")
    
    stats["stage_time"]["minhash_lsh"] = round(time.time() - start_time, 2)
    remaining = len(keep_indices)
    logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['minhash_lsh']}ç§’ | "
                 f"è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']} | å‰©ä½™ï¼š{remaining}")
    return output_file

# ========== 3. å¤æ–‡æ£€æµ‹å‡½æ•°ï¼ˆæ‰©å……å…³é”®è¯ï¼Œæå‡è¯†åˆ«å‡†ç¡®ç‡ï¼‰ ==========
def is_classic_chinese(text):
    """æ‰©å……å¤æ–‡å…³é”®è¯ï¼Œç²¾å‡†åˆ¤å®šçœŸå®å¤æ–‡ï¼ˆé¿å…è¯¯åˆ¤ï¼‰"""
    # æ‰©å……å¤æ–‡æ ¸å¿ƒç‰¹å¾è¯ï¼ˆæ–°å¢30+ï¼Œè¦†ç›–è™šè¯ã€å®è¯ã€å†å²äººç‰©ã€å…¸ç±ï¼‰
    classic_words = [
        # è™šè¯ï¼ˆæ ¸å¿ƒï¼‰
        'ä¹‹', 'ä¹', 'è€…', 'ä¹Ÿ', 'æ›°', 'å¾', 'æ±', 'å°”', 'ä¹ƒ', 'å…®', 'çŸ£', 'å“‰', 'è€¶', 'æ¬¤', 'ç„‰', 'ä¹', 'å…¶', 'è€Œ', 'ä»¥', 'äº',
        # # ä»£è¯/åè¯
        'å¤«', 'ç›–', 'åˆ™', 'ä¸”', 'è‹¥', 'ä½•', 'å­°', 'å®‰', 'å­°ä¸', 'æ‰€ä»¥', 'æ‰€', 'å¯', 'èƒ½', 'å¿…', 'å½“', 'åº”',
        # å†å²æœä»£/äººç‰©
        'å¤', 'å•†', 'å‘¨', 'ç§¦', 'æ±‰', 'é­', 'èœ€', 'å´', 'æ™‹', 'éš‹', 'å”', 'å®‹', 'å…ƒ', 'æ˜', 'æ¸…',
        'é»„å¸', 'ç‚å¸', 'å°§', 'èˆœ', 'ç¦¹', 'æ±¤', 'æ–‡ç‹', 'æ­¦ç‹', 'å‘¨å…¬', 'å­”å­', 'å­Ÿå­', 'è€å­', 'åº„å­', 'å¢¨å­', 'è€å­', 'éŸ©éå­',
        # ç»å…¸å…¸ç±
        'ã€Šè®ºè¯­ã€‹', 'ã€Šå­Ÿå­ã€‹', 'ã€Šå¤§å­¦ã€‹', 'ã€Šä¸­åº¸ã€‹', 'ã€Šè¯—ç»ã€‹', 'ã€Šå°šä¹¦ã€‹', 'ã€Šç¤¼è®°ã€‹', 'ã€Šå‘¨æ˜“ã€‹', 'ã€Šæ˜¥ç§‹ã€‹',
        'ã€Šé“å¾·ç»ã€‹', 'ã€Šåº„å­ã€‹', 'ã€Šå¢¨å­ã€‹', 'ã€Šè€å­ã€‹', 'ã€ŠéŸ©éå­ã€‹', 'ã€Šå²è®°ã€‹', 'ã€Šæ±‰ä¹¦ã€‹', 'ã€Šåæ±‰ä¹¦ã€‹', 'ã€Šä¸‰å›½å¿—ã€‹',
        # å¤æ–‡å¥å¼ç‰¹å¾è¯
        'å‘œå‘¼', 'å—Ÿå¤«', 'ç›–é—»', 'çªƒä»¥ä¸º', 'è‡£é—»', 'åœ£ç‹', 'è´¤å›', 'å¿ è‡£', 'ä¹‰å£«', 'å­å­', 'çƒˆå¥³'
    ]
    # ç°ä»£æ–‡å¼ºç‰¹å¾è¯ï¼ˆå¿«é€Ÿæ’é™¤ï¼‰
    modern_words = ["æ‰‹æœº", "äº’è”ç½‘", "ç”µè„‘", "å¾®ä¿¡", "æ”¯ä»˜å®", "å¿«é€’", "é«˜é“", "ç©ºè°ƒ", "ç”µè§†", "ç½‘ç»œ", "APP",
                   "å¾®åš", "æŠ–éŸ³", "å¿«æ‰‹", "ç›´æ’­", "ç”µå•†", "ç½‘è´­", "å¤–å–", "æ‰“è½¦", "å…±äº«å•è½¦", "5G", "WiFi"]
    
    # å«ç°ä»£è¯ç›´æ¥åˆ¤å®šä¸ºç°ä»£æ–‡
    for word in modern_words:
        if word in text:
            return False
    
    total_chars = len(text)
    if total_chars == 0:
        return False
    
    # å¤æ–‡ç‰¹å¾è¯å¯†åº¦é˜ˆå€¼ï¼ˆä¿æŒ2%ï¼Œç¡®ä¿ç²¾å‡†åº¦ï¼‰
    classic_char_count = 0
    for word in classic_words:
        classic_char_count += text.count(word)
    density_threshold = 0.02
    
    # æ‰©å……å¤æ–‡å¥å¼åŒ¹é…
    classic_patterns = [
        r'^[\u4e00-\u9fff]{1,5}æ›°', r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^å‘œå‘¼', r'^å—Ÿå¤«',
        r'^ç›–é—»', r'^çªƒä»¥ä¸º', r'^è‡£é—»', r'^åœ£ç‹', r'^è´¤å›', r'^å¿ è‡£', r'^ä¹‰å£«'
    ]
    pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
    # åˆ¤å®šé€»è¾‘ï¼šå¯†åº¦è¾¾æ ‡ æˆ– å¥å¼åŒ¹é…
    is_classic = (classic_char_count / total_chars > density_threshold) or pattern_match
    return is_classic

# ========== 4. å›°æƒ‘åº¦åˆ†æä¸ç­›é€‰ï¼ˆä¿æŒæ ¸å¿ƒé€»è¾‘ï¼‰ ==========
def load_perplexity_model():
    logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦ï¼ˆç”¨äºè¯„ä¼°æ–‡æœ¬è´¨é‡ï¼‰")
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
    model.eval()
    logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
    return tokenizer, model

def calculate_perplexity_batch_optimized(texts, tokenizer, model):
    """æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¼˜åŒ–ç»´åº¦åŒ¹é…ï¼‰"""
    try:
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=MAX_SEQ_LENGTH, 
            return_tensors="pt"
        ).to(DEVICE)
        
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        batch_size = input_ids.shape[0]
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            perplexities = []
            for i in range(batch_size):
                valid_mask = (input_ids[i] != tokenizer.pad_token_id)
                valid_token_ids = input_ids[i][valid_mask]
                
                if len(valid_token_ids) <= 1:
                    perplexities.append(float('inf'))
                    continue
                
                # ä¿®æ­£ç»´åº¦åŒ¹é…é€»è¾‘
                shift_logits = logits[i, :-1, :].contiguous()
                shift_labels = valid_token_ids[1:].contiguous()
                valid_len = len(shift_labels)
                shift_logits = shift_logits[:valid_len, :]
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)), 
                    shift_labels.view(-1)
                )
                
                perplexity = torch.exp(loss).item()
                perplexities.append(perplexity)
            
            return perplexities
            
    except RuntimeError as e:
        if "out of memory" in str(e):
            logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
            half_size = len(texts) // 2
            if half_size >= 1:
                perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
                perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
                return perplexities1 + perplexities2
            else:
                return [calculate_perplexity(text, tokenizer, model) for text in texts]
        else:
            raise e
    except Exception as e:
        logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
        return [calculate_perplexity(text, tokenizer, model) for text in texts]

def calculate_perplexity(text, tokenizer, model):
    """å•æ–‡æœ¬å›°æƒ‘åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["input_ids"][..., 1:].contiguous()
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            avg_loss = loss.mean()
            return torch.exp(avg_loss).item()
    except:
        return float('inf')

def analyze_perplexity_distribution_layered(input_file, sample_size=1000):
    """åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç²¾å‡†åˆ†ä½æ•°ï¼‰"""
    logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç°ä»£æ–‡=ä½å›°æƒ‘åº¦ä¼˜è´¨ï¼Œå¤æ–‡=é«˜å›°æƒ‘åº¦çœŸå®ï¼‰")
    tokenizer, model = load_perplexity_model()
    
    texts = []
    with open(input_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
        if len(lines) > sample_size:
            import random
            lines = random.sample(lines, sample_size)
        for line in lines:
            try:
                data = json.loads(line)
                text = data.get("text", "").strip()
                if text and len(text) >= MIN_CHAR_LEN:
                    texts.append(text)
            except:
                continue
    
    if not texts:
        logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
        return 100.0, 30.0, None, None
    
    logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
    # åˆ†ç±»å¤æ–‡/ç°ä»£æ–‡ï¼ˆæ‰©å……å…³é”®è¯åæ›´ç²¾å‡†ï¼‰
    classic_texts = []
    modern_texts = []
    for text in texts:
        if is_classic_chinese(text):
            classic_texts.append(text)
        else:
            modern_texts.append(text)
    
    logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
    # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
    modern_perplexities = []
    for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
        batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        modern_perplexities.extend(batch_perplexities)
    
    classic_perplexities = []
    for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
        batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        classic_perplexities.extend(batch_perplexities)
    
    # å¤„ç†æœ‰æ•ˆæ•°æ®ï¼ˆè¿‡æ»¤å¼‚å¸¸å€¼ï¼‰
    modern_perplexities = np.array(modern_perplexities)
    classic_perplexities = np.array(classic_perplexities)
    
    modern_valid = modern_perplexities[modern_perplexities < 15000]
    classic_valid = classic_perplexities[classic_perplexities < 15000]
    
    if len(modern_valid) == 0:
        logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        modern_valid = np.array([100.0])
    
    if len(classic_valid) == 0:
        logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        classic_valid = np.array([30.0])
    
    # å®šä¹‰åˆ†ä½æ•°ï¼ˆæ˜ç¡®ç´¢å¼•å¯¹åº”å…³ç³»ï¼‰
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_percentiles = np.percentile(modern_valid, percentiles)
    classic_percentiles = np.percentile(classic_valid, percentiles)
    
    # æ ¸å¿ƒé˜ˆå€¼é…ç½®ï¼ˆç¡®ä¿ä¿ç•™ç‡50%ï¼‰
    modern_threshold_idx = 6  # 75%åˆ†ä½æ•°ï¼ˆè¿‡æ»¤é«˜å›°æƒ‘åº¦ç°ä»£æ–‡ï¼‰
    classic_threshold_idx = 1  # 10%åˆ†ä½æ•°ï¼ˆä¿ç•™é«˜å›°æƒ‘åº¦å¤æ–‡ï¼‰
    
    modern_threshold = modern_percentiles[modern_threshold_idx]
    classic_threshold = classic_percentiles[classic_threshold_idx]
    
    # æ—¥å¿—è¾“å‡ºï¼ˆæ¸…æ™°æ˜“æ‡‚ï¼‰
    logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆä½å›°æƒ‘åº¦=è´¨é‡é«˜ã€æ¨¡å‹æ˜“ç†è§£ï¼‰:")
    for p, val in zip(percentiles, modern_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f} ({percentiles[modern_threshold_idx]}%åˆ†ä½æ•°)")
    
    logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆé«˜å›°æƒ‘åº¦=æ›´çœŸå®ã€éç°ä»£æ”¹å†™ï¼‰:")
    for p, val in zip(percentiles, classic_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ å¤æ–‡é˜ˆå€¼: â‰¥{classic_threshold:.2f} ({percentiles[classic_threshold_idx]}%åˆ†ä½æ•°)")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    try:
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(12, 8))
        plt.hist(modern_valid, bins=50, alpha=0.7, label='Modern Chinese (Low Perplexity = High Quality)', color='skyblue', edgecolor='black')
        plt.hist(classic_valid, bins=50, alpha=0.7, label='Classic Chinese (High Perplexity = Authentic)', color='salmon', edgecolor='black')
        plt.axvline(modern_threshold, color='blue', linestyle='--', linewidth=2, label=f'Modern â‰¤ {modern_threshold:.2f} ({percentiles[modern_threshold_idx]}%tile)')
        plt.axvline(classic_threshold, color='red', linestyle='--', linewidth=2, label=f'Classic â‰¥ {classic_threshold:.2f} ({percentiles[classic_threshold_idx]}%tile)')
        plt.xlabel('Perplexity', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.title('Layered Perplexity Distribution (CLMMU/CEVAL Metric Priority)', fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        output_path = os.path.join(OUTPUT_PATH, 'layered_perplexity_distribution_final.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"ğŸ“Š åˆ†å±‚åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
        
    except Exception as e:
        logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
    
    return modern_threshold, classic_threshold, modern_percentiles, classic_percentiles

def determine_layered_thresholds(input_file):
    """ç¡®å®šåˆ†å±‚é˜ˆå€¼"""
    logging.info("ğŸ¯ å¼€å§‹ç¡®å®šåˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼")
    
    try:
        modern_threshold, classic_threshold, modern_percentiles, classic_percentiles = analyze_perplexity_distribution_layered(input_file, sample_size=500)
        
        # æ˜ç¡®è¾“å‡ºé˜ˆå€¼å’Œå¯¹åº”åˆ†ä½æ•°
        percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
        modern_p = percentiles[6]  # 75%åˆ†ä½æ•°
        classic_p = percentiles[1]  # 10%åˆ†ä½æ•°
        
        logging.info(f"ğŸ¤– ç°ä»£æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)")
        logging.info(f"ğŸ“œ å¤æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)")
        
        return modern_threshold, classic_threshold
        
    except Exception as e:
        logging.error(f"âŒ ç¡®å®šåˆ†å±‚é˜ˆå€¼å¤±è´¥: {str(e)}")
        logging.info("ğŸ”„ ä½¿ç”¨é»˜è®¤é˜ˆå€¼: ç°ä»£æ–‡=80 (75%åˆ†ä½æ•°), å¤æ–‡=30 (10%åˆ†ä½æ•°)")
        return 80.0, 30.0

def layered_perplexity_filter(input_file, modern_threshold, classic_threshold):
    """åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰"""
    start_time = time.time()
    
    # æ˜ç¡®é˜ˆå€¼å’Œè§£é‡Š
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_p = percentiles[6]
    classic_p = percentiles[1]
    
    logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰")
    logging.info(f"   - ç°ä»£æ–‡ï¼šâ‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™ä½å›°æƒ‘åº¦ä¼˜è´¨æ•°æ®")
    logging.info(f"   - å¤æ–‡ï¼šâ‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™é«˜å›°æƒ‘åº¦çœŸå®å¤æ–‡")
    
    tokenizer, model = load_perplexity_model()
    
    kept_file = os.path.join(OUTPUT_PATH, "clmmu_kept_data_final.jsonl")
    filtered_file = os.path.join(OUTPUT_PATH, "clmmu_filtered_data_final.jsonl")
    classic_file = os.path.join(OUTPUT_PATH, "clmmu_classic_chinese_data_final.jsonl")
    
    batch_texts = []
    batch_data = []
    
    with open(input_file, "r", encoding="utf-8") as f_in, \
         open(kept_file, "w", encoding="utf-8") as f_kept, \
         open(filtered_file, "w", encoding="utf-8") as f_filtered, \
         open(classic_file, "w", encoding="utf-8") as f_classic:
        
        total_input = 0
        for line in f_in:
            total_input += 1
        f_in.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        for line in tqdm(f_in, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰", total=total_input):
            try:
                data = json.loads(line)
                batch_texts.append(data["text"])
                batch_data.append(data)
            except:
                continue
            
            if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
                perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
                for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                    data_item["perplexity"] = round(perp, 2)
                    data_item["is_classic"] = is_classic_chinese(text)
                    
                    # ç­›é€‰é€»è¾‘
                    if data_item["is_classic"]:
                        # å¤æ–‡ï¼šä¿ç•™â‰¥10%åˆ†ä½æ•°çš„é«˜å›°æƒ‘åº¦æ•°æ®ï¼Œæ’é™¤å¼‚å¸¸å€¼
                        if perp >= classic_threshold and perp < 10000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "classic_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["classic_chinese_kept"] += 1
                        else:
                            filtered_data = data_item.copy()
                            filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¥{classic_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
                            f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
                            stats["perplexity_filtered"] += 1
                    else:
                        # ç°ä»£æ–‡ï¼šä¿ç•™â‰¤75%åˆ†ä½æ•°çš„ä½å›°æƒ‘åº¦æ•°æ®ï¼Œæ’é™¤å¼‚å¸¸å€¼
                        if perp <= modern_threshold and perp < 5000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "modern_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["modern_chinese_kept"] += 1
                        else:
                            filtered_data = data_item.copy()
                            filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¤{modern_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
                            f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
                            stats["perplexity_filtered"] += 1
                
                batch_texts = []
                batch_data = []
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if batch_texts:
            perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
            for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                data_item["perplexity"] = round(perp, 2)
                data_item["is_classic"] = is_classic_chinese(text)
                
                if data_item["is_classic"]:
                    if perp >= classic_threshold and perp < 10000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "classic_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        f_classic.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["classic_chinese_kept"] += 1
                    else:
                        filtered_data = data_item.copy()
                        filtered_data["filter_reason"] = f"å¤æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¥{classic_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
                        f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
                        stats["perplexity_filtered"] += 1
                else:
                    if perp <= modern_threshold and perp < 5000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "modern_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["modern_chinese_kept"] += 1
                    else:
                        filtered_data = data_item.copy()
                        filtered_data["filter_reason"] = f"ç°ä»£æ–‡å›°æƒ‘åº¦ä¸è¾¾æ ‡ï¼ˆéœ€â‰¤{modern_threshold:.2f}ï¼Œå½“å‰{perp:.2f}ï¼‰"
                        f_filtered.write(json.dumps(filtered_data, ensure_ascii=False) + "\n")
                        stats["perplexity_filtered"] += 1
    
    stats["stage_time"]["perplexity"] = round(time.time() - start_time, 2)
    logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’")
    logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
    logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
    return kept_file, filtered_file, classic_file

# ========== ä¸»å‡½æ•° ==========
def main():
    global monitor_running
    start_time = time.time()
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(OUTPUT_PATH, "clmmu_filter_log_final.log"), encoding="utf-8"),
            logging.StreamHandler()
        ]
    )
    
    logging.info("ğŸ‰ å¯åŠ¨CLMMU/CEVALç­›é€‰æµç¨‹ï¼ˆæœ€ç»ˆç‰ˆï¼‰")
    logging.info("ğŸ“‹ æ ¸å¿ƒé…ç½®ï¼ˆæŒ‰éœ€æ±‚è°ƒæ•´ï¼‰:")
    logging.info(f"   - è¯­ä¹‰å»é‡é˜ˆå€¼: {LSH_THRESHOLD}ï¼ˆå‡å°‘å†—ä½™ï¼‰")
    logging.info(f"   - ç°ä»£æ–‡å›°æƒ‘åº¦: â‰¤75%åˆ†ä½æ•°ï¼ˆä½å›°æƒ‘åº¦=é«˜è´¨é‡ï¼‰")
    logging.info(f"   - å¤æ–‡å›°æƒ‘åº¦: â‰¥10%åˆ†ä½æ•°ï¼ˆé«˜å›°æƒ‘åº¦=çœŸå®å¤æ–‡ï¼‰")
    logging.info(f"   - å£è¯­å…³é”®è¯: æ‰©å……è‡³{len(COLLOQUIAL_WORDS)}ä¸ªï¼ˆå¼ºåŒ–å­¦æœ¯è¿‡æ»¤ï¼‰")
    logging.info(f"   - å¤æ–‡å…³é”®è¯: æ‰©å……è‡³{len([w for w in is_classic_chinese.__code__.co_consts if isinstance(w, str) and len(w) <= 10])}ä¸ªï¼ˆç²¾å‡†è¯†åˆ«ï¼‰")
    logging.info(f"   - æ•æ„Ÿè¯é¢˜è¿‡æ»¤: å¯ç”¨ï¼ˆè¦†ç›–è‰²æƒ…ã€æš´åŠ›ã€æ¯’å“ç­‰ï¼‰")
    logging.info(f"   - å·²åˆ é™¤: å­¦ç§‘è¿‡æ»¤ã€é¢˜å‹é€‚é…ã€äº‹å®å‡†ç¡®æ€§æ£€æµ‹")
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor = threading.Thread(target=monitor_thread, daemon=True)
    monitor.start()
    
    try:
        # 1. é¢„å¤„ç†+MD5å»é‡+åŸºç¡€ç­›é€‰ï¼ˆå«æ•æ„Ÿè¿‡æ»¤ï¼‰
        preprocessed_file = preprocess_and_md5_deduplicate()
        
        # 2. Minhash LSHè¯­ä¹‰å»é‡
        minhash_file = minhash_lsh_deduplicate(preprocessed_file)
        
        # 3. ç¡®å®šåˆ†å±‚é˜ˆå€¼
        modern_threshold, classic_threshold = determine_layered_thresholds(minhash_file)
        
        # 4. åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰
        kept_file, filtered_file, classic_file = layered_perplexity_filter(minhash_file, modern_threshold, classic_threshold)
        
        # åœæ­¢ç›‘æ§
        monitor_running = False
        monitor.join()
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        total_time = round(time.time() - start_time, 2)
        logging.info("\n" + "="*80)
        logging.info("ğŸ“Š CLMMU/CEVALç­›é€‰æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Šï¼ˆæœ€ç»ˆç‰ˆï¼‰")
        logging.info("="*80)
        logging.info(f"æ€»è¾“å…¥æ•°æ®: {stats['total_input']}æ¡")
        logging.info(f"æŠ½æ ·åæ•°æ®: {stats['sampled_count']}æ¡")
        logging.info(f"ğŸ“Œ å„é˜¶æ®µè¿‡æ»¤ç»Ÿè®¡:")
        logging.info(f"   - é•¿åº¦è¿‡æ»¤: {stats['preprocess_filtered']}æ¡")
        logging.info(f"   - æ•æ„Ÿè¯é¢˜è¿‡æ»¤: {stats['sensitive_filtered']}æ¡")
        logging.info(f"   - å£è¯­åŒ–: {stats['colloquial_filtered']}æ¡")
        logging.info(f"   - æ— å­¦æœ¯ç‰¹å¾: {stats['non_academic_filtered']}æ¡")
        logging.info(f"   - MD5å®Œå…¨é‡å¤: {stats['md5_duplicated']}æ¡")
        logging.info(f"   - è¯­ä¹‰ç›¸ä¼¼é‡å¤: {stats['minhash_duplicated']}æ¡")
        logging.info(f"   - å›°æƒ‘åº¦è¿‡æ»¤: {stats['perplexity_filtered']}æ¡")
        logging.info(f"ğŸ“Œ æœ€ç»ˆä¿ç•™ç»Ÿè®¡:")
        logging.info(f"   - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']}æ¡")
        logging.info(f"   - å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}æ¡")
        logging.info(f"   - æ€»è®¡ä¿ç•™: {stats['final_kept']}æ¡")
        logging.info(f"   - ä¿ç•™æ¯”ä¾‹: {stats['final_kept']/stats['sampled_count']*100:.1f}%")
        logging.info(f"ğŸ“Œ é˜ˆå€¼é…ç½®:")
        logging.info(f"   - ç°ä»£æ–‡å›°æƒ‘åº¦: â‰¤{modern_threshold:.2f} (75%åˆ†ä½æ•°ï¼Œä½å›°æƒ‘åº¦ä¼˜è´¨)")
        logging.info(f"   - å¤æ–‡å›°æƒ‘åº¦: â‰¥{classic_threshold:.2f} (10%åˆ†ä½æ•°ï¼Œé«˜å›°æƒ‘åº¦çœŸå®)")
        logging.info(f"   - è¯­ä¹‰å»é‡: {LSH_THRESHOLD}")
        logging.info(f"ğŸ“Œ æ€§èƒ½ç»Ÿè®¡:")
        logging.info(f"   - æ€»è€—æ—¶: {total_time}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logging.info("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        logging.info(f"âœ… é«˜è´¨é‡æ•°æ®ï¼ˆæœ€ç»ˆç‰ˆï¼‰: {kept_file}")
        logging.info(f"ğŸ“œ å¤æ–‡ä¸“é—¨æ•°æ®: {classic_file}")
        logging.info(f"âŒ è¿‡æ»¤æ•°æ®è¯¦æƒ…: {filtered_file}")
        logging.info(f"ğŸ“Š ç­›é€‰æ—¥å¿—: {os.path.join(OUTPUT_PATH, 'clmmu_filter_log_final.log')}")
        logging.info("="*80)
        
        # ä¿ç•™æ¯”ä¾‹æ ¡å‡†æç¤º
        retention_ratio = stats['final_kept']/stats['sampled_count']*100
        if retention_ratio < 45:
            logging.warning(f"âš ï¸ ä¿ç•™æ¯”ä¾‹è¿‡ä½ï¼ˆ{retention_ratio:.1f}%ï¼‰ï¼Œå»ºè®®é€‚åº¦æ”¾æ¾ï¼š")
            logging.warning(f"   1. è¯­ä¹‰å»é‡é˜ˆå€¼ä»0.76â†’0.73")
            logging.warning(f"   2. ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†ä½æ•°ä»75%â†’80%")
        elif retention_ratio > 55:
            logging.warning(f"âš ï¸ ä¿ç•™æ¯”ä¾‹è¿‡é«˜ï¼ˆ{retention_ratio:.1f}%ï¼‰ï¼Œå»ºè®®é€‚åº¦æ”¶ç´§ï¼š")
            logging.warning(f"   1. è¯­ä¹‰å»é‡é˜ˆå€¼ä»0.76â†’0.78")
            logging.warning(f"   2. ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†ä½æ•°ä»75%â†’70%")
        else:
            logging.info("âœ… ä¿ç•™æ¯”ä¾‹è¾¾æ ‡ï¼ˆ45%-55%ï¼‰ï¼Œæ•°æ®è´¨é‡ä¸æ•°é‡å¹³è¡¡è‰¯å¥½ï¼")
        
    except Exception as e:
        logging.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
    finally:
        monitor_running = False
        monitor.join(timeout=5)
        logging.info("ğŸ”š CLMMU/CEVALç­›é€‰æµç¨‹ç»“æŸï¼ˆæœ€ç»ˆç‰ˆï¼‰")

if __name__ == "__main__":
    main()