import os
import json
import torch
import logging
import mmap
import time
import hashlib
import psutil
import threading
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from datasketch import MinHash, MinHashLSH

# æ¢å¤ matplotlib ç”¨äºç”Ÿæˆå›°æƒ‘åº¦åˆ†å¸ƒå›¾ï¼ˆå…³é”®ï¼‰
try:
    import matplotlib.pyplot as plt
    plt.switch_backend('Agg')  # æ— GUIç¯å¢ƒå…¼å®¹ï¼ˆé¿å…æŠ¥é”™ï¼‰
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    logging.warning("âš ï¸ æœªå®‰è£… matplotlibï¼Œæ— æ³•ç”Ÿæˆå›°æƒ‘åº¦åˆ†å¸ƒå›¾")
    MATPLOTLIB_AVAILABLE = False

# å¿½ç•¥æ— å…³è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========== æ ¸å¿ƒé…ç½®ï¼ˆä¿ç•™æ ¸å¿ƒæ–‡ä»¶+åˆ†å¸ƒå›¾ï¼‰ ==========
INPUT_DIR = "data"
INTERMEDIATE_PATH = "data/intermediate"
OUTPUT_PATH = "data/output"
os.makedirs(INTERMEDIATE_PATH, exist_ok=True)
os.makedirs(OUTPUT_PATH, exist_ok=True)

# å¤ç”¨ä¸­é—´æ–‡ä»¶é…ç½®ï¼ˆå¿…é¡»å¯ç”¨ï¼‰
USE_EXISTING_INTERMEDIATE = True
EXISTING_MINHASH_FILE = os.path.join(INTERMEDIATE_PATH, "minhash_dedup_with_sensitive_filter.jsonl")

# æ‰¹é‡é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
BATCH_SIZE_PREPROCESS = 1024
BATCH_SIZE_PERPLEXITY = 32
BATCH_SIZE_MINHASH = 5000

# åŸºç¡€é…ç½®ï¼ˆä¿æŒä¸å˜ï¼Œå…¼å®¹ä¸­é—´æ–‡ä»¶ï¼‰
MIN_CHAR_LEN = 8
MAX_CHAR_LEN = 12000
MAX_SEQ_LENGTH = 512
MINHASH_NUM_PERM = 128
LSH_THRESHOLD = 0.76

# æ¨¡å‹é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ========== æ ¸å¿ƒç­›é€‰é…ç½®ï¼ˆä¿æŒä¸å˜ï¼Œå…¼å®¹ä¸­é—´æ–‡ä»¶ï¼‰ ==========
COLLOQUIAL_WORDS = [
    "å§æ§½", "ç‰›é€¼", "å“ˆå“ˆå“ˆ", "å˜»å˜»", "å˜¿å˜¿", "è€é“", "æ‡‚å§", "ç»™åŠ›", "666", "yyds",
    "ç»ç»å­", "å®¶äººä»¬", "è°æ‡‚å•Š", "æ•‘å‘½", "å“­æ­»", "ç¬‘ä¸æ´»", "æ “Q", "æ‹¿æ", "ç ´é˜²", "èººå¹³",
    "ç»äº†", "æ— è¯­", "æœäº†", "å‡‘æ´»", "å’‹æ•´", "å” å—‘", "ä¾ƒå¤§å±±", "æ‰¯çŠŠå­", "çé€¼é€¼", "é€¼é€¼èµ–èµ–",
    "ç£¨ç£¨å”§å”§", "å½å½æ­ªæ­ª", "ç¢ç¢å¿µ", "åæ§½", "æ€¼äºº", "æ ç²¾", "å†…å·", "æ‘†çƒ‚", "æ‘¸é±¼",
    "åˆ’æ°´", "æ‰“å·¥äºº", "å¹²é¥­äºº", "å°¾æ¬¾äºº", "å·¥å…·äºº", "å†¤ç§", "æ˜¾çœ¼åŒ…", "ç¤¾æ", "ç¤¾ç‰›", "ç¤¾æ­»",
    "emo", "ä½›ç³»", "å·ç‹", "æ‘†çƒ‚å¼", "æ‘¸é±¼å¼", "åˆ’æ°´å¼", "èººå¹³å¼", "æ•·è¡å¼", "ç³Šå¼„å­¦", "PUA",
    "CPU", "KTV", "yyds", "awsl", "ç»ç»å­", "YYDS", "æ “Q", "æ‹¿æäº†", "ç ´é˜²äº†"
]

SENSITIVE_KEYWORDS = {
    "è‰²æƒ…ç›¸å…³": ["è‰²æƒ…", "é»„è‰²", "è£¸èŠ", "æ€§äº¤æ˜“", "å«–å¨¼", "å–æ·«", "æ·«è¡", "è‰²æƒ…è§†é¢‘", "è‰²æƒ…å›¾ç‰‡", "AV",
                "ä¸‰çº§ç‰‡", "æ˜¥å®«", "è‰³ç…§", "éœ²éª¨", "æ€§è¡Œä¸º", "æ€§å™¨å®˜", "æ‰‹æ·«", "åŒ…å…»", "å°ä¸‰",
                "äºŒå¥¶", "æƒ…å¤«", "æƒ…å¦‡", "ä¸æ­£å½“å…³ç³»", "ä¸€å¤œæƒ…", "çº¦ç‚®", "æ€§æœåŠ¡", "è‰²æƒ…ç›´æ’­", "è‰²æƒ…å°è¯´"],
    "æš´åŠ›ç›¸å…³": ["æ€äºº", "æŠ¢åŠ«", "å¼ºå¥¸", "ç»‘æ¶", "æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ€äººæ”¾ç«", "çˆ†ç‚¸", "æŠ•æ¯’",
                "å‡¶å™¨", "æªæ”¯", "å¼¹è¯", "ç®¡åˆ¶åˆ€å…·", "æš´åŠ›", "è¡€è…¥", "ææ€–", "è™æ€", "è™å¾…", "æ–½æš´",
                "æ®´æ‰“", "ç¾¤æ®´", "äº’æ®´", "å¯»è¡…æ»‹äº‹", "èšä¼—æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ•…æ„æ€äºº", "æŠ¢åŠ«è´¢ç‰©"],
    "æ¯’å“ç›¸å…³": ["æ¯’å“", "å¤§éº»", "æµ·æ´›å› ", "å†°æ¯’", "å¯å¡å› ", "æ‘‡å¤´ä¸¸", "Kç²‰", "é¸¦ç‰‡", "å—å•¡", "æœå†·ä¸",
                "å¸æ¯’", "è´©æ¯’", "åˆ¶æ¯’", "å¸æ¯’è€…", "æ¯’è´©", "æ¯’å“äº¤æ˜“", "æ¯’å“è¿è¾“", "æ¯’å“èµ°ç§"],
    "æ”¿æ²»æ•æ„Ÿ": ["æ•æ„Ÿæ”¿æ²»äººç‰©", "æ”¿æ²»æ•æ„Ÿäº‹ä»¶", "é¢ è¦†", "åˆ†è£‚", "å›å›½", "æš´åŠ¨", "éªšä¹±", "éæ³•é›†ä¼š",
                "ååŠ¨", "åæ”¿åºœ", "åç¤¾ä¼š", "æç«¯ä¸»ä¹‰", "ææ€–ä¸»ä¹‰", "é‚ªæ•™", "æ³•è½®åŠŸ", "å°ç‹¬", "æ¸¯ç‹¬", "ç–†ç‹¬"],
    "å…¶ä»–æ•æ„Ÿ": ["èµŒåš", "è¯ˆéª—", "ä¼ é”€", "éæ³•é›†èµ„", "æ´—é’±", "å·ç¨æ¼ç¨", "è´ªæ±¡è…è´¥", "è¡Œè´¿å—è´¿",
                "å‡å¸", "éæ³•äº¤æ˜“", "é»‘å®¢", "å…¥ä¾µ", "ç—…æ¯’", "ç›—å·", "è¯ˆéª—çŸ­ä¿¡", "è¯ˆéª—ç”µè¯"]
}

ACADEMIC_PATTERNS = [
    r"[A-Za-z0-9]=.*[A-Za-z0-9]", r"å®šä¹‰[:ï¼š]", r"å®šç†", r"å…¬ç†", r"å‘½é¢˜", r"æ¨è®º", r"åŸç†", r"æ–¹æ³•", r"å®éªŒ", r"åˆ†æ", r"ç»“è®º",
]
ACADEMIC_REQUIRE = False

# å…¨å±€ç»Ÿè®¡å˜é‡ï¼ˆåªä¿ç•™æ ¸å¿ƒç»Ÿè®¡é¡¹ï¼‰
stats = {
    "final_kept": 0,
    "classic_chinese_kept": 0,
    "modern_chinese_kept": 0,
    "perplexity_filtered": 0,
    "stage_time": {}
}

# ç›‘æ§çº¿ç¨‹å˜é‡ï¼ˆä¿æŒä¸å˜ï¼‰
monitor_running = True
gpu_util = 0
cpu_mem = 0

# ========== å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œå…¼å®¹ä¸­é—´æ–‡ä»¶ï¼‰ ==========
def get_gpu_utilization():
    try:
        result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
        return int(result.strip().split("\n")[0]) if result else 0
    except:
        return 0

def get_cpu_memory():
    process = psutil.Process(os.getpid())
    return round(process.memory_info().rss / (1024 * 1024), 2)

def monitor_thread():
    global monitor_running, gpu_util, cpu_mem
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
    while monitor_running:
        gpu_util = get_gpu_utilization()
        cpu_mem = get_cpu_memory()
        progress = (stats["final_kept"] / (stats["final_kept"] + stats["perplexity_filtered"]) * 100) if (stats["final_kept"] + stats["perplexity_filtered"]) > 0 else 0.0
        logging.info(
            f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
            f"å·²ä¿ç•™ï¼š{stats['final_kept']} | å·²è¿‡æ»¤ï¼š{stats['perplexity_filtered']} | è¿›åº¦ï¼š{progress:.1f}%"
        )
        time.sleep(30)
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

# ========== æ ¸å¿ƒç­›é€‰å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œå…¼å®¹ä¸­é—´æ–‡ä»¶ï¼‰ ==========
def is_colloquial(text):
    for word in COLLOQUIAL_WORDS:
        if word in text:
            return True
    if re.search(r"[ï¼ï¼Ÿã€‚,ï¼Œï¼›;ï¼š:]{3,}", text):
        return True
    colloquial_patterns = [
        r"[æˆ‘ä½ ä»–å¥¹å®ƒ]ï¼ˆä»¬ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"[è¿™é‚£å“ª]ï¼ˆä¸ªäº›ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"^[å“ˆå“ˆå˜¿å˜¿å˜»å˜»å‘µå‘µ]+"
    ]
    if any(re.search(pattern, text) for pattern in colloquial_patterns):
        return True
    return False

def is_sensitive(text):
    for category, words in SENSITIVE_KEYWORDS.items():
        for word in words:
            if word in text:
                return True
    sensitive_patterns = [
        r"å‡ºå”®.*(è‰²æƒ…|AV|ä¸‰çº§ç‰‡)",
        r"(å«–å¨¼|å–æ·«|æ€§äº¤æ˜“).*(ä»·æ ¼|è”ç³»æ–¹å¼|åœ°ç‚¹)",
        r"(æ€äºº|æŠ¢åŠ«|ç»‘æ¶).*(æ–¹æ³•|æ•™ç¨‹|å·¥å…·)",
        r"(æ¯’å“|å¤§éº»|å†°æ¯’).*(è´­ä¹°|å‡ºå”®|è¿è¾“)",
        r"(å°ç‹¬|æ¸¯ç‹¬|ç–†ç‹¬).*(æ”¯æŒ|å®£ä¼ |åˆ†è£‚)"
    ]
    if any(re.search(pattern, text, re.IGNORECASE) for pattern in sensitive_patterns):
        return True
    return False

def has_academic_features(text):
    return any(re.search(pattern, text) for pattern in ACADEMIC_PATTERNS)

# ========== å¤æ–‡æ£€æµ‹å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ==========
def is_classic_chinese(text):
    classic_words = [
        'ä¹‹', 'ä¹', 'è€…', 'ä¹Ÿ', 'æ›°', 'å¾', 'æ±', 'å°”', 'ä¹ƒ', 'å…®', 'çŸ£', 'å“‰', 'è€¶', 'æ¬¤', 'ç„‰', 'ä¹', 'å…¶', 'è€Œ', 'ä»¥', 'äº',
        'å¤«', 'ç›–', 'åˆ™', 'ä¸”', 'è‹¥', 'ä½•', 'å­°', 'å®‰', 'å­°ä¸', 'æ‰€ä»¥', 'æ‰€', 'å¯', 'èƒ½', 'å¿…', 'å½“', 'åº”',
        'å¤', 'å•†', 'å‘¨', 'ç§¦', 'æ±‰', 'é­', 'èœ€', 'å´', 'æ™‹', 'éš‹', 'å”', 'å®‹', 'å…ƒ', 'æ˜', 'æ¸…',
        'é»„å¸', 'ç‚å¸', 'å°§', 'èˆœ', 'ç¦¹', 'æ±¤', 'æ–‡ç‹', 'æ­¦ç‹', 'å‘¨å…¬', 'å­”å­', 'å­Ÿå­', 'è€å­', 'åº„å­', 'å¢¨å­', 'è€å­', 'éŸ©éå­',
        'ã€Šè®ºè¯­ã€‹', 'ã€Šå­Ÿå­ã€‹', 'ã€Šå¤§å­¦ã€‹', 'ã€Šä¸­åº¸ã€‹', 'ã€Šè¯—ç»ã€‹', 'ã€Šå°šä¹¦ã€‹', 'ã€Šç¤¼è®°ã€‹', 'ã€Šå‘¨æ˜“ã€‹', 'ã€Šæ˜¥ç§‹ã€‹',
        'ã€Šé“å¾·ç»ã€‹', 'ã€Šåº„å­ã€‹', 'ã€Šå¢¨å­ã€‹', 'ã€Šè€å­ã€‹', 'ã€ŠéŸ©éå­ã€‹', 'ã€Šå²è®°ã€‹', 'ã€Šæ±‰ä¹¦ã€‹', 'ã€Šåæ±‰ä¹¦ã€‹', 'ã€Šä¸‰å›½å¿—ã€‹',
        'å‘œå‘¼', 'å—Ÿå¤«', 'ç›–é—»', 'çªƒä»¥ä¸º', 'è‡£é—»', 'åœ£ç‹', 'è´¤å›', 'å¿ è‡£', 'ä¹‰å£«', 'å­å­', 'çƒˆå¥³'
    ]
    modern_words = ["æ‰‹æœº", "äº’è”ç½‘", "ç”µè„‘", "å¾®ä¿¡", "æ”¯ä»˜å®", "å¿«é€’", "é«˜é“", "ç©ºè°ƒ", "ç”µè§†", "ç½‘ç»œ", "APP",
                   "å¾®åš", "æŠ–éŸ³", "å¿«æ‰‹", "ç›´æ’­", "ç”µå•†", "ç½‘è´­", "å¤–å–", "æ‰“è½¦", "å…±äº«å•è½¦", "5G", "WiFi"]
    
    for word in modern_words:
        if word in text:
            return False
    
    total_chars = len(text)
    if total_chars == 0:
        return False
    
    classic_char_count = 0
    for word in classic_words:
        classic_char_count += text.count(word)
    density_threshold = 0.02
    
    classic_patterns = [
        r'^[\u4e00-\u9fff]{1,5}æ›°', r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^å‘œå‘¼', r'^å—Ÿå¤«',
        r'^ç›–é—»', r'^çªƒä»¥ä¸º', r'^è‡£é—»', r'^åœ£ç‹', r'^è´¤å›', r'^å¿ è‡£', r'^ä¹‰å£«'
    ]
    pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
    is_classic = (classic_char_count / total_chars > density_threshold) or pattern_match
    return is_classic

# ========== å›°æƒ‘åº¦è®¡ç®—å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ==========
def load_perplexity_model():
    logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦")
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
    model.eval()
    logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
    return tokenizer, model

def calculate_perplexity_batch_optimized(texts, tokenizer, model):
    try:
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=MAX_SEQ_LENGTH, 
            return_tensors="pt"
        ).to(DEVICE)
        
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        batch_size = input_ids.shape[0]
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            perplexities = []
            for i in range(batch_size):
                valid_mask = (input_ids[i] != tokenizer.pad_token_id)
                valid_token_ids = input_ids[i][valid_mask]
                
                if len(valid_token_ids) <= 1:
                    perplexities.append(float('inf'))
                    continue
                
                shift_logits = logits[i, :-1, :].contiguous()
                shift_labels = valid_token_ids[1:].contiguous()
                valid_len = len(shift_labels)
                shift_logits = shift_logits[:valid_len, :]
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)), 
                    shift_labels.view(-1)
                )
                
                perplexity = torch.exp(loss).item()
                perplexities.append(perplexity)
            
            return perplexities
            
    except RuntimeError as e:
        if "out of memory" in str(e):
            logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
            half_size = len(texts) // 2
            if half_size >= 1:
                perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
                perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
                return perplexities1 + perplexities2
            else:
                return [calculate_perplexity(text, tokenizer, model) for text in texts]
        else:
            raise e
    except Exception as e:
        logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
        return [calculate_perplexity(text, tokenizer, model) for text in texts]

def calculate_perplexity(text, tokenizer, model):
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["input_ids"][..., 1:].contiguous()
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            avg_loss = loss.mean()
            return torch.exp(avg_loss).item()
    except:
        return float('inf')

# ========== é˜ˆå€¼è®¡ç®—ï¼ˆæ¢å¤å›°æƒ‘åº¦åˆ†å¸ƒå›¾ç”Ÿæˆï¼‰ ==========
def analyze_perplexity_distribution_layered(input_file, sample_size=1000):
    logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆå°†ç”Ÿæˆåˆ†å¸ƒå›¾ï¼‰")
    tokenizer, model = load_perplexity_model()
    
    texts = []
    with open(input_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
        if len(lines) > sample_size:
            import random
            lines = random.sample(lines, sample_size)
        for line in lines:
            try:
                data = json.loads(line)
                text = data.get("text", "").strip()
                if text and len(text) >= MIN_CHAR_LEN:
                    texts.append(text)
            except:
                continue
    
    if not texts:
        logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
        return 100.0, 30.0, None, None
    
    logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
    classic_texts = []
    modern_texts = []
    for text in texts:
        if is_classic_chinese(text):
            classic_texts.append(text)
        else:
            modern_texts.append(text)
    
    logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
    modern_perplexities = []
    for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
        batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        modern_perplexities.extend(batch_perplexities)
    
    classic_perplexities = []
    for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
        batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        classic_perplexities.extend(batch_perplexities)
    
    modern_perplexities = np.array(modern_perplexities)
    classic_perplexities = np.array(classic_perplexities)
    
    # è¿‡æ»¤å¼‚å¸¸å¤§çš„å›°æƒ‘åº¦å€¼ï¼ˆé¿å…åˆ†å¸ƒå›¾å¤±çœŸï¼‰
    modern_valid = modern_perplexities[modern_perplexities < 15000]
    classic_valid = classic_perplexities[classic_perplexities < 15000]
    
    if len(modern_valid) == 0:
        logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        modern_valid = np.array([100.0])
    
    if len(classic_valid) == 0:
        logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        classic_valid = np.array([30.0])
    
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_percentiles = np.percentile(modern_valid, percentiles)
    classic_percentiles = np.percentile(classic_valid, percentiles)
    
    modern_threshold_idx = 6  # 75%åˆ†ä½æ•°
    classic_threshold_idx = 1  # 10%åˆ†ä½æ•°
    
    modern_threshold = modern_percentiles[modern_threshold_idx]
    classic_threshold = classic_percentiles[classic_threshold_idx]
    
    # è¾“å‡ºé˜ˆå€¼æ—¥å¿—
    logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒ:")
    for p, val in zip(percentiles, modern_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f} ({percentiles[modern_threshold_idx]}%åˆ†ä½æ•°)")
    
    logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒ:")
    for p, val in zip(percentiles, classic_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ å¤æ–‡é˜ˆå€¼: â‰¥{classic_threshold:.2f} ({percentiles[classic_threshold_idx]}%åˆ†ä½æ•°)")
    
    # æ¢å¤ç”Ÿæˆå›°æƒ‘åº¦åˆ†å¸ƒå›¾ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
    if MATPLOTLIB_AVAILABLE:
        try:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']  # å…¼å®¹ä¸­æ–‡/è‹±æ–‡
            plt.rcParams['axes.unicode_minus'] = False
            
            # åˆ›å»º2x1å­å›¾ï¼Œåˆ†åˆ«æ˜¾ç¤ºç°ä»£æ–‡å’Œå¤æ–‡åˆ†å¸ƒ
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            fig.suptitle('Perplexity Distribution (Classic vs Modern Chinese)', fontsize=16, fontweight='bold')
            
            # ç°ä»£æ–‡å›°æƒ‘åº¦ç›´æ–¹å›¾ï¼ˆæ·»åŠ é˜ˆå€¼çº¿ï¼‰
            ax1.hist(modern_valid, bins=50, color='#2E86AB', alpha=0.7, edgecolor='black', linewidth=0.5)
            ax1.axvline(modern_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {modern_threshold:.1f} (75th)')
            ax1.set_title('Modern Chinese Perplexity', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Perplexity Value', fontsize=12)
            ax1.set_ylabel('Frequency', fontsize=12)
            ax1.legend(fontsize=10)
            ax1.grid(True, alpha=0.3)
            
            # å¤æ–‡å›°æƒ‘åº¦ç›´æ–¹å›¾ï¼ˆæ·»åŠ é˜ˆå€¼çº¿ï¼‰
            ax2.hist(classic_valid, bins=50, color='#A23B72', alpha=0.7, edgecolor='black', linewidth=0.5)
            ax2.axvline(classic_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {classic_threshold:.1f} (10th)')
            ax2.set_title('Classic Chinese Perplexity', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Perplexity Value', fontsize=12)
            ax2.set_ylabel('Frequency', fontsize=12)
            ax2.legend(fontsize=10)
            ax2.grid(True, alpha=0.3)
            
            # è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾é‡å 
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡ï¼ˆä»…ç”Ÿæˆ1å¼ ï¼Œæ— å…¶ä»–å†—ä½™ï¼‰
            plot_path = os.path.join(OUTPUT_PATH, 'perplexity_distribution.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logging.info(f"âœ… å›°æƒ‘åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜ï¼š{plot_path}")
            logging.info(f"ğŸ“Š å›¾ç‰‡å¤§å°ï¼š{os.path.getsize(plot_path)/1024/1024:.2f}MB")
        except Exception as e:
            logging.warning(f"âš ï¸ ç”Ÿæˆå›°æƒ‘åº¦åˆ†å¸ƒå›¾å¤±è´¥ï¼š{str(e)}")
    else:
        logging.warning("âš ï¸ æœªå®‰è£… matplotlibï¼Œè·³è¿‡åˆ†å¸ƒå›¾ç”Ÿæˆ")
    
    return modern_threshold, classic_threshold, modern_percentiles, classic_percentiles

def determine_layered_thresholds(input_file):
    logging.info("ğŸ¯ å¼€å§‹ç¡®å®šåˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼")
    try:
        modern_threshold, classic_threshold, modern_percentiles, classic_percentiles = analyze_perplexity_distribution_layered(input_file, sample_size=500)
        percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
        modern_p = percentiles[6]
        classic_p = percentiles[1]
        logging.info(f"ğŸ¤– ç°ä»£æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)")
        logging.info(f"ğŸ“œ å¤æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)")
        return modern_threshold, classic_threshold
    except Exception as e:
        logging.error(f"âŒ ç¡®å®šåˆ†å±‚é˜ˆå€¼å¤±è´¥: {str(e)}")
        logging.info("ğŸ”„ ä½¿ç”¨é»˜è®¤é˜ˆå€¼: ç°ä»£æ–‡=80 (75%åˆ†ä½æ•°), å¤æ–‡=30 (10%åˆ†ä½æ•°)")
        return 80.0, 30.0

# ========== åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆä»…ä¿ç•™æ ¸å¿ƒæ–‡ä»¶+åˆ†å¸ƒå›¾ï¼‰ ==========
def layered_perplexity_filter(input_file, modern_threshold, classic_threshold):
    start_time = time.time()
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_p = percentiles[6]
    classic_p = percentiles[1]
    
    logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆä»…ä¿ç•™æ ¸å¿ƒæ•°æ®æ–‡ä»¶+åˆ†å¸ƒå›¾ï¼‰")
    logging.info(f"   - ç°ä»£æ–‡ï¼šâ‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)")
    logging.info(f"   - å¤æ–‡ï¼šâ‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)")
    
    tokenizer, model = load_perplexity_model()
    
    # åªåˆ›å»ºæ ¸å¿ƒè¾“å‡ºæ–‡ä»¶ï¼ˆä¸ç”Ÿæˆè¿‡æ»¤æ–‡ä»¶ã€å¤æ–‡å•ç‹¬æ–‡ä»¶ï¼‰
    kept_file = os.path.join(OUTPUT_PATH, "clmmu_kept_data_final.jsonl")
    
    batch_texts = []
    batch_data = []
    
    # åªæ‰“å¼€æ ¸å¿ƒæ–‡ä»¶
    with open(input_file, "r", encoding="utf-8") as f_in, \
         open(kept_file, "w", encoding="utf-8") as f_kept:
        
        total_input = sum(1 for _ in f_in)  # ç»Ÿè®¡ä¸­é—´æ–‡ä»¶æ€»è¡Œæ•°
        f_in.seek(0)
        
        for line in tqdm(f_in, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰", total=total_input):
            try:
                data = json.loads(line)
                batch_texts.append(data["text"])
                batch_data.append(data)
            except:
                continue
            
            if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
                perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
                for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                    data_item["perplexity"] = round(perp, 2)
                    data_item["is_classic"] = is_classic_chinese(text)
                    
                    # ç­›é€‰é€»è¾‘ä¸å˜ï¼Œä»…å†™å…¥æ ¸å¿ƒæ–‡ä»¶
                    if data_item["is_classic"]:
                        if perp >= classic_threshold and perp < 10000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "classic_chinese"  # ä¿ç•™ç±»å‹æ ‡è®°
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["classic_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                    else:
                        if perp <= modern_threshold and perp < 5000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "modern_chinese"  # ä¿ç•™ç±»å‹æ ‡è®°
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["modern_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                
                batch_texts = []
                batch_data = []
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if batch_texts:
            perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
            for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                data_item["perplexity"] = round(perp, 2)
                data_item["is_classic"] = is_classic_chinese(text)
                
                if data_item["is_classic"]:
                    if perp >= classic_threshold and perp < 10000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "classic_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["classic_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
                else:
                    if perp <= modern_threshold and perp < 5000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "modern_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["modern_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
    
    stats["stage_time"]["perplexity"] = round(time.time() - start_time, 2)
    logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’")
    logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
    logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
    return kept_file  # åªè¿”å›æ ¸å¿ƒæ–‡ä»¶

# ========== ä¸»å‡½æ•°ï¼ˆä»…è¾“å‡ºæ ¸å¿ƒæ–‡ä»¶+åˆ†å¸ƒå›¾ï¼Œå…³é—­æ—¥å¿—æ–‡ä»¶ï¼‰ ==========
def main():
    global monitor_running
    start_time = time.time()
    
    # æ—¥å¿—ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä¸å†™å…¥æ–‡ä»¶ï¼ˆèŠ‚çœç©ºé—´ï¼‰
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler()]
    )
    
    logging.info("ğŸ‰ å¯åŠ¨CLMMU/CEVALç­›é€‰æµç¨‹ï¼ˆæ ¸å¿ƒæ–‡ä»¶+å›°æƒ‘åº¦åˆ†å¸ƒå›¾ç‰ˆï¼‰")
    logging.info(f"âš ï¸  é…ç½®ï¼šç”Ÿæˆ clmmu_kept_data_final.jsonl + perplexity_distribution.png")
    logging.info(f"ğŸ“‹ å¤ç”¨çš„ä¸­é—´æ–‡ä»¶ï¼š{EXISTING_MINHASH_FILE}")
    logging.info("="*80)
    
    # éªŒè¯ä¸­é—´æ–‡ä»¶
    if not os.path.exists(EXISTING_MINHASH_FILE):
        logging.error(f"âŒ æœªæ‰¾åˆ°ä¸­é—´æ–‡ä»¶ï¼š{EXISTING_MINHASH_FILE}")
        return
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor = threading.Thread(target=monitor_thread, daemon=True)
    monitor.start()
    
    try:
        # åŠ è½½ä¸­é—´æ–‡ä»¶
        minhash_file = EXISTING_MINHASH_FILE
        total_intermediate = sum(1 for _ in open(minhash_file, 'r', encoding='utf-8'))
        logging.info(f"âœ… åŠ è½½ä¸­é—´æ–‡ä»¶ï¼š{minhash_file}ï¼ˆæ€»è¡Œæ•°ï¼š{total_intermediate}æ¡ï¼‰")
        
        # è®¡ç®—é˜ˆå€¼ï¼ˆç”Ÿæˆåˆ†å¸ƒå›¾ï¼‰
        modern_threshold, classic_threshold = determine_layered_thresholds(minhash_file)
        
        # ç­›é€‰ï¼ˆä»…ç”Ÿæˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
        kept_file = layered_perplexity_filter(minhash_file, modern_threshold, classic_threshold)
        
        # åœæ­¢ç›‘æ§
        monitor_running = False
        monitor.join()
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = round(time.time() - start_time, 2)
        logging.info("\n" + "="*80)
        logging.info("ğŸ“Š ç­›é€‰æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Šï¼ˆæ ¸å¿ƒæ–‡ä»¶+åˆ†å¸ƒå›¾ç‰ˆï¼‰")
        logging.info("="*80)
        logging.info(f"ä¸­é—´æ–‡ä»¶æ€»è¡Œæ•°: {total_intermediate}æ¡")
        logging.info(f"ğŸ“Œ ä¿ç•™ç»Ÿè®¡:")
        logging.info(f"   - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']}æ¡")
        logging.info(f"   - å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}æ¡")
        logging.info(f"   - æ€»è®¡ä¿ç•™: {stats['final_kept']}æ¡")
        logging.info(f"   - ä¿ç•™æ¯”ä¾‹: {stats['final_kept']/total_intermediate*100:.1f}%")
        logging.info(f"ğŸ“Œ é˜ˆå€¼é…ç½®:")
        logging.info(f"   - ç°ä»£æ–‡å›°æƒ‘åº¦: â‰¤{modern_threshold:.2f} (75%åˆ†ä½æ•°)")
        logging.info(f"   - å¤æ–‡å›°æƒ‘åº¦: â‰¥{classic_threshold:.2f} (10%åˆ†ä½æ•°)")
        logging.info(f"ğŸ“Œ æ€§èƒ½ç»Ÿè®¡:")
        logging.info(f"   - æ€»è€—æ—¶: {total_time}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logging.info("\nğŸ“ æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆä»…2ä¸ªï¼‰:")
        logging.info(f"âœ… é«˜è´¨é‡æ•°æ®ï¼š{kept_file}")
        logging.info(f"ğŸ“Š æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(kept_file)/1024/1024:.2f}MB")
        if MATPLOTLIB_AVAILABLE and os.path.exists(os.path.join(OUTPUT_PATH, 'perplexity_distribution.png')):
            plot_path = os.path.join(OUTPUT_PATH, 'perplexity_distribution.png')
            logging.info(f"âœ… å›°æƒ‘åº¦åˆ†å¸ƒå›¾ï¼š{plot_path}")
            logging.info(f"ğŸ“Š å›¾ç‰‡å¤§å°ï¼š{os.path.getsize(plot_path)/1024/1024:.2f}MB")
        logging.info("="*80)
        
        # æç¤ºåˆ é™¤æ— ç”¨æ–‡ä»¶é‡Šæ”¾ç©ºé—´
        logging.info("\nğŸ—‘ï¸  å¯åˆ é™¤çš„æ— ç”¨æ–‡ä»¶ï¼ˆé‡Šæ”¾ç©ºé—´ï¼‰:")
        logging.info(f"   1. ä¸­é—´æ–‡ä»¶ï¼š{EXISTING_MINHASH_FILE}ï¼ˆç­›é€‰å®Œæˆåå¯åˆ ï¼‰")
        logging.info(f"   2. é¢„å¤„ç†ä¸­é—´æ–‡ä»¶ï¼šdata/intermediate/preprocessed_md5_dedup_with_sensitive_filter.jsonl")
        
    except Exception as e:
        logging.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
    finally:
        monitor_running = False
        monitor.join(timeout=5)
        logging.info("ğŸ”š ç­›é€‰æµç¨‹ç»“æŸï¼ˆæ ¸å¿ƒæ–‡ä»¶+åˆ†å¸ƒå›¾ç‰ˆï¼‰")

if __name__ == "__main__":
    main()