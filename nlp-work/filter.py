import os
import json
import torch
import logging
import mmap
import time
import hashlib
import psutil
import threading
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from datasketch import MinHash, MinHashLSH

# å¿½ç•¥æ— å…³è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========== æ ¸å¿ƒé…ç½®ï¼ˆæ»¡è¶³éœ€æ±‚+50%ä¿ç•™ç‡ï¼‰ ==========
INPUT_DIR = "data"
OUTPUT_PATH = "data/output"  # ä»…ä¿ç•™è¾“å‡ºç›®å½•ï¼ˆæ— ä¸­é—´ç›®å½•ï¼‰
os.makedirs(OUTPUT_PATH, exist_ok=True)

# æŠ½æ ·é…ç½®
SAMPLING_ENABLE = True
SAMPLE_RATIO = 0.01
MAX_SAMPLE_COUNT = 1000

# æ‰¹é‡é…ç½®
BATCH_SIZE_PREPROCESS = 1024
BATCH_SIZE_PERPLEXITY = 32
BATCH_SIZE_MINHASH = 5000

# æ•°æ®è¿‡æ»¤åŸºç¡€é…ç½®
MIN_CHAR_LEN = 8
MAX_CHAR_LEN = 12000
MAX_SEQ_LENGTH = 512

# å»é‡é…ç½®ï¼ˆé€‚åº¦æ”¶ç´§ï¼Œå¼¥è¡¥åˆ é™¤çš„ç­›é€‰ç¯èŠ‚ï¼‰
MINHASH_NUM_PERM = 128
LSH_THRESHOLD = 0.76  # ä»0.75â†’0.76ï¼Œå‡å°‘å†—ä½™

# æ¨¡å‹é…ç½®
MODEL_ID = "uer/gpt2-chinese-cluecorpussmall"
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ========== æ ¸å¿ƒç­›é€‰é…ç½®ï¼ˆæ»¡è¶³éœ€æ±‚è°ƒæ•´ï¼‰ ==========
# 1. å£è¯­å…³é”®è¯æ‰©å……ï¼ˆæ–°å¢40+ï¼Œå¼ºåŒ–è¿‡æ»¤ï¼‰
COLLOQUIAL_WORDS = [
    # åŸæœ‰å…³é”®è¯
    "å§æ§½", "ç‰›é€¼", "å“ˆå“ˆå“ˆ", "å˜»å˜»", "å˜¿å˜¿", "è€é“", "æ‡‚å§", "ç»™åŠ›", "666", "yyds",
    "ç»ç»å­", "å®¶äººä»¬", "è°æ‡‚å•Š", "æ•‘å‘½", "å“­æ­»", "ç¬‘ä¸æ´»", "æ “Q", "æ‹¿æ", "ç ´é˜²", "èººå¹³",
    # æ–°å¢å£è¯­/ç½‘ç»œæµè¡Œè¯­
    "ç»äº†", "æ— è¯­", "æœäº†", "å‡‘æ´»", "å’‹æ•´", "å” å—‘", "ä¾ƒå¤§å±±", "æ‰¯çŠŠå­", "çé€¼é€¼", "é€¼é€¼èµ–èµ–",
    "ç£¨ç£¨å”§å”§", "å½å½æ­ªæ­ª", "ç¢ç¢å¿µ", "åæ§½", "æ€¼äºº", "æ ç²¾", "å†…å·", "èººå¹³", "æ‘†çƒ‚", "æ‘¸é±¼",
    "åˆ’æ°´", "æ‰“å·¥äºº", "å¹²é¥­äºº", "å°¾æ¬¾äºº", "å·¥å…·äºº", "å†¤ç§", "æ˜¾çœ¼åŒ…", "ç¤¾æ", "ç¤¾ç‰›", "ç¤¾æ­»",
    "emo", "ä½›ç³»", "å·ç‹", "æ‘†çƒ‚å¼", "æ‘¸é±¼å¼", "åˆ’æ°´å¼", "èººå¹³å¼", "æ•·è¡å¼", "ç³Šå¼„å­¦", "PUA",
    "CPU", "KTV", "yyds", "awsl", "ç»ç»å­", "YYDS", "ç»ç»å­", "æ “Q", "æ‹¿æäº†", "ç ´é˜²äº†"
]

# 2. æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼ˆæ–°å¢ï¼è¦†ç›–è‰²æƒ…ã€æš´åŠ›ã€æ¯’å“ã€æ”¿æ²»æ•æ„Ÿç­‰ï¼‰
SENSITIVE_KEYWORDS = {
    "è‰²æƒ…ç›¸å…³": [
        "è‰²æƒ…", "é»„è‰²", "è£¸èŠ", "æ€§äº¤æ˜“", "å«–å¨¼", "å–æ·«", "æ·«è¡", "è‰²æƒ…è§†é¢‘", "è‰²æƒ…å›¾ç‰‡", "AV",
        "ä¸‰çº§ç‰‡", "æ˜¥å®«", "è‰³ç…§", "éœ²éª¨", "æ€§è¡Œä¸º", "æ€§å™¨å®˜", "æ‰‹æ·«", "å«–å¨¼", "åŒ…å…»", "å°ä¸‰",
        "äºŒå¥¶", "æƒ…å¤«", "æƒ…å¦‡", "ä¸æ­£å½“å…³ç³»", "ä¸€å¤œæƒ…", "çº¦ç‚®", "æ€§æœåŠ¡", "è‰²æƒ…ç›´æ’­", "è‰²æƒ…å°è¯´"
    ],
    "æš´åŠ›ç›¸å…³": [
        "æ€äºº", "æŠ¢åŠ«", "å¼ºå¥¸", "ç»‘æ¶", "æ–—æ®´", "æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ€äººæ”¾ç«", "çˆ†ç‚¸", "æŠ•æ¯’",
        "å‡¶å™¨", "æªæ”¯", "å¼¹è¯", "ç®¡åˆ¶åˆ€å…·", "æš´åŠ›", "è¡€è…¥", "ææ€–", "è™æ€", "è™å¾…", "æ–½æš´",
        "æ®´æ‰“", "ç¾¤æ®´", "äº’æ®´", "å¯»è¡…æ»‹äº‹", "èšä¼—æ–—æ®´", "æ•…æ„ä¼¤å®³", "æ•…æ„æ€äºº", "æŠ¢åŠ«è´¢ç‰©"
    ],
    "æ¯’å“ç›¸å…³": [
        "æ¯’å“", "å¤§éº»", "æµ·æ´›å› ", "å†°æ¯’", "å¯å¡å› ", "æ‘‡å¤´ä¸¸", "Kç²‰", "é¸¦ç‰‡", "å—å•¡", "æœå†·ä¸",
        "å¸æ¯’", "è´©æ¯’", "åˆ¶æ¯’", "å¸æ¯’è€…", "æ¯’è´©", "æ¯’å“äº¤æ˜“", "æ¯’å“è¿è¾“", "æ¯’å“èµ°ç§"
    ],
    "æ”¿æ²»æ•æ„Ÿ": [
        "æ•æ„Ÿæ”¿æ²»äººç‰©", "æ”¿æ²»æ•æ„Ÿäº‹ä»¶", "é¢ è¦†", "åˆ†è£‚", "å›å›½", "æš´åŠ¨", "éªšä¹±", "éæ³•é›†ä¼š",
        "ååŠ¨", "åæ”¿åºœ", "åç¤¾ä¼š", "æç«¯ä¸»ä¹‰", "ææ€–ä¸»ä¹‰", "é‚ªæ•™", "æ³•è½®åŠŸ", "å°ç‹¬", "æ¸¯ç‹¬", "ç–†ç‹¬"
    ],
    "å…¶ä»–æ•æ„Ÿ": [
        "èµŒåš", "è¯ˆéª—", "ä¼ é”€", "éæ³•é›†èµ„", "æ´—é’±", "å·ç¨æ¼ç¨", "è´ªæ±¡è…è´¥", "è¡Œè´¿å—è´¿",
        "å‡å¸", "éæ³•äº¤æ˜“", "é»‘å®¢", "å…¥ä¾µ", "ç—…æ¯’", "ç›—å·", "è¯ˆéª—çŸ­ä¿¡", "è¯ˆéª—ç”µè¯"
    ]
}

# 3. å­¦æœ¯ç‰¹å¾ï¼ˆä¿ç•™ï¼Œæå‡æ•°æ®è´¨é‡ï¼‰
ACADEMIC_PATTERNS = [
    r"[A-Za-z0-9]=.*[A-Za-z0-9]",  # å…¬å¼
    r"å®šä¹‰[:ï¼š]", r"å®šç†", r"å…¬ç†", r"å‘½é¢˜", r"æ¨è®º", r"åŸç†", r"æ–¹æ³•", r"å®éªŒ", r"åˆ†æ", r"ç»“è®º",
]
ACADEMIC_REQUIRE = False

# å…¨å±€ç»Ÿè®¡å˜é‡ï¼ˆåˆ é™¤å­¦ç§‘è¿‡æ»¤ç›¸å…³ç»Ÿè®¡é¡¹ï¼‰
stats = {
    "total_input": 0,
    "sampled_count": 0,
    "preprocess_filtered": 0,
    "colloquial_filtered": 0,
    "non_academic_filtered": 0,
    "md5_duplicated": 0,
    "minhash_duplicated": 0,
    "sensitive_filtered": 0,  # æ–°å¢æ•æ„Ÿè¯é¢˜ç»Ÿè®¡
    "perplexity_filtered": 0,
    "final_kept": 0,
    "classic_chinese_kept": 0,
    "modern_chinese_kept": 0,
    "stage_time": {}
}

# ç›‘æ§çº¿ç¨‹å˜é‡
monitor_running = True
gpu_util = 0
cpu_mem = 0

# ========== å·¥å…·å‡½æ•° ==========
def get_gpu_utilization():
    try:
        result = os.popen("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits").read()
        return int(result.strip().split("\n")[0]) if result else 0
    except:
        return 0

def get_cpu_memory():
    process = psutil.Process(os.getpid())
    return round(process.memory_info().rss / (1024 * 1024), 2)

def monitor_thread():
    global monitor_running, gpu_util, cpu_mem
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼ˆæ¯30ç§’æ›´æ–°GPU/å†…å­˜çŠ¶æ€ï¼‰")
    while monitor_running:
        gpu_util = get_gpu_utilization()
        cpu_mem = get_cpu_memory()
        total_processed = stats["sampled_count"] - stats["preprocess_filtered"] - \
                          stats["colloquial_filtered"] - stats["non_academic_filtered"] - \
                          stats["md5_duplicated"] - stats["minhash_duplicated"] - stats["sensitive_filtered"]
        progress = (total_processed / stats["sampled_count"] * 100) if stats["sampled_count"] > 0 else 0.0
        logging.info(
            f"ğŸ“Š ç›‘æ§çŠ¶æ€ - GPUåˆ©ç”¨ç‡ï¼š{gpu_util}% | CPUå†…å­˜ï¼š{cpu_mem}MB | "
            f"æ€»è¾“å…¥ï¼š{stats['total_input']} | æŠ½æ ·åï¼š{stats['sampled_count']} | å·²å¤„ç†ï¼š{total_processed} | è¿›åº¦ï¼š{progress:.1f}%"
        )
        time.sleep(30)
    logging.info("ğŸ” ç›‘æ§çº¿ç¨‹åœæ­¢")

def load_jsonl_files_with_sampling(input_dir):
    jsonl_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(".jsonl")]
    if not jsonl_files:
        raise ValueError(f"âŒ è¾“å…¥ç›®å½• {input_dir} ä¸‹æ— JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
    logging.info(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½{'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    
    sampled_count = 0
    for file_idx, file in enumerate(jsonl_files):
        file_name = os.path.basename(file)
        logging.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶ {file_idx+1}/{len(jsonl_files)}ï¼š{file_name}")
        
        with open(file, "r", encoding="utf-8") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            for line_bytes in iter(mm.readline, b""):
                line = line_bytes.decode("utf-8").strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    text = data.get("text", "").strip()
                    stats["total_input"] += 1
                    
                    if SAMPLING_ENABLE:
                        if sampled_count >= MAX_SAMPLE_COUNT:
                            logging.info(f"âœ… æŠ½æ ·å®Œæˆï¼šå·²æŠ½å– {sampled_count} æ¡æ ·æœ¬ï¼ˆè¾¾åˆ°æœ€å¤§é™åˆ¶ï¼‰")
                            return
                        if np.random.random() > SAMPLE_RATIO:
                            continue
                        sampled_count += 1
                        stats["sampled_count"] = sampled_count
                    
                    yield {"text": text, "original_data": data, "source_file": file_name}
                except:
                    stats["preprocess_filtered"] += 1
                    continue
    logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ {'ï¼ˆæŠ½æ ·æ¨¡å¼ï¼‰' if SAMPLING_ENABLE else 'ï¼ˆå…¨é‡æ¨¡å¼ï¼‰'}")
    logging.info(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ï¼šæ€»è¾“å…¥ {stats['total_input']} æ¡ | æŠ½æ ·å {stats['sampled_count']} æ¡")

# ========== æ ¸å¿ƒç­›é€‰å·¥å…·å‡½æ•°ï¼ˆæŒ‰éœ€æ±‚è°ƒæ•´ï¼‰ ==========
def is_colloquial(text):
    """æ‰©å……å£è¯­æ£€æµ‹ï¼Œè¿‡æ»¤æ›´å¤šæ— å­¦æœ¯ä»·å€¼æ–‡æœ¬"""
    # å…³é”®è¯åŒ¹é…
    for word in COLLOQUIAL_WORDS:
        if word in text:
            return True
    # è¿ç»­æ ‡ç‚¹åŒ¹é…ï¼ˆ3ä¸ªä»¥ä¸Šï¼‰
    if re.search(r"[ï¼ï¼Ÿã€‚,ï¼Œï¼›;ï¼š:]{3,}", text):
        return True
    # å£è¯­åŒ–å¥å¼åŒ¹é…
    colloquial_patterns = [
        r"[æˆ‘ä½ ä»–å¥¹å®ƒ]ï¼ˆä»¬ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"[è¿™é‚£å“ª]ï¼ˆä¸ªäº›ï¼‰?[ä¹Ÿéƒ½è¿˜å°±æ‰åˆå†]?[ä¸æ²¡æ²¡ä»€ä¹ˆæ²¡ä»€ä¹ˆå¤§ä¸äº†]",
        r"^[å“ˆå“ˆå˜¿å˜¿å˜»å˜»å‘µå‘µ]+"
    ]
    if any(re.search(pattern, text) for pattern in colloquial_patterns):
        return True
    return False

def is_sensitive(text):
    """æ–°å¢æ•æ„Ÿè¯é¢˜æ£€æµ‹ï¼Œè¿‡æ»¤è¿è§„æ•°æ®"""
    # æ•æ„Ÿå…³é”®è¯åŒ¹é…
    for category, words in SENSITIVE_KEYWORDS.items():
        for word in words:
            if word in text:
                return True
    # æ•æ„Ÿå¥å¼åŒ¹é…
    sensitive_patterns = [
        r"å‡ºå”®.*(è‰²æƒ…|AV|ä¸‰çº§ç‰‡)",
        r"(å«–å¨¼|å–æ·«|æ€§äº¤æ˜“).*(ä»·æ ¼|è”ç³»æ–¹å¼|åœ°ç‚¹)",
        r"(æ€äºº|æŠ¢åŠ«|ç»‘æ¶).*(æ–¹æ³•|æ•™ç¨‹|å·¥å…·)",
        r"(æ¯’å“|å¤§éº»|å†°æ¯’).*(è´­ä¹°|å‡ºå”®|è¿è¾“)",
        r"(å°ç‹¬|æ¸¯ç‹¬|ç–†ç‹¬).*(æ”¯æŒ|å®£ä¼ |åˆ†è£‚)"
    ]
    if any(re.search(pattern, text, re.IGNORECASE) for pattern in sensitive_patterns):
        return True
    return False

def has_academic_features(text):
    """æ£€æµ‹å­¦æœ¯ç‰¹å¾ï¼ˆCLMMU/CEVALåå¥½ï¼‰"""
    return any(re.search(pattern, text) for pattern in ACADEMIC_PATTERNS)

# ========== 1. é¢„å¤„ç† + MD5å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆæ— ä¸­é—´æ–‡ä»¶ï¼Œè¿”å›å†…å­˜æ•°æ®ï¼‰ ==========
def preprocess_and_md5_deduplicate():
    start_time = time.time()
    logging.info("ğŸš€ å¼€å§‹é¢„å¤„ç† + MD5ç²¾ç¡®å»é‡ + åŸºç¡€ç­›é€‰ï¼ˆå«æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼‰")
    
    md5_set = set()
    processed_data = []  # å†…å­˜ä¸­å­˜å‚¨å¤„ç†åçš„æ•°æ®ï¼Œä¸å†™å…¥æ–‡ä»¶
    
    for item in tqdm(load_jsonl_files_with_sampling(INPUT_DIR), desc="é¢„å¤„ç†+ç­›é€‰", total=stats["sampled_count"] if SAMPLING_ENABLE else None):
        text = item["text"]
        original_data = item["original_data"]
        source_file = item["source_file"]
        
        # 1. åŸºç¡€é•¿åº¦è¿‡æ»¤
        if len(text) < MIN_CHAR_LEN or len(text) > MAX_CHAR_LEN:
            stats["preprocess_filtered"] += 1
            continue
        
        # 2. æ–‡æœ¬æ¸…æ´—
        text = re.sub(r"[\u200b\s]+", " ", text).strip()
        if not text:
            stats["preprocess_filtered"] += 1
            continue
        
        # 3. æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼ˆæ–°å¢ï¼ä¼˜å…ˆè¿‡æ»¤è¿è§„æ•°æ®ï¼‰
        if is_sensitive(text):
            stats["sensitive_filtered"] += 1
            continue
        
        # 4. å£è¯­åŒ–ç­›é€‰ï¼ˆæ‰©å……å…³é”®è¯ï¼‰
        if is_colloquial(text):
            stats["colloquial_filtered"] += 1
            continue
        
        # 5. å­¦æœ¯ç‰¹å¾ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        if ACADEMIC_REQUIRE and not has_academic_features(text):
            stats["non_academic_filtered"] += 1
            continue
        
        # 6. MD5ç²¾ç¡®å»é‡
        md5_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
        if md5_hash in md5_set:
            stats["md5_duplicated"] += 1
            continue
        md5_set.add(md5_hash)
        
        # å­˜å…¥å†…å­˜ï¼ˆä¸å†™å…¥æ–‡ä»¶ï¼‰
        processed_data.append({
            "text": text,
            "original_data": original_data,
            "source_file": source_file,
            "md5": md5_hash,
            "has_academic_features": has_academic_features(text)
        })
    
    stats["stage_time"]["preprocess_md5"] = round(time.time() - start_time, 2)
    remaining = len(processed_data)
    logging.info(
        f"âœ… é¢„å¤„ç†+ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['preprocess_md5']}ç§’ | "
        f"æŠ½æ ·åï¼š{stats['sampled_count']} | é•¿åº¦è¿‡æ»¤ï¼š{stats['preprocess_filtered']} | "
        f"æ•æ„Ÿè¯é¢˜è¿‡æ»¤ï¼š{stats['sensitive_filtered']} | å£è¯­åŒ–ï¼š{stats['colloquial_filtered']} | "
        f"æ— å­¦æœ¯ç‰¹å¾ï¼š{stats['non_academic_filtered']} | å®Œå…¨é‡å¤ï¼ˆMD5ï¼‰ï¼š{stats['md5_duplicated']} | å‰©ä½™ï¼š{remaining}"
    )
    return processed_data  # è¿”å›å†…å­˜æ•°æ®ï¼Œæ— ä¸­é—´æ–‡ä»¶

# ========== 2. Minhash LSHè¯­ä¹‰å»é‡ï¼ˆæ— ä¸­é—´æ–‡ä»¶ï¼Œæ¥æ”¶/è¿”å›å†…å­˜æ•°æ®ï¼‰ ==========
def create_minhash_signature(text, num_perm=MINHASH_NUM_PERM):
    """2-gram Tokenæ•æ‰ä¸­æ–‡è¯­ä¹‰"""
    minhash = MinHash(num_perm=num_perm)
    if len(text) < 2:
        grams = [text] if text else []
    else:
        grams = [text[i:i+2] for i in range(len(text)-1)]
    for gram in grams:
        token_hash = hashlib.sha256(gram.encode('utf-8')).hexdigest()
        minhash.update(token_hash.encode('utf-8'))
    return minhash

def minhash_lsh_deduplicate(processed_data):
    start_time = time.time()
    logging.info(f"ğŸš€ å¼€å§‹Minhash LSHè¯­ä¹‰å»é‡ï¼ˆé˜ˆå€¼{ LSH_THRESHOLD }ï¼‰")
    
    if not processed_data:
        logging.warning("âš ï¸ é¢„å¤„ç†åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
        return []
    
    # æå–æ–‡æœ¬å’Œæ•°æ®ï¼ˆå†…å­˜ä¸­æ“ä½œï¼‰
    texts = [item["text"] for item in processed_data]
    
    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)
    keep_indices = []
    duplicate_count = 0
    
    for i in tqdm(range(len(texts)), desc="MinHashå»é‡"):
        minhash = create_minhash_signature(texts[i])
        similar_docs = lsh.query(minhash)
        if not similar_docs:
            lsh.insert(str(i), minhash)
            keep_indices.append(i)
        else:
            duplicate_count += 1
    
    stats["minhash_duplicated"] = duplicate_count
    # ç­›é€‰å‡ºä¸é‡å¤çš„æ•°æ®ï¼ˆå†…å­˜ä¸­æ“ä½œï¼‰
    deduplicated_data = [processed_data[idx] for idx in keep_indices]
    
    stats["stage_time"]["minhash_lsh"] = round(time.time() - start_time, 2)
    remaining = len(deduplicated_data)
    logging.info(f"âœ… Minhash LSHè¯­ä¹‰å»é‡å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['minhash_lsh']}ç§’ | "
                 f"è¯­ä¹‰ç›¸ä¼¼é‡å¤ï¼š{stats['minhash_duplicated']} | å‰©ä½™ï¼š{remaining}")
    return deduplicated_data  # è¿”å›å†…å­˜æ•°æ®ï¼Œæ— ä¸­é—´æ–‡ä»¶

# ========== 3. å¤æ–‡æ£€æµ‹å‡½æ•°ï¼ˆæ‰©å……å…³é”®è¯ï¼Œæå‡è¯†åˆ«å‡†ç¡®ç‡ï¼‰ ==========
def is_classic_chinese(text):
    """æ‰©å……å¤æ–‡å…³é”®è¯ï¼Œç²¾å‡†åˆ¤å®šçœŸå®å¤æ–‡ï¼ˆé¿å…è¯¯åˆ¤ï¼‰"""
    # æ‰©å……å¤æ–‡æ ¸å¿ƒç‰¹å¾è¯ï¼ˆæ–°å¢30+ï¼Œè¦†ç›–è™šè¯ã€å®è¯ã€å†å²äººç‰©ã€å…¸ç±ï¼‰
    classic_words = [
        # è™šè¯ï¼ˆæ ¸å¿ƒï¼‰
        'ä¹‹', 'ä¹', 'è€…', 'ä¹Ÿ', 'æ›°', 'å¾', 'æ±', 'å°”', 'ä¹ƒ', 'å…®', 'çŸ£', 'å“‰', 'è€¶', 'æ¬¤', 'ç„‰', 'ä¹', 'å…¶', 'è€Œ', 'ä»¥', 'äº',
        # ä»£è¯/åè¯
        'å¤«', 'ç›–', 'åˆ™', 'ä¸”', 'è‹¥', 'ä½•', 'å­°', 'å®‰', 'å­°ä¸', 'æ‰€ä»¥', 'æ‰€', 'å¯', 'èƒ½', 'å¿…', 'å½“', 'åº”',
        # å†å²æœä»£/äººç‰©
        'å¤', 'å•†', 'å‘¨', 'ç§¦', 'æ±‰', 'é­', 'èœ€', 'å´', 'æ™‹', 'éš‹', 'å”', 'å®‹', 'å…ƒ', 'æ˜', 'æ¸…',
        'é»„å¸', 'ç‚å¸', 'å°§', 'èˆœ', 'ç¦¹', 'æ±¤', 'æ–‡ç‹', 'æ­¦ç‹', 'å‘¨å…¬', 'å­”å­', 'å­Ÿå­', 'è€å­', 'åº„å­', 'å¢¨å­', 'è€å­', 'éŸ©éå­',
        # ç»å…¸å…¸ç±
        'ã€Šè®ºè¯­ã€‹', 'ã€Šå­Ÿå­ã€‹', 'ã€Šå¤§å­¦ã€‹', 'ã€Šä¸­åº¸ã€‹', 'ã€Šè¯—ç»ã€‹', 'ã€Šå°šä¹¦ã€‹', 'ã€Šç¤¼è®°ã€‹', 'ã€Šå‘¨æ˜“ã€‹', 'ã€Šæ˜¥ç§‹ã€‹',
        'ã€Šé“å¾·ç»ã€‹', 'ã€Šåº„å­ã€‹', 'ã€Šå¢¨å­ã€‹', 'ã€Šè€å­ã€‹', 'ã€ŠéŸ©éå­ã€‹', 'ã€Šå²è®°ã€‹', 'ã€Šæ±‰ä¹¦ã€‹', 'ã€Šåæ±‰ä¹¦ã€‹', 'ã€Šä¸‰å›½å¿—ã€‹',
        # å¤æ–‡å¥å¼ç‰¹å¾è¯
        'å‘œå‘¼', 'å—Ÿå¤«', 'ç›–é—»', 'çªƒä»¥ä¸º', 'è‡£é—»', 'åœ£ç‹', 'è´¤å›', 'å¿ è‡£', 'ä¹‰å£«', 'å­å­', 'çƒˆå¥³'
    ]
    # ç°ä»£æ–‡å¼ºç‰¹å¾è¯ï¼ˆå¿«é€Ÿæ’é™¤ï¼‰
    modern_words = ["æ‰‹æœº", "äº’è”ç½‘", "ç”µè„‘", "å¾®ä¿¡", "æ”¯ä»˜å®", "å¿«é€’", "é«˜é“", "ç©ºè°ƒ", "ç”µè§†", "ç½‘ç»œ", "APP",
                   "å¾®åš", "æŠ–éŸ³", "å¿«æ‰‹", "ç›´æ’­", "ç”µå•†", "ç½‘è´­", "å¤–å–", "æ‰“è½¦", "å…±äº«å•è½¦", "5G", "WiFi"]
    
    # å«ç°ä»£è¯ç›´æ¥åˆ¤å®šä¸ºç°ä»£æ–‡
    for word in modern_words:
        if word in text:
            return False
    
    total_chars = len(text)
    if total_chars == 0:
        return False
    
    # å¤æ–‡ç‰¹å¾è¯å¯†åº¦é˜ˆå€¼ï¼ˆä¿æŒ2%ï¼Œç¡®ä¿ç²¾å‡†åº¦ï¼‰
    classic_char_count = 0
    for word in classic_words:
        classic_char_count += text.count(word)
    density_threshold = 0.02
    
    # æ‰©å……å¤æ–‡å¥å¼åŒ¹é…
    classic_patterns = [
        r'^[\u4e00-\u9fff]{1,5}æ›°', r'^æ˜”è€…', r'^åˆ', r'^å½“æ˜¯æ—¶', r'^äºæ˜¯', r'^å‘œå‘¼', r'^å—Ÿå¤«',
        r'^ç›–é—»', r'^çªƒä»¥ä¸º', r'^è‡£é—»', r'^åœ£ç‹', r'^è´¤å›', r'^å¿ è‡£', r'^ä¹‰å£«'
    ]
    pattern_match = any(re.match(pattern, text) for pattern in classic_patterns)
    
    # åˆ¤å®šé€»è¾‘ï¼šå¯†åº¦è¾¾æ ‡ æˆ– å¥å¼åŒ¹é…
    is_classic = (classic_char_count / total_chars > density_threshold) or pattern_match
    return is_classic

# ========== 4. å›°æƒ‘åº¦åˆ†æä¸ç­›é€‰ï¼ˆæ— ä¸­é—´æ–‡ä»¶ï¼Œå…¨ç¨‹å†…å­˜æ“ä½œï¼‰ ==========
def load_perplexity_model():
    logging.info("ğŸ“¥ åŠ è½½æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦ï¼ˆç”¨äºè¯„ä¼°æ–‡æœ¬è´¨é‡ï¼‰")
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)
    model.eval()
    logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
    return tokenizer, model

def calculate_perplexity_batch_optimized(texts, tokenizer, model):
    """æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¼˜åŒ–ç»´åº¦åŒ¹é…ï¼‰"""
    try:
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=MAX_SEQ_LENGTH, 
            return_tensors="pt"
        ).to(DEVICE)
        
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        batch_size = input_ids.shape[0]
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            perplexities = []
            for i in range(batch_size):
                valid_mask = (input_ids[i] != tokenizer.pad_token_id)
                valid_token_ids = input_ids[i][valid_mask]
                
                if len(valid_token_ids) <= 1:
                    perplexities.append(float('inf'))
                    continue
                
                # ä¿®æ­£ç»´åº¦åŒ¹é…é€»è¾‘
                shift_logits = logits[i, :-1, :].contiguous()
                shift_labels = valid_token_ids[1:].contiguous()
                valid_len = len(shift_labels)
                shift_logits = shift_logits[:valid_len, :]
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)), 
                    shift_labels.view(-1)
                )
                
                perplexity = torch.exp(loss).item()
                perplexities.append(perplexity)
            
            return perplexities
            
    except RuntimeError as e:
        if "out of memory" in str(e):
            logging.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°")
            half_size = len(texts) // 2
            if half_size >= 1:
                perplexities1 = calculate_perplexity_batch_optimized(texts[:half_size], tokenizer, model)
                perplexities2 = calculate_perplexity_batch_optimized(texts[half_size:], tokenizer, model)
                return perplexities1 + perplexities2
            else:
                return [calculate_perplexity(text, tokenizer, model) for text in texts]
        else:
            raise e
    except Exception as e:
        logging.warning(f"æ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°é€æ¡è®¡ç®—: {str(e)}")
        return [calculate_perplexity(text, tokenizer, model) for text in texts]

def calculate_perplexity(text, tokenizer, model):
    """å•æ–‡æœ¬å›°æƒ‘åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["input_ids"][..., 1:].contiguous()
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            avg_loss = loss.mean()
            return torch.exp(avg_loss).item()
    except:
        return float('inf')

def analyze_perplexity_distribution_layered(deduplicated_data, sample_size=1000):
    """åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç²¾å‡†åˆ†ä½æ•°ï¼Œæ¥æ”¶å†…å­˜æ•°æ®ï¼‰"""
    logging.info("ğŸ” å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦åˆ†å¸ƒåˆ†æï¼ˆç°ä»£æ–‡=ä½å›°æƒ‘åº¦ä¼˜è´¨ï¼Œå¤æ–‡=é«˜å›°æƒ‘åº¦çœŸå®ï¼‰")
    tokenizer, model = load_perplexity_model()
    
    # ä»å†…å­˜æ•°æ®ä¸­æå–æ–‡æœ¬ï¼ˆæ— æ–‡ä»¶è¯»å–ï¼‰
    texts = [item["text"] for item in deduplicated_data if len(item["text"]) >= MIN_CHAR_LEN]
    
    if not texts:
        logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®ç”¨äºå›°æƒ‘åº¦åˆ†æ")
        return 100.0, 30.0, None, None
    
    # æŠ½æ ·åˆ†æï¼ˆé¿å…æ•°æ®é‡è¿‡å¤§ï¼‰
    if len(texts) > sample_size:
        import random
        texts = random.sample(texts, sample_size)
    
    logging.info(f"ğŸ“Š å°†åˆ†æ {len(texts)} æ¡æ–‡æœ¬çš„å›°æƒ‘åº¦åˆ†å¸ƒ")
    
    # åˆ†ç±»å¤æ–‡/ç°ä»£æ–‡ï¼ˆæ‰©å……å…³é”®è¯åæ›´ç²¾å‡†ï¼‰
    classic_texts = []
    modern_texts = []
    for text in texts:
        if is_classic_chinese(text):
            classic_texts.append(text)
        else:
            modern_texts.append(text)
    
    logging.info(f"ğŸ“Š æ–‡æœ¬åˆ†ç±»ç»“æœ: å¤æ–‡ {len(classic_texts)} æ¡, ç°ä»£æ–‡ {len(modern_texts)} æ¡")
    
    # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
    modern_perplexities = []
    for i in tqdm(range(0, len(modern_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—ç°ä»£æ–‡å›°æƒ‘åº¦"):
        batch = modern_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        modern_perplexities.extend(batch_perplexities)
    
    classic_perplexities = []
    for i in tqdm(range(0, len(classic_texts), BATCH_SIZE_PERPLEXITY), desc="è®¡ç®—å¤æ–‡å›°æƒ‘åº¦"):
        batch = classic_texts[i:i+BATCH_SIZE_PERPLEXITY]
        batch_perplexities = calculate_perplexity_batch_optimized(batch, tokenizer, model)
        classic_perplexities.extend(batch_perplexities)
    
    # å¤„ç†æœ‰æ•ˆæ•°æ®ï¼ˆè¿‡æ»¤å¼‚å¸¸å€¼ï¼‰
    modern_perplexities = np.array(modern_perplexities)
    classic_perplexities = np.array(classic_perplexities)
    
    modern_valid = modern_perplexities[modern_perplexities < 15000]
    classic_valid = classic_perplexities[classic_perplexities < 15000]
    
    if len(modern_valid) == 0:
        logging.warning("âš ï¸ ç°ä»£æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        modern_valid = np.array([100.0])
    
    if len(classic_valid) == 0:
        logging.warning("âš ï¸ å¤æ–‡å›°æƒ‘åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        classic_valid = np.array([30.0])
    
    # å®šä¹‰åˆ†ä½æ•°ï¼ˆæ˜ç¡®ç´¢å¼•å¯¹åº”å…³ç³»ï¼‰
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_percentiles = np.percentile(modern_valid, percentiles)
    classic_percentiles = np.percentile(classic_valid, percentiles)
    
    # æ ¸å¿ƒé˜ˆå€¼é…ç½®ï¼ˆç¡®ä¿ä¿ç•™ç‡50%ï¼‰
    modern_threshold_idx = 6  # 75%åˆ†ä½æ•°ï¼ˆè¿‡æ»¤é«˜å›°æƒ‘åº¦ç°ä»£æ–‡ï¼‰
    classic_threshold_idx = 1  # 10%åˆ†ä½æ•°ï¼ˆä¿ç•™é«˜å›°æƒ‘åº¦å¤æ–‡ï¼‰
    
    modern_threshold = modern_percentiles[modern_threshold_idx]
    classic_threshold = classic_percentiles[classic_threshold_idx]
    
    # æ—¥å¿—è¾“å‡ºï¼ˆæ¸…æ™°æ˜“æ‡‚ï¼‰
    logging.info("ğŸ“ˆ ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆä½å›°æƒ‘åº¦=è´¨é‡é«˜ã€æ¨¡å‹æ˜“ç†è§£ï¼‰:")
    for p, val in zip(percentiles, modern_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ ç°ä»£æ–‡é˜ˆå€¼: â‰¤{modern_threshold:.2f} ({percentiles[modern_threshold_idx]}%åˆ†ä½æ•°)")
    
    logging.info("ğŸ“ˆ å¤æ–‡å›°æƒ‘åº¦åˆ†å¸ƒï¼ˆé«˜å›°æƒ‘åº¦=æ›´çœŸå®ã€éç°ä»£æ”¹å†™ï¼‰:")
    for p, val in zip(percentiles, classic_percentiles):
        logging.info(f"    {p}% åˆ†ä½æ•°: {val:.2f}")
    logging.info(f"ğŸ¯ å¤æ–‡é˜ˆå€¼: â‰¥{classic_threshold:.2f} ({percentiles[classic_threshold_idx]}%åˆ†ä½æ•°)")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾ï¼ˆä¿ç•™ï¼Œç”¨æˆ·éœ€æ±‚ï¼‰
    try:
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(12, 8))
        plt.hist(modern_valid, bins=50, alpha=0.7, label='Modern Chinese (Low Perplexity = High Quality)', color='skyblue', edgecolor='black')
        plt.hist(classic_valid, bins=50, alpha=0.7, label='Classic Chinese (High Perplexity = Authentic)', color='salmon', edgecolor='black')
        plt.axvline(modern_threshold, color='blue', linestyle='--', linewidth=2, label=f'Modern â‰¤ {modern_threshold:.2f} ({percentiles[modern_threshold_idx]}%tile)')
        plt.axvline(classic_threshold, color='red', linestyle='--', linewidth=2, label=f'Classic â‰¥ {classic_threshold:.2f} ({percentiles[classic_threshold_idx]}%tile)')
        plt.xlabel('Perplexity', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.title('Layered Perplexity Distribution (CLMMU/CEVAL Metric Priority)', fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        output_path = os.path.join(OUTPUT_PATH, 'layered_perplexity_distribution_final.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"ğŸ“Š åˆ†å±‚åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
        
    except Exception as e:
        logging.warning(f"âš ï¸ ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")
    
    return modern_threshold, classic_threshold, modern_percentiles, classic_percentiles

def determine_layered_thresholds(deduplicated_data):
    """ç¡®å®šåˆ†å±‚é˜ˆå€¼ï¼ˆæ¥æ”¶å†…å­˜æ•°æ®ï¼‰"""
    logging.info("ğŸ¯ å¼€å§‹ç¡®å®šåˆ†å±‚å›°æƒ‘åº¦é˜ˆå€¼")
    
    try:
        modern_threshold, classic_threshold, modern_percentiles, classic_percentiles = analyze_perplexity_distribution_layered(deduplicated_data, sample_size=500)
        
        # æ˜ç¡®è¾“å‡ºé˜ˆå€¼å’Œå¯¹åº”åˆ†ä½æ•°
        percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
        modern_p = percentiles[6]  # 75%åˆ†ä½æ•°
        classic_p = percentiles[1]  # 10%åˆ†ä½æ•°
        
        logging.info(f"ğŸ¤– ç°ä»£æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)")
        logging.info(f"ğŸ“œ å¤æ–‡æœ€ç»ˆé˜ˆå€¼: â‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)")
        
        return modern_threshold, classic_threshold
        
    except Exception as e:
        logging.error(f"âŒ ç¡®å®šåˆ†å±‚é˜ˆå€¼å¤±è´¥: {str(e)}")
        logging.info("ğŸ”„ ä½¿ç”¨é»˜è®¤é˜ˆå€¼: ç°ä»£æ–‡=80 (75%åˆ†ä½æ•°), å¤æ–‡=30 (10%åˆ†ä½æ•°)")
        return 80.0, 30.0

def layered_perplexity_filter(deduplicated_data, modern_threshold, classic_threshold):
    """åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆä»…ç”Ÿæˆæœ€ç»ˆé«˜è´¨é‡æ•°æ®æ–‡ä»¶ï¼‰"""
    start_time = time.time()
    
    # æ˜ç¡®é˜ˆå€¼å’Œè§£é‡Š
    percentiles = [0, 10, 20, 25, 30, 50, 75, 90, 95, 100]
    modern_p = percentiles[6]
    classic_p = percentiles[1]
    
    logging.info(f"ğŸš€ å¼€å§‹åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰")
    logging.info(f"   - ç°ä»£æ–‡ï¼šâ‰¤{modern_threshold:.2f} ({modern_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™ä½å›°æƒ‘åº¦ä¼˜è´¨æ•°æ®")
    logging.info(f"   - å¤æ–‡ï¼šâ‰¥{classic_threshold:.2f} ({classic_p}%åˆ†ä½æ•°)ï¼Œä¿ç•™é«˜å›°æƒ‘åº¦çœŸå®å¤æ–‡")
    
    tokenizer, model = load_perplexity_model()
    
    # ä»…ç”Ÿæˆæœ€ç»ˆé«˜è´¨é‡æ•°æ®æ–‡ä»¶ï¼ˆæ— å…¶ä»–æ–‡ä»¶ï¼‰
    kept_file = os.path.join(OUTPUT_PATH, "clmmu_kept_data_final.jsonl")
    
    batch_texts = []
    batch_data = []
    
    with open(kept_file, "w", encoding="utf-8") as f_kept:
        # éå†å†…å­˜ä¸­çš„å»é‡æ•°æ®ï¼ˆæ— æ–‡ä»¶è¯»å–ï¼‰
        for data in tqdm(deduplicated_data, desc="åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰", total=len(deduplicated_data)):
            try:
                batch_texts.append(data["text"])
                batch_data.append(data)
            except:
                continue
            
            if len(batch_texts) >= BATCH_SIZE_PERPLEXITY:
                perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
                
                for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                    data_item["perplexity"] = round(perp, 2)
                    data_item["is_classic"] = is_classic_chinese(text)
                    
                    # ç­›é€‰é€»è¾‘ï¼ˆåªä¿ç•™ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼‰
                    if data_item["is_classic"]:
                        # å¤æ–‡ï¼šä¿ç•™â‰¥10%åˆ†ä½æ•°çš„é«˜å›°æƒ‘åº¦æ•°æ®ï¼Œæ’é™¤å¼‚å¸¸å€¼
                        if perp >= classic_threshold and perp < 10000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "classic_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["classic_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                    else:
                        # ç°ä»£æ–‡ï¼šä¿ç•™â‰¤75%åˆ†ä½æ•°çš„ä½å›°æƒ‘åº¦æ•°æ®ï¼Œæ’é™¤å¼‚å¸¸å€¼
                        if perp <= modern_threshold and perp < 5000:
                            final_data = data_item["original_data"].copy()
                            final_data["cleaned_text"] = text
                            final_data["md5"] = data_item["md5"]
                            final_data["perplexity"] = data_item["perplexity"]
                            final_data["source_file"] = data_item["source_file"]
                            final_data["text_type"] = "modern_chinese"
                            final_data["has_academic_features"] = data_item.get("has_academic_features", False)
                            f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                            stats["final_kept"] += 1
                            stats["modern_chinese_kept"] += 1
                        else:
                            stats["perplexity_filtered"] += 1
                
                batch_texts = []
                batch_data = []
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if batch_texts:
            perplexities = calculate_perplexity_batch_optimized(batch_texts, tokenizer, model)
            for text, data_item, perp in zip(batch_texts, batch_data, perplexities):
                data_item["perplexity"] = round(perp, 2)
                data_item["is_classic"] = is_classic_chinese(text)
                
                if data_item["is_classic"]:
                    if perp >= classic_threshold and perp < 10000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "classic_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["classic_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
                else:
                    if perp <= modern_threshold and perp < 5000:
                        final_data = data_item["original_data"].copy()
                        final_data["cleaned_text"] = text
                        final_data["md5"] = data_item["md5"]
                        final_data["perplexity"] = data_item["perplexity"]
                        final_data["source_file"] = data_item["source_file"]
                        final_data["text_type"] = "modern_chinese"
                        f_kept.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                        stats["final_kept"] += 1
                        stats["modern_chinese_kept"] += 1
                    else:
                        stats["perplexity_filtered"] += 1
    
    stats["stage_time"]["perplexity"] = round(time.time() - start_time, 2)
    logging.info(f"âœ… åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰å®Œæˆ | è€—æ—¶ï¼š{stats['stage_time']['perplexity']}ç§’")
    logging.info(f"ğŸ“Š åˆ†å±‚ç»Ÿè®¡ - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']} | å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}")
    logging.info(f"ğŸ“Š æ€»è®¡ä¿ç•™: {stats['final_kept']} | è¿‡æ»¤: {stats['perplexity_filtered']}")
    
    return kept_file  # ä»…è¿”å›æœ€ç»ˆæ–‡ä»¶è·¯å¾„

# ========== ä¸»å‡½æ•°ï¼ˆå…¨ç¨‹å†…å­˜æµè½¬ï¼Œæ— ä¸­é—´æ–‡ä»¶ï¼‰ ==========
def main():
    global monitor_running
    start_time = time.time()
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(OUTPUT_PATH, "clmmu_filter_log_final.log"), encoding="utf-8"),
            logging.StreamHandler()
        ]
    )
    
    logging.info("ğŸ‰ å¯åŠ¨CLMMU/CEVALç­›é€‰æµç¨‹ï¼ˆæ— ä¸­é—´æ–‡ä»¶ç‰ˆï¼‰")
    logging.info("ğŸ“‹ æ ¸å¿ƒé…ç½®ï¼ˆæŒ‰éœ€æ±‚è°ƒæ•´ï¼‰:")
    logging.info(f"   - è¯­ä¹‰å»é‡é˜ˆå€¼: {LSH_THRESHOLD}ï¼ˆå‡å°‘å†—ä½™ï¼‰")
    logging.info(f"   - ç°ä»£æ–‡å›°æƒ‘åº¦: â‰¤75%åˆ†ä½æ•°ï¼ˆä½å›°æƒ‘åº¦=é«˜è´¨é‡ï¼‰")
    logging.info(f"   - å¤æ–‡å›°æƒ‘åº¦: â‰¥10%åˆ†ä½æ•°ï¼ˆé«˜å›°æƒ‘åº¦=çœŸå®å¤æ–‡ï¼‰")
    logging.info(f"   - å£è¯­å…³é”®è¯: æ‰©å……è‡³{len(COLLOQUIAL_WORDS)}ä¸ªï¼ˆå¼ºåŒ–å­¦æœ¯è¿‡æ»¤ï¼‰")
    logging.info(f"   - å¤æ–‡å…³é”®è¯: æ‰©å……è‡³{len([w for w in is_classic_chinese.__code__.co_consts if isinstance(w, str) and len(w) <= 10])}ä¸ªï¼ˆç²¾å‡†è¯†åˆ«ï¼‰")
    logging.info(f"   - æ•æ„Ÿè¯é¢˜è¿‡æ»¤: å¯ç”¨ï¼ˆè¦†ç›–è‰²æƒ…ã€æš´åŠ›ã€æ¯’å“ç­‰ï¼‰")
    logging.info(f"   - æ•°æ®æµè½¬: å…¨ç¨‹å†…å­˜æ“ä½œï¼Œä¸ç”Ÿæˆä»»ä½•ä¸­é—´æ–‡ä»¶")
    logging.info(f"   - æœ€ç»ˆè¾“å‡º: ä»…3ç±»æ–‡ä»¶ï¼ˆé«˜è´¨é‡æ•°æ®ã€å›°æƒ‘åº¦åˆ†å¸ƒå›¾ã€ç­›é€‰æ—¥å¿—ï¼‰")
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor = threading.Thread(target=monitor_thread, daemon=True)
    monitor.start()
    
    try:
        # 1. é¢„å¤„ç†+MD5å»é‡+åŸºç¡€ç­›é€‰ï¼ˆè¿”å›å†…å­˜æ•°æ®ï¼Œæ— ä¸­é—´æ–‡ä»¶ï¼‰
        processed_data = preprocess_and_md5_deduplicate()
        if not processed_data:
            logging.warning("âš ï¸ é¢„å¤„ç†åæ— æœ‰æ•ˆæ•°æ®ï¼Œæµç¨‹ç»ˆæ­¢")
            return
        
        # 2. Minhash LSHè¯­ä¹‰å»é‡ï¼ˆæ¥æ”¶/è¿”å›å†…å­˜æ•°æ®ï¼Œæ— ä¸­é—´æ–‡ä»¶ï¼‰
        deduplicated_data = minhash_lsh_deduplicate(processed_data)
        if not deduplicated_data:
            logging.warning("âš ï¸ è¯­ä¹‰å»é‡åæ— æœ‰æ•ˆæ•°æ®ï¼Œæµç¨‹ç»ˆæ­¢")
            return
        
        # 3. ç¡®å®šåˆ†å±‚é˜ˆå€¼ï¼ˆæ¥æ”¶å†…å­˜æ•°æ®ï¼‰
        modern_threshold, classic_threshold = determine_layered_thresholds(deduplicated_data)
        
        # 4. åˆ†å±‚å›°æƒ‘åº¦ç­›é€‰ï¼ˆç”Ÿæˆæœ€ç»ˆæ–‡ä»¶ï¼Œæ— å…¶ä»–æ–‡ä»¶ï¼‰
        kept_file = layered_perplexity_filter(deduplicated_data, modern_threshold, classic_threshold)
        
        # åœæ­¢ç›‘æ§
        monitor_running = False
        monitor.join()
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        total_time = round(time.time() - start_time, 2)
        logging.info("\n" + "="*80)
        logging.info("ğŸ“Š CLMMU/CEVALç­›é€‰æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Šï¼ˆæ— ä¸­é—´æ–‡ä»¶ç‰ˆï¼‰")
        logging.info("="*80)
        logging.info(f"æ€»è¾“å…¥æ•°æ®: {stats['total_input']}æ¡")
        logging.info(f"æŠ½æ ·åæ•°æ®: {stats['sampled_count']}æ¡")
        logging.info(f"ğŸ“Œ å„é˜¶æ®µè¿‡æ»¤ç»Ÿè®¡:")
        logging.info(f"   - é•¿åº¦è¿‡æ»¤: {stats['preprocess_filtered']}æ¡")
        logging.info(f"   - æ•æ„Ÿè¯é¢˜è¿‡æ»¤: {stats['sensitive_filtered']}æ¡")
        logging.info(f"   - å£è¯­åŒ–: {stats['colloquial_filtered']}æ¡")
        logging.info(f"   - æ— å­¦æœ¯ç‰¹å¾: {stats['non_academic_filtered']}æ¡")
        logging.info(f"   - MD5å®Œå…¨é‡å¤: {stats['md5_duplicated']}æ¡")
        logging.info(f"   - è¯­ä¹‰ç›¸ä¼¼é‡å¤: {stats['minhash_duplicated']}æ¡")
        logging.info(f"   - å›°æƒ‘åº¦è¿‡æ»¤: {stats['perplexity_filtered']}æ¡")
        logging.info(f"ğŸ“Œ æœ€ç»ˆä¿ç•™ç»Ÿè®¡:")
        logging.info(f"   - ç°ä»£æ–‡ä¿ç•™: {stats['modern_chinese_kept']}æ¡")
        logging.info(f"   - å¤æ–‡ä¿ç•™: {stats['classic_chinese_kept']}æ¡")
        logging.info(f"   - æ€»è®¡ä¿ç•™: {stats['final_kept']}æ¡")
        logging.info(f"   - ä¿ç•™æ¯”ä¾‹: {stats['final_kept']/stats['sampled_count']*100:.1f}%")
        logging.info(f"ğŸ“Œ é˜ˆå€¼é…ç½®:")
        logging.info(f"   - ç°ä»£æ–‡å›°æƒ‘åº¦: â‰¤{modern_threshold:.2f} (75%åˆ†ä½æ•°ï¼Œä½å›°æƒ‘åº¦ä¼˜è´¨)")
        logging.info(f"   - å¤æ–‡å›°æƒ‘åº¦: â‰¥{classic_threshold:.2f} (10%åˆ†ä½æ•°ï¼Œé«˜å›°æƒ‘åº¦çœŸå®)")
        logging.info(f"   - è¯­ä¹‰å»é‡: {LSH_THRESHOLD}")
        logging.info(f"ğŸ“Œ æ€§èƒ½ç»Ÿè®¡:")
        logging.info(f"   - æ€»è€—æ—¶: {total_time}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logging.info("\nğŸ“ æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆä»…3ç±»ï¼‰:")
        logging.info(f"âœ… é«˜è´¨é‡æ•°æ®ï¼ˆæœ€ç»ˆç‰ˆï¼‰: {kept_file}")
        logging.info(f"ğŸ“Š å›°æƒ‘åº¦åˆ†å¸ƒå›¾: {os.path.join(OUTPUT_PATH, 'layered_perplexity_distribution_final.png')}")
        logging.info(f"ğŸ“‹ ç­›é€‰æ—¥å¿—: {os.path.join(OUTPUT_PATH, 'clmmu_filter_log_final.log')}")
        logging.info("="*80)
        
        # ä¿ç•™æ¯”ä¾‹æ ¡å‡†æç¤º
        retention_ratio = stats['final_kept']/stats['sampled_count']*100
        if retention_ratio < 45:
            logging.warning(f"âš ï¸ ä¿ç•™æ¯”ä¾‹è¿‡ä½ï¼ˆ{retention_ratio:.1f}%ï¼‰ï¼Œå»ºè®®é€‚åº¦æ”¾æ¾ï¼š")
            logging.warning(f"   1. è¯­ä¹‰å»é‡é˜ˆå€¼ä»0.76â†’0.73")
            logging.warning(f"   2. ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†ä½æ•°ä»75%â†’80%")
        elif retention_ratio > 55:
            logging.warning(f"âš ï¸ ä¿ç•™æ¯”ä¾‹è¿‡é«˜ï¼ˆ{retention_ratio:.1f}%ï¼‰ï¼Œå»ºè®®é€‚åº¦æ”¶ç´§ï¼š")
            logging.warning(f"   1. è¯­ä¹‰å»é‡é˜ˆå€¼ä»0.76â†’0.78")
            logging.warning(f"   2. ç°ä»£æ–‡å›°æƒ‘åº¦åˆ†ä½æ•°ä»75%â†’70%")
        else:
            logging.info("âœ… ä¿ç•™æ¯”ä¾‹è¾¾æ ‡ï¼ˆ45%-55%ï¼‰ï¼Œæ•°æ®è´¨é‡ä¸æ•°é‡å¹³è¡¡è‰¯å¥½ï¼")
        
    except Exception as e:
        logging.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
    finally:
        # ç¡®ä¿ç›‘æ§çº¿ç¨‹åœæ­¢
        monitor_running = False
        monitor.join(timeout=5)
        
        logging.info("ğŸ”š CLMMU/CEVALç­›é€‰æµç¨‹ç»“æŸï¼ˆæ— ä¸­é—´æ–‡ä»¶ç‰ˆï¼‰")

if __name__ == "__main__":
    main()