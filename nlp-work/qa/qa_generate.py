import os
import json
import logging
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from qa.qa_config import (
    REWRITTEN_INPUT_PATH, QA_OUTPUT_PATH, QA_FAILED_PATH, QA_LOG_PATH,
    QA_MODEL_ID, MAX_NEW_TOKENS_QA, TEMPERATURE_QA, TOP_P_QA, BATCH_SIZE_QA,
    MAX_SOURCE_CHARS, MIN_QUESTION_LEN, MAX_QUESTION_LEN, MIN_ANSWER_LEN,
    MAX_ANSWER_LEN, REQUIRED_CHINESE_PUNCT, SEMANTIC_SIMILARITY_MIN,
    TYPE_KEYWORDS, TYPE_PROMPT_MAPPING
)
from qa.prompt_templates import QA_PROMPTS

# å¤ç”¨å·²æœ‰ embedding æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from rewrite.model_utils import text_to_embedding
    _HAS_EMB = True
except Exception:
    _HAS_EMB = False

# ========== æ¨¡å‹åŠ è½½ ==========
def load_qa_model():
    logging.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½QAç”Ÿæˆæ¨¡å‹: {QA_MODEL_ID}")
    tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        QA_MODEL_ID,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        load_in_4bit=True
    )
    model.eval()
    logging.info("âœ… QAæ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer

# ========== é—®é¢˜ç±»å‹çŒœæµ‹ ==========
def detect_question_type(source_text: str) -> str:
    lower_text = source_text.lower()
    for q_type, kws in TYPE_KEYWORDS.items():
        for kw in kws:
            if kw in source_text or kw in lower_text:
                return q_type
    # ç®€å•å…¬å¼/æ•°å­—åˆ¤æ–­å½’å…¥ calculate
    if any(ch.isdigit() for ch in source_text) and any(sym in source_text for sym in ["=", "+", "-", "â†’", "%"]):
        return "calculate"
    return "fallback"

# ========== è¯­ä¹‰ç›¸å…³åº¦ ==========
def semantic_similarity(a: str, b: str) -> float:
    if not _HAS_EMB:
        return 1.0  # æ²¡æœ‰åµŒå…¥å‡½æ•°æ—¶ç›´æ¥æ”¾è¡Œ
    try:
        emb1 = text_to_embedding(a)
        emb2 = text_to_embedding(b)
        import numpy as np
        sim = float((emb1 @ emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))
        return sim
    except Exception:
        return 0.0

# ========== ç”Ÿæˆå•ä¸ªQA ==========
def generate_single_qa(model, tokenizer, prompt: str) -> str:
    try:
        messages = [{"role": "user", "content": prompt}]
        if hasattr(tokenizer, "apply_chat_template"):
            input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
        else:
            input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)["input_ids"]
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                max_new_tokens=MAX_NEW_TOKENS_QA,
                temperature=TEMPERATURE_QA,
                top_p=TOP_P_QA,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        gen = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True).strip()
        return gen
    except Exception as e:
        logging.warning(f"âš ï¸ ç”Ÿæˆå¤±è´¥: {e}")
        return ""

# ========== è§£æè¾“å‡º ==========
def parse_generated(text: str):
    # å¯»æ‰¾â€œé—®é¢˜ï¼šâ€å’Œâ€œç­”æ¡ˆï¼šâ€åˆ†éš”
    q_marker, a_marker = "é—®é¢˜ï¼š", "ç­”æ¡ˆï¼š"
    q_idx = text.find(q_marker)
    a_idx = text.find(a_marker)
    if q_idx == -1 or a_idx == -1 or a_idx <= q_idx:
        return None, None
    question = text[q_idx + len(q_marker):a_idx].strip().strip("\n")
    answer = text[a_idx + len(a_marker):].strip()
    return question, answer

# ========== è´¨é‡æ ¡éªŒ ==========
def validate_pair(source_text, question, answer):
    if not question or not answer:
        return False, "ç©ºé—®ç­”"
    if REQUIRED_CHINESE_PUNCT not in question:
        return False, "ç¼ºå°‘é—®å·"
    if not (MIN_QUESTION_LEN <= len(question) <= MAX_QUESTION_LEN):
        return False, "é—®é¢˜é•¿åº¦å¼‚å¸¸"
    if not (MIN_ANSWER_LEN <= len(answer) <= MAX_ANSWER_LEN):
        return False, "ç­”æ¡ˆé•¿åº¦å¼‚å¸¸"
    sim = semantic_similarity(source_text, question + " " + answer)
    if sim < SEMANTIC_SIMILARITY_MIN:
        return False, f"ç›¸å…³åº¦ä½({sim:.2f})"
    return True, "ok"

# ========== ä¸»æµç¨‹ ==========
def generate_qa_pairs():
    if not os.path.exists(REWRITTEN_INPUT_PATH):
        logging.error(f"âŒ æ”¹å†™ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼š{REWRITTEN_INPUT_PATH}")
        return

    model, tokenizer = load_qa_model()

    total_lines = sum(1 for _ in open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8'))
    logging.info(f"ğŸ“„ è¾“å…¥æ”¹å†™æ•°æ®æ¡æ•°ï¼š{total_lines}")

    success_count = 0
    fail_count = 0

    with open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8') as f_in, \
         open(QA_OUTPUT_PATH, 'w', encoding='utf-8') as f_ok, \
         open(QA_FAILED_PATH, 'w', encoding='utf-8') as f_fail, \
         open(QA_LOG_PATH, 'w', encoding='utf-8') as f_log:

        for line in tqdm(f_in, desc="ç”ŸæˆQA"):
            try:
                item = json.loads(line)
                source_text = item.get("rewritten_text") or item.get("original_text") or item.get("text") or ""
                source_text = source_text.strip()
                if not source_text:
                    fail_count += 1
                    f_fail.write(json.dumps({"id": item.get("id"), "reason": "ç©ºæºæ–‡æœ¬"}, ensure_ascii=False) + "\n")
                    continue
                # æˆªæ–­è¶…é•¿æ–‡æœ¬
                if len(source_text) > MAX_SOURCE_CHARS:
                    source_text = source_text[:MAX_SOURCE_CHARS]

                q_type = detect_question_type(source_text)
                prompt_key = TYPE_PROMPT_MAPPING.get(q_type, "generic_q")
                prompt_template = QA_PROMPTS[prompt_key]
                prompt = prompt_template.format(text=source_text)

                raw_output = generate_single_qa(model, tokenizer, prompt)
                question, answer = parse_generated(raw_output)
                ok, reason = validate_pair(source_text, question, answer)

                record = {
                    "id": item.get("id"),
                    "question": question,
                    "answer": answer,
                    "status": "success" if ok else "failed",
                    "fail_reason": None if ok else reason,
                    "question_type": q_type,
                    "prompt_key": prompt_key
                }

                if ok:
                    f_ok.write(json.dumps(record, ensure_ascii=False) + "\n")
                    success_count += 1
                else:
                    f_fail.write(json.dumps(record, ensure_ascii=False) + "\n")
                    fail_count += 1

                # ç®€å•æ—¥å¿—
                if (success_count + fail_count) % 200 == 0:
                    f_log.write(f"è¿›åº¦: æˆåŠŸ={success_count}, å¤±è´¥={fail_count}\n")

            except Exception as e:
                fail_count += 1
                f_fail.write(json.dumps({"id": None, "reason": f"å¼‚å¸¸: {str(e)}"}, ensure_ascii=False) + "\n")
                continue

    logging.info(f"âœ… QAç”Ÿæˆå®Œæˆ | æˆåŠŸ {success_count} | å¤±è´¥ {fail_count}")
    logging.info(f"è¾“å‡ºæ–‡ä»¶ï¼š{QA_OUTPUT_PATH}")


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(QA_LOG_PATH, encoding="utf-8")
        ]
    )
    logging.info("ğŸš€ å¯åŠ¨QAç”Ÿæˆæµç¨‹")
    generate_qa_pairs()
    logging.info("ğŸ‰ QAç”Ÿæˆæµç¨‹ç»“æŸ")

if __name__ == "__main__":
    main()
