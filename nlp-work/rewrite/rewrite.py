import json
import logging
import os
from shutil import disk_usage
from tqdm import tqdm
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from rewrite_config import *
from model_utils import (
    load_rewrite_model, generate_rewrite,
    calculate_semantic_similarity, calculate_redundancy_ratio
)
from prompt_templates import PROMPTS

# ========== å›°æƒ‘åº¦è®¡ç®—ï¼ˆä¿æŒä¸å˜ï¼‰ ==========
try:
    perplexity_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if perplexity_tokenizer.pad_token is None:
        perplexity_tokenizer.pad_token = perplexity_tokenizer.eos_token
    
    perplexity_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        low_cpu_mem_usage=True,
        ignore_mismatched_sizes=True,
        trust_remote_code=True
    ).to(DEVICE)
    perplexity_model.eval()
    print(f"âœ… å›°æƒ‘åº¦è®¡ç®—æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆå¤ç”¨{MODEL_ID}ï¼‰")
except Exception as e:
    print(f"âŒ åŠ è½½å›°æƒ‘åº¦è®¡ç®—æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    exit(1)

def calculate_perplexity(text):
    if not text.strip():
        return float('inf')
    
    inputs = perplexity_tokenizer(
        text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQ_LENGTH
    ).to(DEVICE)
    
    with torch.no_grad():
        outputs = perplexity_model(** inputs, labels=inputs["input_ids"])
    
    return torch.exp(outputs.loss).item()

# ========== åŠ¨æ€è®¡ç®—é˜ˆå€¼ï¼ˆå…¨é‡æ•°æ®ï¼‰ ==========
def calculate_modern_perplexity_threshold(input_data_path):
    logging.info(f"ğŸ” æ­£åœ¨è®¡ç®—ç°ä»£æ–‡{MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°")
    modern_perplexities = []
    
    with open(input_data_path, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="æå–ç°ä»£æ–‡å›°æƒ‘åº¦"):
            try:
                item = json.loads(line)
                if item.get("text_type") == "modern_chinese":
                    perplexity = item.get("perplexity", 0)
                    if perplexity > 0:
                        modern_perplexities.append(perplexity)
            except Exception as e:
                continue
    
    if not modern_perplexities:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç°ä»£æ–‡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼800")
        return 800.0
    
    threshold = np.percentile(modern_perplexities, MODERN_PERPLEXITY_PERCENTILE)
    logging.info(f"ğŸ¯ ç°ä»£æ–‡æ”¹å†™é˜ˆå€¼ï¼š{threshold:.2f}")
    return threshold

# ========== ç£ç›˜ç©ºé—´æ£€æŸ¥ï¼ˆé€‚é…13.9GBï¼‰ ==========
def check_disk_space(path, required_gb=10):  # é™ä½è¦æ±‚è‡³10GB
    try:
        disk = disk_usage(path)
        free_gb = disk.free / (1024 **3)
        if free_gb < required_gb:
            logging.error(f"ç£ç›˜ç©ºé—´ä¸è¶³ï¼éœ€è¦è‡³å°‘{required_gb}GBï¼Œå½“å‰å‰©ä½™{free_gb:.2f}GB")
            return False
        return True
    except Exception as e:
        logging.warning(f"ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥ï¼š{str(e)}ï¼Œç»§ç»­æ‰§è¡Œä½†å¯èƒ½æœ‰é£é™©")
        return True

# ========== æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼ˆç²¾ç®€é€»è¾‘ï¼‰ ==========
def should_rewrite(item, high_perplexity_threshold):
    text = item["text"]
    if SKIP_CLASSIC_CHINESE and item.get("text_type") == "classic_chinese":
        return False, "å¤æ–‡"
    
    perplexity = item.get("perplexity", 0)
    redundancy_ratio = calculate_redundancy_ratio(text)
    
    if perplexity > high_perplexity_threshold:
        return True, "é«˜å›°æƒ‘åº¦"
    if redundancy_ratio > REDUNDANCY_RATIO_THRESHOLD:
        return True, "å†—ä½™"
    
    return False, "æ— éœ€æ”¹å†™"

def check_quality(original_text, rewritten_text, rewrite_reason, original_perplexity):
    if not rewritten_text:
        return False, "ç”Ÿæˆç©ºç»“æœ", None
    
    sim_score = calculate_semantic_similarity(original_text, rewritten_text)
    if sim_score < SEMANTIC_SIMILARITY_THRESHOLD:
        return False, f"ç›¸ä¼¼åº¦ä½({sim_score:.2f})", None
    
    rew_perplexity = None
    if rewrite_reason == "é«˜å›°æƒ‘åº¦":
        rew_perplexity = calculate_perplexity(rewritten_text)
        if rew_perplexity >= original_perplexity * PERPLEXITY_REDUCTION_RATIO:
            return False, f"å›°æƒ‘åº¦æœªé™({rew_perplexity:.0f})", rew_perplexity
    
    elif rewrite_reason == "å†—ä½™":
        orig_red = calculate_redundancy_ratio(original_text)
        rew_red = calculate_redundancy_ratio(rewritten_text)
        if rew_red >= orig_red:
            return False, f"å†—ä½™æœªé™({rew_red:.2f})", None
    
    return True, "åˆæ ¼", rew_perplexity

# ========== ä¸»æµç¨‹ï¼ˆç©ºé—´ä¼˜åŒ–ç‰ˆï¼‰ ==========
def main():
    # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆæœ€ä½10GBï¼‰
    if not check_disk_space(os.path.dirname(REWRITTEN_OUTPUT_PATH)):
        print("é”™è¯¯ï¼šç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®æ¸…ç†è‡³å°‘10GBç©ºé—´åé‡è¯•")
        return
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(INPUT_DATA_PATH):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{INPUT_DATA_PATH}")
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼{INPUT_DATA_PATH}")
        return
    
    # åŠ¨æ€è®¡ç®—é˜ˆå€¼
    high_perplexity_threshold = calculate_modern_perplexity_threshold(INPUT_DATA_PATH)
    
    # æ‰¹é‡å¤§å°ä¿æŒ6ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜ï¼‰
    adjusted_batch_size = 6
    logging.info(f"æ‰¹é‡å¤§å°ï¼š{adjusted_batch_size}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer = load_rewrite_model()
    except Exception as e:
        logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        print(f"é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼{str(e)}")
        return
    
    # ç»Ÿè®¡æ€»æ•°æ®é‡
    with open(INPUT_DATA_PATH, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    print(f"æ€»æ•°æ®é‡ï¼š{total_lines}æ¡ï¼Œå¯ç”¨ç£ç›˜ç©ºé—´ï¼š{disk_usage(os.path.dirname(REWRITTEN_OUTPUT_PATH)).free/(1024**3):.2f}GB")
    
    # å…¨é‡å¤„ç†ï¼ˆä¼˜åŒ–ç©ºé—´å ç”¨ï¼‰
    print("å¼€å§‹å…¨é‡å¤„ç†ï¼ˆç©ºé—´ä¼˜åŒ–æ¨¡å¼ï¼‰...")
    batch_buffer = []  # æ‰¹é‡ç¼“å­˜è¾“å‡ºï¼Œå‡å°‘IO
    log_buffer = []    # å†…å­˜ç¼“å­˜æ—¥å¿—ï¼Œæœ€åå†™å…¥
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    try:
        os.makedirs(os.path.dirname(REWRITTEN_OUTPUT_PATH), exist_ok=True)
    except Exception:
        pass

    with open(INPUT_DATA_PATH, "r", encoding="utf-8") as f_in, \
         open(REWRITTEN_OUTPUT_PATH, "w", encoding="utf-8") as f_out:
        
        for batch_start in tqdm(range(0, total_lines, adjusted_batch_size), desc="æ”¹å†™è¿›åº¦"):
            # è¯»å–å½“å‰æ‰¹æ¬¡
            batch = []
            for _ in range(adjusted_batch_size):
                line = f_in.readline()
                if not line:
                    break
                batch.append(line)
            if not batch:
                break
            
            # å¤„ç†æ‰¹æ¬¡æ•°æ®
            for line in batch:
                try:
                    item = json.loads(line)
                    original_id = item.get("id", str(hash(item.get("text", ""))))
                    original_text = item.get("text", "").strip()
                    
                    # ç©ºæ–‡æœ¬å¤„ç†
                    if not original_text:
                        batch_buffer.append(json.dumps({
                            "id": original_id,
                            "text": original_text,
                            "rewritten": None,
                            "status": "skipped",
                            "reason": "ç©ºæ–‡æœ¬"
                        }, ensure_ascii=False))
                        continue
                    
                    # åˆ¤æ–­æ˜¯å¦æ”¹å†™
                    need_rewrite, rewrite_reason = should_rewrite(item, high_perplexity_threshold)
                    if not need_rewrite:
                        batch_buffer.append(json.dumps({
                            "id": original_id,
                            "text": original_text,
                            "rewritten": None,
                            "status": "skipped",
                            "reason": rewrite_reason
                        }, ensure_ascii=False))
                        continue
                    
                    # ç”Ÿæˆæ”¹å†™
                    prompt = PROMPTS[
                        "high_perplexity" if rewrite_reason == "é«˜å›°æƒ‘åº¦" else "redundant"
                    ].format(text=original_text)
                    
                    rewritten_text = None
                    for retry in range(3):
                        try:
                            rewritten_text = generate_rewrite(model, tokenizer, prompt)
                            if rewritten_text:
                                break
                        except Exception as e:
                            log_buffer.append(f"ID={original_id} é‡è¯•{retry+1}æ¬¡å¤±è´¥ï¼š{str(e)}")
                    
                    # è´¨é‡æ£€æŸ¥
                    original_perplexity = item.get("perplexity", 0)
                    quality_ok, quality_reason, rew_perplexity = check_quality(
                        original_text, rewritten_text, rewrite_reason, original_perplexity
                    )
                    
                    # ç²¾ç®€è¾“å‡ºå­—æ®µ
                    result = {
                        "id": original_id,
                        "original_text": original_text,
                        "rewritten_text": rewritten_text,
                        "status": "success" if quality_ok else "failed",
                        "reason": quality_reason,
                        "orig_perplexity": round(original_perplexity, 2) if rewrite_reason == "é«˜å›°æƒ‘åº¦" else None,
                        "rew_perplexity": round(rew_perplexity, 2) if rew_perplexity else None
                    }
                    batch_buffer.append(json.dumps(result, ensure_ascii=False))
                    
                    # æ¯1000æ¡æ‰“å°è¿›åº¦
                    if int(hash(original_id)) % 1000 == 0:
                        print(f"å·²å¤„ç† {batch_start + len(batch)}/{total_lines} æ¡")
                
                except Exception as e:
                    batch_buffer.append(json.dumps({
                        "id": original_id if 'original_id' in locals() else "unknown",
                        "text": original_text if 'original_text' in locals() else "è§£æå¤±è´¥",
                        "rewritten": None,
                        "status": "error",
                        "reason": str(e)
                    }, ensure_ascii=False))
                    log_buffer.append(f"å¤„ç†ID={original_id}å¤±è´¥ï¼š{str(e)}")
                    continue
            
            # æ¯å¤„ç†100ä¸ªæ‰¹æ¬¡å†™å…¥ä¸€æ¬¡ç£ç›˜ï¼ˆå‡å°‘IOï¼‰
            if len(batch_buffer) >= 100 * adjusted_batch_size:
                f_out.write("\n".join(batch_buffer) + "\n")
                batch_buffer = []
        
        # å†™å…¥å‰©ä½™ç¼“å­˜æ•°æ®
        if batch_buffer:
            f_out.write("\n".join(batch_buffer) + "\n")
    
    # æœ€åå†™å…¥æ—¥å¿—ï¼ˆé¿å…å®æ—¶å ç”¨ç©ºé—´ï¼‰
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    try:
        os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
    except Exception:
        pass

    with open(LOG_PATH, "w", encoding="utf-8") as f_log:
        f_log.write("\n".join(log_buffer))
    
    print(f"\nå¤„ç†å®Œæˆï¼ç»“æœæ–‡ä»¶ï¼š{REWRITTEN_OUTPUT_PATH}")
    print(f"å‰©ä½™ç£ç›˜ç©ºé—´ï¼š{disk_usage(os.path.dirname(REWRITTEN_OUTPUT_PATH)).free/(1024**3):.2f}GB")

if __name__ == "__main__":
    main()