# qa/qa_generate.py (vLLM é«˜æ€§èƒ½ç‰ˆ + ä¸¥æ ¼è´¨æ£€)

import os
import json
import logging
from tqdm import tqdm
import random

from vllm import LLM, SamplingParams

from qa.qa_config import (
    REWRITTEN_INPUT_PATH, QA_OUTPUT_PATH, QA_FAILED_PATH, QA_LOG_PATH,
    QA_MODEL_ID, MAX_NEW_TOKENS_QA, TEMPERATURE_QA, TOP_P_QA, BATCH_SIZE_QA,
    MAX_SOURCE_CHARS, MIN_QUESTION_LEN, MAX_QUESTION_LEN, MIN_ANSWER_LEN,
    MAX_ANSWER_LEN, REQUIRED_CHINESE_PUNCT, SEMANTIC_SIMILARITY_MIN,
    TYPE_KEYWORDS, TYPE_PROMPT_MAPPING, NUM_QA_ATTEMPTS_PER_TEXT
)
from qa.prompt_templates import get_random_prompt

try:
    from rewrite.model_utils import text_to_embedding
    _HAS_EMB = True
    logging.info("âœ… æˆåŠŸå¯¼å…¥è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•° (text_to_embedding)")
except ImportError:
    _HAS_EMB = False
    logging.warning("âš ï¸ æœªæ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œç›¸å…³è´¨æ£€å°†è·³è¿‡ã€‚")

# ================================================================
# ========== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ==========
# ================================================================

def load_qa_model():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    logging.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½QAç”Ÿæˆæ¨¡å‹ (vLLM Engine): {QA_MODEL_ID}")
    try:
        model = LLM(model=QA_MODEL_ID, trust_remote_code=True)
        tokenizer = model.get_tokenizer()
        logging.info("âœ… QAæ¨¡å‹ (vLLM) åŠ è½½å®Œæˆ")
        return model, tokenizer
    except Exception as e:
        logging.error(f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
        exit(1)

def detect_question_type(source_text: str) -> str:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    lower_text = source_text.lower()
    for q_type, kws in TYPE_KEYWORDS.items():
        for kw in kws:
            if kw in source_text or kw in lower_text:
                return q_type
    if any(ch.isdigit() for ch in source_text) and any(sym in source_text for sym in ["=", "+", "-", "â†’", "%"]):
        return "calculate"
    return "fallback"

def semantic_similarity(a: str, b: str) -> float:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    if not _HAS_EMB: return 1.0
    try:
        emb1 = text_to_embedding(a)
        emb2 = text_to_embedding(b)
        import numpy as np
        emb1 = emb1 / np.linalg.norm(emb1)
        emb2 = emb2 / np.linalg.norm(emb2)
        sim = float(np.dot(emb1, emb2.T))
        return sim
    except Exception:
        return 0.0

# ========== ã€ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 1ã€‘ã€‘: å‡çº§è§£æå™¨ ==========
def parse_generated(text: str) -> tuple:
    """
    ã€å‡çº§ç‰ˆã€‘ä»æ¨¡å‹è¾“å‡ºä¸­è§£æã€å•ä¸ªã€‘é—®ç­”å¯¹ï¼Œå¹¶ä¸»åŠ¨æ‹’ç»ä¸è‰¯æ ¼å¼ã€‚
    è¿”å›: (question, answer_or_error_reason)
    """
    q_marker, a_marker = "é—®é¢˜ï¼š", "ç­”æ¡ˆï¼š"
    
    # è§„åˆ™1ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä½™çš„ "é—®é¢˜ï¼š" æ ‡è®°
    if text.count(q_marker) > 1:
        return None, "è§£æå¤±è´¥: è¾“å‡ºåŒ…å«å¤šä¸ªQAå¯¹"

    q_idx = text.find(q_marker)
    a_idx = text.find(a_marker)

    # è§„åˆ™2ï¼šæ£€æŸ¥åŸºæœ¬ç»“æ„æ˜¯å¦å­˜åœ¨
    if q_idx == -1 or a_idx == -1 or a_idx <= q_idx:
        return None, "è§£æå¤±è´¥: ç»“æ„ä¸ç¬¦"
        
    question = text[q_idx + len(q_marker):a_idx].strip()
    answer = text[a_idx + len(a_marker):].strip()

    # è§„åˆ™3ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«æ— æ„ä¹‰çš„å ä½ç¬¦æˆ–ä¸ºç©º
    if not answer or "..." in answer:
        return question, "è§£æå¤±è´¥: ç­”æ¡ˆä¸ºç©ºæˆ–æœªå®Œæˆ"

    return question, answer

# ========== ã€ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 2ã€‘ã€‘: å‡çº§è´¨æ£€å‘˜ ==========
def validate_pair(source_text, question, answer):
    """
    ã€å‡çº§ç‰ˆã€‘å¯¹ç”Ÿæˆçš„é—®ç­”å¯¹è¿›è¡Œå¤šç»´åº¦è´¨é‡æ ¡éªŒã€‚
    """
    # è§„åˆ™0ï¼šå‰ç½®è§£æå·²å¤±è´¥
    if not question:
        return False, answer or "è§£æå¤±è´¥: æœªæ‰¾åˆ°é—®é¢˜"
    if answer and "è§£æå¤±è´¥:" in answer:
        return False, answer
        
    # è§„åˆ™1ï¼šé—®é¢˜ç¼ºå°‘é—®å·
    if REQUIRED_CHINESE_PUNCT and REQUIRED_CHINESE_PUNCT not in question:
        return False, "é—®é¢˜ç¼ºå°‘é—®å·"
        
    # è§„åˆ™2ï¼šé—®é¢˜é•¿åº¦å¼‚å¸¸
    if not (MIN_QUESTION_LEN <= len(question) <= MAX_QUESTION_LEN):
        return False, f"é—®é¢˜é•¿åº¦å¼‚å¸¸({len(question)})"
        
    # è§„åˆ™3ï¼šç­”æ¡ˆé•¿åº¦å¼‚å¸¸
    if not (MIN_ANSWER_LEN <= len(answer) <= MAX_ANSWER_LEN):
        return False, f"ç­”æ¡ˆé•¿åº¦å¼‚å¸¸({len(answer)})"
    
    # è§„åˆ™4ï¼šä¸åŸæ–‡ç›¸å…³åº¦ä½
    sim = semantic_similarity(source_text, question + " " + answer)
    if sim < SEMANTIC_SIMILARITY_MIN:
        return False, f"ä¸åŸæ–‡ç›¸å…³åº¦ä½({sim:.2f})"
    
    return True, "ok"


# ================================================================
# ========== vLLM æ‰¹é‡ç”Ÿæˆå‡½æ•° (ä¿æŒä¸å˜) ==========
# ================================================================
def generate_batch_qa(model: LLM, prompts: list[str]) -> list[str]:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    try:
        sampling_params = SamplingParams(temperature=TEMPERATURE_QA, top_p=TOP_P_QA, max_tokens=MAX_NEW_TOKENS_QA)
        outputs = model.generate(prompts, sampling_params, use_tqdm=False)
        return [output.outputs[0].text.strip() for output in outputs]
    except Exception as e:
        logging.warning(f"âš ï¸ vLLM æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        return [""] * len(prompts)


# ================================================================
# ========== ä¸»æµç¨‹ (ä¿æŒä¸å˜ï¼Œå·²æ”¯æŒå¤šè½®ç”Ÿæˆ) ==========
# ================================================================

def process_batch(batch_data, model, f_ok, f_fail):
    # ... (æ­¤å‡½æ•°ç°åœ¨ä¸å‡çº§åçš„ parse/validate æ— ç¼å¯¹æ¥ï¼Œæ— éœ€ä¿®æ”¹) ...
    if not batch_data: return 0, 0
    batch_prompts = [item['prompt'] for item in batch_data]
    raw_outputs = generate_batch_qa(model, batch_prompts)
    success_count, fail_count = 0, 0
    seen_in_batch = set() # æ‰¹æ¬¡å†…å»é‡

    for i, raw_output in enumerate(raw_outputs):
        item_data = batch_data[i]
        
        # ã€æ³¨æ„ã€‘è¿™é‡Œçš„ question å’Œ answer å·²ç»ç»è¿‡äº†å‡çº§ç‰ˆçš„ parse_generated
        question, answer = parse_generated(raw_output)
        
        # ã€æ³¨æ„ã€‘è¿™é‡Œçš„ validate_pair æ˜¯å‡çº§ç‰ˆçš„
        ok, reason = validate_pair(item_data['source_text'], question, answer)

        # (å¯é€‰) æ‰¹æ¬¡å†…å»é‡é€»è¾‘
        if ok:
            qa_pair_str = f"{question}|{answer}"
            if qa_pair_str in seen_in_batch:
                ok, reason = False, "æ‰¹æ¬¡å†…é‡å¤"
            else:
                seen_in_batch.add(qa_pair_str)

        record = {
            "id": item_data["id"], "question": question, "answer": answer,
            "status": "success" if ok else "failed", "fail_reason": None if ok else reason,
            "question_type": item_data["q_type"], "prompt_key": item_data["prompt_key"]
        }

        if ok:
            f_ok.write(json.dumps(record, ensure_ascii=False) + "\n")
            success_count += 1
        else:
            f_fail.write(json.dumps(record, ensure_ascii=False) + "\n")
            fail_count += 1
            
    return success_count, fail_count

def generate_qa_pairs():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    if not os.path.exists(REWRITTEN_INPUT_PATH):
        logging.error(f"âŒ æ”¹å†™ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼š{REWRITTEN_INPUT_PATH}")
        return
    model, _ = load_qa_model()
    try:
        total_lines = sum(1 for _ in open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8'))
    except Exception as e:
        logging.error(f"âŒ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶è¡Œæ•°: {e}")
        return
    logging.info(f"ğŸ“„ è¾“å…¥æ”¹å†™æ•°æ®æ€»æ¡æ•°ï¼š{total_lines}")
    logging.info(f"ğŸš€ ä½¿ç”¨ vLLM å¼•æ“ï¼Œæ‰¹å¤„ç†å¤§å° (Batch Size): {BATCH_SIZE_QA}")
    logging.info(f"ğŸš€ æ¯ä¸ªæºæ–‡æœ¬å°†å°è¯•ç”Ÿæˆ {NUM_QA_ATTEMPTS_PER_TEXT} æ¬¡ QA")
    total_success, total_fail, batch_data = 0, 0, []
    with open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8') as f_in, \
         open(QA_OUTPUT_PATH, 'w', encoding='utf-8') as f_ok, \
         open(QA_FAILED_PATH, 'w', encoding='utf-8') as f_fail:
        for line in tqdm(f_in, total=total_lines, desc="QA ç”Ÿæˆè¿›åº¦"):
            try:
                item = json.loads(line)
                source_text = (item.get("rewritten_text") or item.get("original_text") or item.get("text") or "").strip()
                if not source_text:
                    total_fail += 1
                    f_fail.write(json.dumps({"id": item.get("id"), "reason": "ç©ºæºæ–‡æœ¬"}, ensure_ascii=False) + "\n")
                    continue
                if len(source_text) > MAX_SOURCE_CHARS:
                    source_text = source_text[:MAX_SOURCE_CHARS]
                for _ in range(NUM_QA_ATTEMPTS_PER_TEXT):
                    q_type = detect_question_type(source_text)
                    prompt_key = TYPE_PROMPT_MAPPING.get(q_type, "generic_q")
                    prompt_template = get_random_prompt(prompt_key)
                    prompt = prompt_template.format(text=source_text)
                    batch_data.append({
                        "id": item.get("id"), "prompt": prompt, "source_text": source_text,
                        "q_type": q_type, "prompt_key": prompt_key
                    })
                if len(batch_data) >= BATCH_SIZE_QA:
                    s, f = process_batch(batch_data, model, f_ok, f_fail)
                    total_success += s
                    total_fail += f
                    batch_data = []
            except Exception as e:
                logging.error(f"å¤„ç†è¡Œæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
                total_fail += 1
                continue
        if batch_data:
            s, f = process_batch(batch_data, model, f_ok, f_fail)
            total_success += s
            total_fail += f
    logging.info(f"âœ… QAç”Ÿæˆå®Œæˆ | æˆåŠŸ {total_success} | å¤±è´¥ {total_fail}")
    logging.info(f"ğŸ“„ æˆåŠŸé—®ç­”å¯¹è¾“å‡ºè‡³: {QA_OUTPUT_PATH}")
    logging.info(f"ğŸ“„ å¤±è´¥è®°å½•è¾“å‡ºè‡³: {QA_FAILED_PATH}")


def main():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    os.makedirs(os.path.dirname(QA_LOG_PATH), exist_ok=True)
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(), logging.FileHandler(QA_LOG_PATH, mode='w', encoding="utf-8")]
    )
    logging.info("ğŸš€ å¯åŠ¨QAç”Ÿæˆæµç¨‹ (vLLM é«˜æ€§èƒ½ç‰ˆ)")
    generate_qa_pairs()
    logging.info("ğŸ‰ QAç”Ÿæˆæµç¨‹ç»“æŸ")

if __name__ == "__main__":
    main()
