# qa/qa_generate.py (vLLM é«˜æ€§èƒ½ç‰ˆæœ¬)

import os
import json
import logging
from tqdm import tqdm
import random # ç¡®ä¿å¯¼å…¥ random

# å¯¼å…¥ vLLM æ ¸å¿ƒåº“
from vllm import LLM, SamplingParams

# ä»é…ç½®æ–‡ä»¶å¯¼å…¥æ‰€æœ‰å‚æ•°
from qa.qa_config import (
    REWRITTEN_INPUT_PATH, QA_OUTPUT_PATH, QA_FAILED_PATH, QA_LOG_PATH,
    QA_MODEL_ID, MAX_NEW_TOKENS_QA, TEMPERATURE_QA, TOP_P_QA, BATCH_SIZE_QA,
    MAX_SOURCE_CHARS, MIN_QUESTION_LEN, MAX_QUESTION_LEN, MIN_ANSWER_LEN,
    MAX_ANSWER_LEN, REQUIRED_CHINESE_PUNCT, SEMANTIC_SIMILARITY_MIN,
    TYPE_KEYWORDS, TYPE_PROMPT_MAPPING
)
# ä»æ¨¡æ¿æ–‡ä»¶å¯¼å…¥éšæœºé€‰æ‹©å‡½æ•°
from qa.prompt_templates import get_random_prompt

# åŠ¨æ€å¯¼å…¥è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œä¿æŒæ¾è€¦åˆ
try:
    from rewrite.model_utils import text_to_embedding
    _HAS_EMB = True
    logging.info("âœ… æˆåŠŸå¯¼å…¥è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•° (text_to_embedding)")
except ImportError:
    _HAS_EMB = False
    logging.warning("âš ï¸ æœªæ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œç›¸å…³è´¨æ£€å°†è·³è¿‡ã€‚")

# ================================================================
# ========== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (å¤§éƒ¨åˆ†ä¿æŒä¸å˜) ==========
# ================================================================

def load_qa_model():
    """ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹"""
    logging.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½QAç”Ÿæˆæ¨¡å‹ (vLLM Engine): {QA_MODEL_ID}")
    try:
        # vLLM ä¼šè‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œå¯¹äº1.8Bæ¨¡å‹ï¼Œ4090åŠ è½½FP16æ€§èƒ½æœ€ä½³
        model = LLM(model=QA_MODEL_ID, trust_remote_code=True)
        tokenizer = model.get_tokenizer() # Tokenizer ä» vLLM å®ä¾‹ä¸­è·å–
        logging.info("âœ… QAæ¨¡å‹ (vLLM) åŠ è½½å®Œæˆ")
        return model, tokenizer
    except Exception as e:
        logging.error(f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
        logging.error("å¯èƒ½åŸå› ï¼šæ¨¡å‹IDé”™è¯¯ã€ç½‘ç»œé—®é¢˜ã€vLLMä¸CUDAé©±åŠ¨ä¸å…¼å®¹æˆ–æ˜¾å­˜ä¸è¶³ã€‚")
        exit(1)

def detect_question_type(source_text: str) -> str:
    """æ ¹æ®å…³é”®è¯çŒœæµ‹é—®é¢˜ç±»å‹"""
    lower_text = source_text.lower()
    for q_type, kws in TYPE_KEYWORDS.items():
        for kw in kws:
            if kw in source_text or kw in lower_text:
                return q_type
    if any(ch.isdigit() for ch in source_text) and any(sym in source_text for sym in ["=", "+", "-", "â†’", "%"]):
        return "calculate"
    return "fallback"

def semantic_similarity(a: str, b: str) -> float:
    """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
    if not _HAS_EMB:
        return 1.0  # æ²¡æœ‰åµŒå…¥å‡½æ•°æ—¶ç›´æ¥æ”¾è¡Œ
    try:
        emb1 = text_to_embedding(a)
        emb2 = text_to_embedding(b)
        import numpy as np
        # å½’ä¸€åŒ–åç‚¹ç§¯ï¼Œæ›´ç¨³å®š
        emb1 = emb1 / np.linalg.norm(emb1)
        emb2 = emb2 / np.linalg.norm(emb2)
        sim = float(np.dot(emb1, emb2.T))
        return sim
    except Exception:
        return 0.0

def parse_generated(text: str):
    """ä»æ¨¡å‹è¾“å‡ºä¸­è§£æé—®ç­”å¯¹"""
    q_marker, a_marker = "é—®é¢˜ï¼š", "ç­”æ¡ˆï¼š"
    q_idx = text.find(q_marker)
    a_idx = text.find(a_marker)
    if q_idx == -1 or a_idx == -1 or a_idx <= q_idx:
        return None, None
    question = text[q_idx + len(q_marker):a_idx].strip()
    answer = text[a_idx + len(a_marker):].strip()
    return question, answer

def validate_pair(source_text, question, answer):
    """å¯¹ç”Ÿæˆçš„é—®ç­”å¯¹è¿›è¡Œå¤šç»´åº¦è´¨é‡æ ¡éªŒ"""
    if not question or not answer:
        return False, "ç”Ÿæˆç©ºé—®ç­”"
    if REQUIRED_CHINESE_PUNCT not in question:
        return False, "é—®é¢˜ç¼ºå°‘é—®å·"
    if not (MIN_QUESTION_LEN <= len(question) <= MAX_QUESTION_LEN):
        return False, f"é—®é¢˜é•¿åº¦å¼‚å¸¸({len(question)})"
    if not (MIN_ANSWER_LEN <= len(answer) <= MAX_ANSWER_LEN):
        return False, f"ç­”æ¡ˆé•¿åº¦å¼‚å¸¸({len(answer)})"
    
    sim = semantic_similarity(source_text, question + " " + answer)
    if sim < SEMANTIC_SIMILARITY_MIN:
        return False, f"ä¸åŸæ–‡ç›¸å…³åº¦ä½({sim:.2f})"
    
    return True, "ok"

# ================================================================
# ========== vLLM æ‰¹é‡ç”Ÿæˆå‡½æ•° (æ–°å¢) ==========
# ================================================================

def generate_batch_qa(model: LLM, prompts: list[str]) -> list[str]:
    """ä½¿ç”¨ vLLM å¯¹ä¸€ä¸ªæ‰¹æ¬¡çš„ prompts è¿›è¡Œé«˜æ•ˆç”Ÿæˆ"""
    try:
        sampling_params = SamplingParams(
            temperature=TEMPERATURE_QA,
            top_p=TOP_P_QA,
            max_tokens=MAX_NEW_TOKENS_QA
        )
        outputs = model.generate(prompts, sampling_params, use_tqdm=False)
        generated_texts = [output.outputs[0].text.strip() for output in outputs]
        return generated_texts
    except Exception as e:
        logging.warning(f"âš ï¸ vLLM æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        return [""] * len(prompts)

# ================================================================
# ========== ä¸»æµç¨‹ (å®Œå…¨é‡æ„ä¸ºæ‰¹é‡å¤„ç†æ¨¡å¼) ==========
# ================================================================

def process_batch(batch_data, model, f_ok, f_fail):
    """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼šç”Ÿæˆã€è§£æã€è´¨æ£€ã€å†™å…¥"""
    if not batch_data:
        return 0, 0

    # å‡†å¤‡æ‰¹é‡æ•°æ®
    batch_prompts = [item['prompt'] for item in batch_data]
    
    # æ‰¹é‡ç”Ÿæˆ
    raw_outputs = generate_batch_qa(model, batch_prompts)

    success_count, fail_count = 0, 0

    # éå†æ‰¹æ¬¡ç»“æœå¹¶å¤„ç†
    for i, raw_output in enumerate(raw_outputs):
        item_data = batch_data[i]
        question, answer = parse_generated(raw_output)
        ok, reason = validate_pair(item_data['source_text'], question, answer)

        record = {
            "id": item_data["id"],
            "question": question,
            "answer": answer,
            "status": "success" if ok else "failed",
            "fail_reason": None if ok else reason,
            "question_type": item_data["q_type"],
            "prompt_key": item_data["prompt_key"]
        }

        if ok:
            f_ok.write(json.dumps(record, ensure_ascii=False) + "\n")
            success_count += 1
        else:
            f_fail.write(json.dumps(record, ensure_ascii=False) + "\n")
            fail_count += 1
            
    return success_count, fail_count

def generate_qa_pairs():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£è¯»å–ã€æ‰¹å¤„ç†å’Œå†™å…¥"""
    if not os.path.exists(REWRITTEN_INPUT_PATH):
        logging.error(f"âŒ æ”¹å†™ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼š{REWRITTEN_INPUT_PATH}")
        return

    model, _ = load_qa_model()

    try:
        total_lines = sum(1 for _ in open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8'))
    except Exception as e:
        logging.error(f"âŒ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶è¡Œæ•°: {e}")
        return
        
    logging.info(f"ğŸ“„ è¾“å…¥æ”¹å†™æ•°æ®æ€»æ¡æ•°ï¼š{total_lines}")
    logging.info(f"ğŸš€ ä½¿ç”¨ vLLM å¼•æ“ï¼Œæ‰¹å¤„ç†å¤§å° (Batch Size): {BATCH_SIZE_QA}")

    total_success = 0
    total_fail = 0
    batch_data = []

    with open(REWRITTEN_INPUT_PATH, 'r', encoding='utf-8') as f_in, \
         open(QA_OUTPUT_PATH, 'w', encoding='utf-8') as f_ok, \
         open(QA_FAILED_PATH, 'w', encoding='utf-8') as f_fail:

        for line in tqdm(f_in, total=total_lines, desc="QA ç”Ÿæˆè¿›åº¦"):
            try:
                item = json.loads(line)
                source_text = (item.get("rewritten_text") or item.get("original_text") or item.get("text") or "").strip()

                if not source_text:
                    total_fail += 1
                    f_fail.write(json.dumps({"id": item.get("id"), "reason": "ç©ºæºæ–‡æœ¬"}, ensure_ascii=False) + "\n")
                    continue
                
                if len(source_text) > MAX_SOURCE_CHARS:
                    source_text = source_text[:MAX_SOURCE_CHARS]

                q_type = detect_question_type(source_text)
                prompt_key = TYPE_PROMPT_MAPPING.get(q_type, "generic_q")
                prompt_template = get_random_prompt(prompt_key)
                prompt = prompt_template.format(text=source_text)
                
                batch_data.append({
                    "id": item.get("id"),
                    "prompt": prompt,
                    "source_text": source_text,
                    "q_type": q_type,
                    "prompt_key": prompt_key
                })
                
                if len(batch_data) >= BATCH_SIZE_QA:
                    s, f = process_batch(batch_data, model, f_ok, f_fail)
                    total_success += s
                    total_fail += f
                    batch_data = []

            except json.JSONDecodeError:
                total_fail += 1
                f_fail.write(json.dumps({"id": None, "reason": "JSONè§£æå¤±è´¥"}, ensure_ascii=False) + "\n")
                continue
            except Exception as e:
                logging.error(f"å¤„ç†è¡Œæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
                total_fail += 1
                continue
        
        # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
        if batch_data:
            s, f = process_batch(batch_data, model, f_ok, f_fail)
            total_success += s
            total_fail += f

    logging.info(f"âœ… QAç”Ÿæˆå®Œæˆ | æˆåŠŸ {total_success} | å¤±è´¥ {total_fail}")
    logging.info(f"ğŸ“„ æˆåŠŸé—®ç­”å¯¹è¾“å‡ºè‡³: {QA_OUTPUT_PATH}")
    logging.info(f"ğŸ“„ å¤±è´¥è®°å½•è¾“å‡ºè‡³: {QA_FAILED_PATH}")


def main():
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(QA_LOG_PATH), exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(QA_LOG_PATH, mode='w', encoding="utf-8")
        ]
    )
    logging.info("ğŸš€ å¯åŠ¨QAç”Ÿæˆæµç¨‹ (vLLM é«˜æ€§èƒ½ç‰ˆ)")
    generate_qa_pairs()
    logging.info("ğŸ‰ QAç”Ÿæˆæµç¨‹ç»“æŸ")

if __name__ == "__main__":
    main()