# rewrite.py (æœ€ç»ˆä¿®å¤ç‰ˆ)

import json
import logging
import os
import random
from shutil import disk_usage
from tqdm import tqdm
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from rewrite_config import *
# å¯¼å…¥ä¿®æ”¹åçš„å‡½æ•°
from model_utils import (
    load_rewrite_model, generate_rewrites_batch,
    calculate_semantic_similarity, calculate_redundancy_ratio
)
from prompt_templates import PROMPTS

# ================================================================
# ========== ã€æ ¸å¿ƒä¿®æ”¹1ã€‘: ç»Ÿä¸€åŠ è½½â€œå·¥å…·æ¨¡å‹â€ ==========
# ================================================================

# åœ¨å…¨å±€åªåŠ è½½ä¸€æ¬¡â€œå·¥å…·æ¨¡å‹â€ï¼Œç”¨äºå›°æƒ‘åº¦å’Œè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
try:
    print(f"æ­£åœ¨åŠ è½½ç»Ÿä¸€çš„å·¥å…·æ¨¡å‹ï¼ˆ{MODEL_ID}ï¼‰...")
    tool_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tool_tokenizer.pad_token is None:
        tool_tokenizer.pad_token = tool_tokenizer.eos_token
    
    tool_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID, device_map="auto", low_cpu_mem_usage=True,
        ignore_mismatched_sizes=True, trust_remote_code=True
    ).to(DEVICE)
    tool_model.eval()
    print(f"âœ… å·¥å…·æ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ åŠ è½½å·¥å…·æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    exit(1)

# ================================================================
# ========== æ ¸å¿ƒè¾…åŠ©å‡½æ•° (é€‚é…æ¨¡å‹ä¼ é€’) ==========
# ================================================================

def calculate_perplexity(text):
    """ã€å·²ä¿®æ”¹ã€‘ä½¿ç”¨å…¨å±€åŠ è½½çš„å·¥å…·æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦"""
    if not text.strip(): return float('inf')
    inputs = tool_tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)
    with torch.no_grad():
        outputs = tool_model(**inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()

def calculate_modern_perplexity_threshold(input_data_path):
    # æ­¤å‡½æ•°ä¸å˜
    logging.info(f"ğŸ” æ­£åœ¨è®¡ç®—ç°ä»£æ–‡{MODERN_PERPLEXITY_PERCENTILE}%åˆ†ä½æ•°")
    # ... (ä»£ç ä¸å˜)
    modern_perplexities = []
    with open(input_data_path, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="æå–ç°ä»£æ–‡å›°æƒ‘åº¦"):
            try:
                item = json.loads(line)
                if item.get("text_type") == "modern_chinese":
                    perplexity = item.get("perplexity", 0)
                    if perplexity > 0: modern_perplexities.append(perplexity)
            except: continue
    if not modern_perplexities:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç°ä»£æ–‡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼800")
        return 800.0
    threshold = np.percentile(modern_perplexities, MODERN_PERPLEXITY_PERCENTILE)
    logging.info(f"ğŸ¯ ç°ä»£æ–‡æ”¹å†™é˜ˆå€¼ï¼š{threshold:.2f}")
    return threshold


def check_disk_space(path, required_gb=10):
    # æ­¤å‡½æ•°ä¸å˜
    # ...
    try:
        free_gb = disk_usage(path).free / (1024**3)
        if free_gb < required_gb:
            logging.error(f"ç£ç›˜ç©ºé—´ä¸è¶³ï¼éœ€è¦è‡³å°‘{required_gb}GBï¼Œå½“å‰å‰©ä½™{free_gb:.2f}GB")
            return False
        return True
    except:
        return True

def should_rewrite(item, high_perplexity_threshold):
    # æ­¤å‡½æ•°ä¸å˜
    # ...
    text = item.get("text", "").strip()
    if not text: return False, None
    if SKIP_CLASSIC_CHINESE and item.get("text_type") == "classic_chinese": return False, "å¤æ–‡"
    perplexity = item.get("perplexity", 0)
    if perplexity > high_perplexity_threshold: return True, "high_perplexity"
    redundancy_ratio = calculate_redundancy_ratio(text)
    if redundancy_ratio > REDUNDANCY_RATIO_THRESHOLD: return True, "redundant"
    if ENABLE_LOGIC_INJECTION and item.get("text_type") == "modern_chinese":
        is_short = len(text) < LOGIC_CANDIDATE_MAX_LEN
        has_no_logic = not any(word in text for word in LOGIC_KEYWORDS)
        if is_short and has_no_logic and random.random() < LOGIC_INJECTION_SAMPLING_RATE:
            return True, "add_logic_chain"
    return False, "æ— éœ€æ”¹å†™"


def check_quality(original_text, rewritten_text, rewrite_reason, original_perplexity):
    """ã€å·²ä¿®æ”¹ã€‘å°†å…¨å±€å·¥å…·æ¨¡å‹ä½œä¸ºå‚æ•°ä¼ é€’ç»™ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°"""
    if not rewritten_text or "æ”¹å†™å¤±è´¥" in rewritten_text: return False, "ç”Ÿæˆç©ºç»“æœæˆ–å¤±è´¥", None

    if rewrite_reason == "add_logic_chain":
        if not any(word in rewritten_text for word in LOGIC_KEYWORDS): return False, "æœªæ³¨å…¥é€»è¾‘é“¾", None
        sim_score = calculate_semantic_similarity(original_text, rewritten_text, tool_model, tool_tokenizer) # ä¼ é€’æ¨¡å‹
        if sim_score < LOGIC_SIMILARITY_THRESHOLD: return False, f"ç›¸ä¼¼åº¦è¿‡ä½({sim_score:.2f})", None
        rew_perplexity = calculate_perplexity(rewritten_text)
        return True, "logic_injected", rew_perplexity

    sim_score = calculate_semantic_similarity(original_text, rewritten_text, tool_model, tool_tokenizer) # ä¼ é€’æ¨¡å‹
    if sim_score < SEMANTIC_SIMILARITY_THRESHOLD: return False, f"ç›¸ä¼¼åº¦ä½({sim_score:.2f})", None
    
    rew_perplexity = None
    if rewrite_reason == "high_perplexity":
        rew_perplexity = calculate_perplexity(rewritten_text)
        if rew_perplexity >= original_perplexity * PERPLEXITY_REDUCTION_RATIO: return False, f"å›°æƒ‘åº¦æœªé™({rew_perplexity:.0f})", rew_perplexity
    elif rewrite_reason == "redundant":
        orig_red = calculate_redundancy_ratio(original_text)
        rew_red = calculate_redundancy_ratio(rewritten_text)
        if rew_red >= orig_red: return False, f"å†—ä½™æœªé™({rew_red:.2f})", None
        
    return True, "åˆæ ¼", rew_perplexity

# ================================================================
# ========== ä¸»æµç¨‹ (ä¿æŒvLLMçš„â€œå…ˆåˆ†ç±»ï¼Œåæ‰¹é‡â€æ¨¡å¼) ==========
# ================================================================

def main():
    if not check_disk_space(os.path.dirname(REWRITTEN_OUTPUT_PATH)): return
    if not os.path.exists(INPUT_DATA_PATH):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{INPUT_DATA_PATH}")
        return

    high_perplexity_threshold = calculate_modern_perplexity_threshold(INPUT_DATA_PATH)
    
    try:
        vllm_model, vllm_tokenizer = load_rewrite_model()
    except Exception as e:
        logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return
        
    print("æ­¥éª¤ 1/2: æ­£åœ¨åˆ†ç±»æ•°æ®...")
    items_to_rewrite = []
    skipped_items_output = []
    
    with open(INPUT_DATA_PATH, "r", encoding="utf-8") as f:
        all_lines = f.readlines()
        
    for line in tqdm(all_lines, desc="åˆ†ç±»è¿›åº¦"):
        try:
            item = json.loads(line)
            # --- ã€ã€ã€æ–°å¢ä»£ç ï¼šæˆªæ–­è¶…é•¿æ–‡æœ¬ã€‘ã€‘ã€‘ ---
            original_text = item.get("text", "").strip()
            # æˆ‘ä»¬éœ€è¦ç»™Promptæ¨¡æ¿çš„æ–‡æœ¬ç•™å‡ºä¸€äº›ç©ºé—´ï¼Œæ‰€ä»¥æˆªæ–­é•¿åº¦è¦æ¯”max_model_lenå°
            # å‡è®¾Promptæ¨¡æ¿æœ¬èº«å¤§çº¦å 200ä¸ªtoken
            max_model_len=8192
            max_text_len = max_model_len - 200 
            if len(original_text) > max_text_len:
                original_text = original_text[:max_text_len]
                item['text'] = original_text # å°†æˆªæ–­åçš„æ–‡æœ¬å†™å›itemï¼Œä¾›åç»­ä½¿ç”¨
            need_rewrite, rewrite_reason = should_rewrite(item, high_perplexity_threshold)
            
            if need_rewrite and rewrite_reason in PROMPTS:
                item['rewrite_reason'] = rewrite_reason
                items_to_rewrite.append(item)
            else:
                original_id = item.get("id", str(hash(item.get("text", ""))))
                skipped_items_output.append(json.dumps({
                    "id": original_id, "original_text": item.get("text", ""), "rewritten_text": None,
                    "status": "skipped", "reason": rewrite_reason
                }, ensure_ascii=False))
        except Exception:
            continue
            
    print(f"æ­¥éª¤ 2/2: æ‰¾åˆ° {len(items_to_rewrite)} æ¡æ•°æ®éœ€è¦æ”¹å†™ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    
    os.makedirs(os.path.dirname(REWRITTEN_OUTPUT_PATH), exist_ok=True)

    with open(REWRITTEN_OUTPUT_PATH, "w", encoding="utf-8") as f_out:
        if skipped_items_output:
            f_out.write("\n".join(skipped_items_output) + "\n")
            
        for i in tqdm(range(0, len(items_to_rewrite), BATCH_SIZE), desc="vLLMæ”¹å†™è¿›åº¦"):
            batch_chunk = items_to_rewrite[i : i + BATCH_SIZE]
            prompts = [PROMPTS[item['rewrite_reason']].format(text=item.get("text", "").strip()) for item in batch_chunk]
            rewritten_texts = generate_rewrites_batch(vllm_model, prompts)
            
            output_buffer = []
            for j, rewritten_text in enumerate(rewritten_texts):
                original_item = batch_chunk[j]
                original_text = original_item.get("text", "").strip()
                original_id = original_item.get("id", str(hash(original_text)))
                
                quality_ok, quality_reason, rew_perplexity = check_quality(
                    original_text, rewritten_text, original_item['rewrite_reason'], original_item.get("perplexity", 0)
                )
                
                result = {
                    "id": original_id, "original_text": original_text, "rewritten_text": rewritten_text,
                    "status": "success" if quality_ok else "failed", "reason": quality_reason,
                    "orig_perplexity": round(original_item.get("perplexity", 0), 2) if original_item['rewrite_reason'] == "high_perplexity" else None,
                    "rew_perplexity": round(rew_perplexity, 2) if rew_perplexity else None
                }
                output_buffer.append(json.dumps(result, ensure_ascii=False))
            
            f_out.write("\n".join(output_buffer) + "\n")

    print(f"\nå¤„ç†å®Œæˆï¼ç»“æœæ–‡ä»¶ï¼š{REWRITTEN_OUTPUT_PATH}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    main()